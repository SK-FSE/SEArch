[{"metamdp": 1, "approach": 1, "exploration": 1, "lifelong": 1, "reinforcement": 3, "learn": 3, "francisco": 1, "garcia": 1, "philip": 1, "thomas": 1, "college": 1, "information": 1, "computer": 1, "sciences": 1, "university": 1, "massachusetts": 1, "amherst": 2, "usa": 1, "fmgarciapthomascsumassedu": 1, "": 1, "abstract": 1, "paper": 1, "consider": 1, "problem": 1, "agent": 1, "task": 1, "solve": 2, "sequence": 2, "problems": 2, "markov": 1, "decision": 1, "process": 1, "use": 1, "knowledge": 1, "acquire": 1, "early": 1, "lifetime": 1, "improve": 1, "ability": 1, "new": 1}, {"argue": 1, "previous": 1, "experience": 1, "similar": 1, "problems": 1, "provide": 1, "agent": 1, "information": 1, "explore": 1, "face": 1, "new": 1, "relate": 1, "problem": 1}, {"show": 1, "search": 1, "optimal": 1, "exploration": 1, "strategy": 2, "formulate": 1, "reinforcement": 1, "learn": 1, "problem": 1, "demonstrate": 1, "leverage": 1, "pattern": 1, "find": 1, "structure": 1, "relate": 1, "problems": 1}, {"conclude": 1, "experiment": 1, "show": 1, "benefit": 1, "optimize": 1, "exploration": 1, "strategy": 1, "use": 1, "propose": 1, "framework": 1}, {"1": 1, "": 2, "introduction": 1, "one": 1, "hallmark": 1, "human": 1, "intelligence": 1, "ability": 1, "leverage": 1, "knowledge": 1, "collect": 1, "lifetimes": 1, "face": 1, "new": 1, "problem": 1}, {"deal": 1, "new": 1, "problem": 2, "relate": 1, "one": 1, "already": 1, "know": 1, "address": 1, "leverage": 1, "experience": 1, "obtain": 1, "solve": 1, "former": 1}, {"example": 1, "upon": 1, "buy": 1, "new": 2, "car": 3, "relearn": 1, "scratch": 1, "drive": 2, "instead": 1, "use": 1, "experience": 1, "previous": 1, "quickly": 1, "adapt": 1, "control": 1, "dynamics": 1}, {"standard": 1, "reinforcement": 1, "learn": 1, "rl": 1, "methods": 1, "lack": 1, "ability": 1}, {"face": 2, "new": 2, "problema": 1, "markov": 1, "decision": 1, "process": 1, "mdpthey": 1, "typically": 1, "start": 1, "learn": 2, "scratch": 1, "initially": 1, "make": 1, "uninformed": 1, "decisions": 1, "order": 1, "explore": 1, "current": 1, "problem": 1}, {"problem": 2, "create": 1, "agents": 1, "leverage": 1, "previous": 1, "experience": 1, "solve": 1, "new": 1, "problems": 1, "call": 1, "lifelong": 1, "learn": 3, "continual": 1, "relate": 1, "transfer": 1}, {"although": 1, "idea": 1, "agent": 2, "learn": 5, "explore": 1, "quite": 1, "time": 1, "14": 1, "15": 1, "paper": 1, "focus": 1, "one": 1, "aspect": 1, "lifelong": 1, "face": 1, "sequence": 1, "mdps": 2, "sample": 1, "distribution": 1, "reinforcement": 1, "optimal": 1, "policy": 1, "exploration": 1}, {"specifically": 1, "consider": 1, "question": 1, "agent": 2, "explore": 2, "much": 1, "well": 1, "study": 1, "area": 1, "reinforcement": 1, "learn": 1, "research": 1, "20": 1, "10": 1, "1": 1, "5": 1, "18": 1}, {"instead": 1, "study": 1, "question": 1, "give": 1, "agent": 1, "decide": 1, "explore": 1, "action": 1, "take": 1}, {"work": 1, "formally": 1, "define": 1, "problem": 2, "search": 1, "optimal": 1, "exploration": 1, "policy": 1, "show": 1, "model": 1, "mdp": 1}, {"mean": 1, "task": 1, "find": 1, "optimal": 1, "exploration": 1, "strategy": 1, "learn": 2, "agent": 4, "solve": 3, "another": 1, "reinforcement": 1, "new": 1, "metamdp": 2, "operate": 1, "different": 1, "timescale": 1, "rl": 2, "specific": 1, "taskone": 1, "episode": 1, "correspond": 1, "entire": 1, "lifetime": 1}, {"difference": 1, "timescales": 1, "distinguish": 1, "approach": 1, "previous": 1, "metamdp": 1, "methods": 1, "optimize": 1, "components": 1, "reinforcement": 1, "learn": 1, "algorithms": 1, "21": 1, "9": 1, "22": 1, "8": 1, "3": 1}, {"33rd": 1, "conference": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "neurips": 1, "2019": 1, "vancouver": 1, "canada": 1}, {"contend": 1, "ignore": 1, "experience": 1, "agent": 1, "might": 1, "relate": 2, "mdps": 1, "miss": 1, "opportunity": 1, "guide": 1, "exploration": 1, "novel": 1, "problems": 1}, {"one": 1, "example": 1, "exploration": 1, "random": 1, "action": 1, "selection": 1, "common": 1, "use": 1, "qlearning": 1, "23": 1, "sarsa": 1, "19": 1, "dqn": 1, "11": 1}, {"address": 1, "limitation": 1, "propose": 1, "separate": 1, "policies": 1, "define": 1, "agents": 1, "behavior": 1, "exploration": 1, "policy": 2, "train": 2, "across": 1, "many": 1, "relate": 1, "mdps": 1, "exploitation": 1, "specific": 1, "mdp": 1}, {"paper": 1, "make": 1, "follow": 1, "contributions": 1, "1": 1, "formally": 1, "define": 1, "problem": 2, "search": 1, "optimal": 1, "exploration": 1, "policy": 1, "2": 1, "prove": 1, "model": 1, "new": 1, "mdp": 1, "describe": 1, "one": 1, "algorithm": 1, "solve": 1, "metamdp": 1, "3": 1, "present": 1, "experimental": 1, "result": 1, "show": 1, "benefit": 1, "approach": 1}, {"although": 1, "search": 1, "optimal": 1, "exploration": 1, "policy": 1, "one": 1, "necessary": 1, "components": 1, "lifelong": 1, "learn": 1, "along": 1, "decide": 1, "explore": 1, "represent": 1, "data": 1, "transfer": 1, "model": 1, "etc": 1}, {"": 1, "provide": 1, "one": 1, "key": 1, "step": 1, "towards": 1, "agents": 1, "leverage": 1, "prior": 1, "knowledge": 1, "solve": 1, "challenge": 1, "problems": 1}, {"code": 1, "use": 1, "paper": 1, "find": 1, "httpsgithubcomfmaxgarciametamdp": 1, "": 3, "2": 1, "relate": 1, "work": 2, "large": 1, "body": 1, "discuss": 1, "problem": 1, "agent": 1, "behave": 1, "exploration": 1, "face": 1, "single": 1, "mdp": 1}, {"simple": 1, "strategies": 1, "greedy": 1, "random": 1, "actionselection": 3, "boltzmann": 1, "softmax": 1, "make": 1, "sense": 1, "agent": 1, "prior": 1, "knowledge": 1, "problem": 1, "currently": 1, "try": 1, "solve": 1}, {"wellknown": 1, "fact": 1, "performance": 2, "agent": 1, "explore": 1, "unguided": 1, "exploration": 1, "techniques": 1, "random": 1, "actionselection": 2, "reduce": 1, "drastically": 1, "size": 1, "statespace": 1, "increase": 1, "24": 1, "example": 1, "boltzmann": 1, "softmax": 1, "hinge": 1, "accuracy": 1, "actionvalue": 1, "estimate": 1}, {"estimate": 1, "poor": 1, "eg": 1, "early": 1, "learn": 2, "process": 1, "drastic": 1, "negative": 1, "effect": 1, "overall": 1, "ability": 1, "agent": 1}, {"give": 1, "limitation": 1, "unguided": 1, "technique": 1, "information": 1, "available": 1, "guide": 1, "agents": 1, "exploration": 1, "strategy": 1, "exploit": 1}, {"exist": 1, "sophisticate": 1, "methods": 1, "exploration": 1, "example": 1, "possible": 1, "use": 1, "statevisitation": 1, "count": 1, "encourage": 1, "agent": 1, "explore": 1, "state": 1, "frequently": 1, "visit": 1, "20": 1, "10": 1}, {"recent": 1, "research": 1, "also": 1, "show": 1, "add": 1, "exploration": 2, "bonus": 2, "reward": 1, "function": 1, "effective": 1, "way": 1, "improve": 1, "vime": 1, "6": 1, "take": 2, "bayesian": 1, "approach": 1, "maintain": 1, "model": 3, "dynamics": 1, "environment": 1, "obtain": 1, "posterior": 1, "action": 1, "use": 1, "kl": 1, "divergence": 1, "two": 1}, {"intuition": 1, "behind": 1, "approach": 1, "encourage": 1, "action": 1, "make": 1, "large": 1, "update": 1, "model": 2, "allow": 1, "agent": 1, "better": 1, "explore": 1, "areas": 1, "current": 1, "inaccurate": 1}, {"12": 1, "define": 1, "bonus": 1, "reward": 2, "function": 1, "add": 1, "intrinsic": 1}, {"propose": 1, "use": 1, "neural": 1, "network": 1, "predict": 1, "state": 1, "transition": 1, "base": 1, "action": 1, "take": 1, "provide": 1, "intrinsic": 1, "reward": 1, "proportional": 1, "prediction": 1, "error": 1}, {"agent": 1, "therefore": 1, "encourage": 1, "make": 1, "state": 1, "transition": 1, "model": 1, "accurately": 1}, {"another": 1, "relevant": 1, "work": 1, "exploration": 1, "present": 1, "3": 1, "author": 1, "propose": 1, "build": 1, "library": 1, "policies": 1, "prior": 1, "experience": 1, "explore": 1, "environment": 1, "new": 1, "problems": 1, "efficiently": 1}, {"techniques": 1, "efficient": 1, "agent": 2, "deal": 1, "single": 1, "mdp": 1, "however": 1, "face": 1, "new": 1, "problem": 1, "ignore": 1, "potentially": 1, "useful": 1, "information": 1, "may": 1, "discover": 1, "solve": 1, "previous": 1, "task": 1}, {"fail": 1, "leverage": 1, "prior": 1, "experience": 1}, {"aim": 1, "address": 1, "limitation": 1, "exploit": 1, "exist": 1, "knowledge": 1, "specifically": 1, "exploration": 1}, {"take": 1, "metalearning": 3, "approach": 1, "metaagent": 1, "learn": 2, "policy": 1, "use": 1, "guide": 1, "rl": 2, "agent": 1, "whenever": 1, "decide": 1, "explore": 1, "contrast": 1, "performance": 1, "method": 2, "model": 1, "agnostic": 1, "maml": 1, "stateoftheart": 1, "general": 1, "show": 1, "capable": 1, "speed": 1, "task": 1, "4": 1}, {"3": 1, "": 7, "background": 1, "markov": 1, "decision": 1, "process": 1, "mdp": 1, "tuple": 1, "p": 2, "r": 1, "d0": 2, "set": 2, "possible": 2, "state": 6, "environment": 2, "action": 3, "agent": 2, "take": 3, "s0": 4, "probability": 1, "transition": 2, "2": 3, "rs": 1, "function": 1, "denote": 1, "reward": 1, "receive": 1, "initial": 1, "distribution": 1}, {"use": 1, "2": 2, "0": 1, "1": 1, "": 1}, {"": 1}, {"": 1}, {"": 5, "index": 1, "timestep": 1, "write": 1, "st": 1, "rt": 1, "denote": 1, "state": 1, "action": 1, "reward": 2, "time": 2, "also": 1, "consider": 1, "undiscounted": 1, "episodic": 1, "set": 1, "wherein": 1, "discount": 1, "base": 1, "2": 1, "occur": 1}, {"assume": 1, "": 1, "maximum": 1, "time": 1, "step": 1, "finite": 1, "thus": 1, "restrict": 1, "discussion": 1, "episodic": 1, "mdps": 1}, {"use": 1, "denote": 1, "total": 1, "number": 1, "episodes": 1, "agent": 1, "interact": 1, "environment": 1}, {"policy": 1, "": 4}, {"0": 1, "1": 1, "provide": 1, "conditional": 1, "distribution": 1, "action": 1, "give": 1, "possible": 1, "state": 1, "": 3, "prat": 1, "ast": 1}, {"furthermore": 1, "assume": 1, "policies": 1, "": 1, "task": 1, "c": 2, "2": 1, "define": 1, "later": 1, "expect": 1, "return": 1, "normalize": 1, "interval": 1, "0": 1, "1": 1}, {"one": 2, "key": 1, "challenge": 1, "within": 1, "rl": 1, "work": 1, "focus": 1, "relate": 1, "explorationexploitation": 1, "dilemma": 1}, {"ensure": 1, "agent": 1, "able": 1, "find": 1, "good": 1, "policy": 1, "act": 1, "sole": 1, "purpose": 1, "gather": 1, "information": 1, "environment": 1, "exploration": 1}, {"however": 1, "enough": 1, "information": 1, "gather": 1, "behave": 1, "accord": 1, "believe": 1, "best": 1, "policy": 1, "exploitation": 1}, {"work": 1, "separate": 1, "behavior": 1, "rl": 1, "agent": 1, "two": 1, "distinct": 1, "policies": 1, "exploration": 1, "policy": 2, "exploitation": 1}, {"assume": 1, "greedy": 1, "exploration": 2, "schedule": 1, "ie": 1, "probability": 2, "agent": 2, "explore": 1, "1": 2, "exploit": 1, "ii1": 1, "sequence": 1, "rat": 1, "2": 1, "0": 1, "refer": 1, "episode": 1, "number": 1, "current": 1, "task": 1}, {"note": 1, "sophisticate": 1, "decisions": 1, "explore": 1, "certainly": 1, "possible": 1, "could": 1, "exploit": 1, "propose": 1, "method": 1}, {"assume": 2, "exploration": 1, "strategy": 1, "agent": 2, "forgo": 1, "ability": 1, "learn": 1, "explore": 2, "decision": 1, "whether": 1, "random": 1}, {"say": 1, "greedy": 1, "currently": 1, "widely": 1, "use": 1, "egsarsa": 1, "19": 1, "qlearning": 1, "23": 1, "dqn": 1, "11": 1, "popularity": 1, "make": 1, "study": 1, "still": 1, "relevant": 1, "today": 1}, {"let": 1, "c": 2, "set": 1, "task": 1, "": 4, "pc": 1, "rc": 1, "dc0": 1}, {"c": 2, "2": 1, "mdps": 1, "share": 1, "stateset": 1, "action": 1, "set": 1, "may": 1, "different": 1, "transition": 1, "function": 2, "pc": 1, "": 3, "reward": 1, "rc": 1, "initial": 1, "state": 1, "distributions": 1, "dc0": 1}, {"agent": 1, "require": 1, "solve": 1, "set": 2, "task": 1, "c": 3, "2": 1, "refer": 1, "problem": 1, "class": 1}, {"give": 1, "task": 2, "separate": 1, "mdp": 1, "exploitation": 1, "policy": 1, "might": 1, "directly": 1, "apply": 1, "novel": 1}, {"fact": 1, "could": 1, "hinder": 1, "agents": 1, "ability": 1, "learn": 1, "appropriate": 1, "policy": 1}, {"type": 1, "scenarios": 1, "arise": 1, "example": 1, "control": 1, "problems": 1, "policy": 1, "learn": 1, "one": 1, "specific": 1, "agent": 1, "work": 1, "another": 1, "due": 1, "differences": 1, "environment": 1, "dynamics": 1, "physical": 1, "properties": 1}, {"concrete": 1, "example": 1, "intelligent": 1, "control": 1, "flight": 1, "systems": 1, "icfs": 1, "area": 1, "study": 1, "bear": 1, "necessity": 1, "address": 1, "limitations": 1, "pid": 1, "controllers": 1, "rl": 1, "gain": 1, "significant": 1, "traction": 1, "recent": 1, "years": 1, "26": 1, "27": 1}, {"one": 1, "particular": 1, "scenario": 1, "propose": 1, "problem": 1, "would": 3, "arise": 1, "use": 1, "rl": 1, "control": 2, "autonomous": 1, "vehicles": 2, "7": 1, "single": 1, "policy": 2, "likely": 1, "work": 1, "number": 1, "distinct": 1, "need": 1, "adapt": 1, "specifics": 1, "vehicle": 1}, {"framework": 1, "agent": 1, "taskspecific": 1, "policy": 1, "": 1, "update": 1, "agents": 1, "learn": 1, "algorithm": 1}, {"policy": 2, "define": 1, "agents": 1, "behavior": 1, "exploitation": 2, "refer": 1}, {"behavior": 1, "agent": 1, "exploration": 1, "determine": 1, "advisor": 1, "maintain": 1, "policy": 1, "": 1, "tailor": 1, "problem": 1, "class": 1, "ie": 1, "share": 1, "across": 1, "task": 1, "c": 1}, {"refer": 1, "policy": 2, "exploration": 1}, {"agent": 1, "give": 1, "k": 1, "": 1, "timesteps": 1, "interactions": 1, "sample": 1, "task": 1}, {"hereafter": 1, "use": 1, "denote": 3, "index": 1, "current": 3, "episode": 2, "task": 2, "time": 3, "step": 3, "within": 1, "k": 3, "number": 1, "pass": 1, "ie": 1, "": 2, "refer": 1, "advisor": 1}, {"every": 1, "timestep": 1, "k": 1, "advisor": 1, "suggest": 1, "action": 1, "uk": 2, "": 2, "agent": 1, "sample": 1, "accord": 1}, {"agent": 1, "decide": 1, "explore": 1, "step": 1, "take": 2, "action": 2, "uk": 1, "": 2, "otherwise": 1, "ak": 1, "sample": 1, "accord": 1, "agents": 1, "policy": 1}, {"refer": 1, "optimal": 2, "policy": 2, "agent": 1, "solve": 1, "specific": 1, "task": 1, "c": 3, "2": 1, "exploitation": 1, "": 1}, {"formally": 1, "c": 2, "2": 1, "argmax": 1, "e": 1, "g": 2, "": 2, "pt": 1, "t0": 1, "rt": 1, "refer": 1, "return": 1}, {"thus": 1, "agent": 1, "solve": 1, "specific": 1, "task": 1, "optimize": 1, "standard": 1, "expect": 1, "return": 1, "objective": 1}, {"refer": 1, "agent": 3, "solve": 1, "specific": 1, "task": 1, "even": 1, "though": 1, "advisor": 1, "also": 1, "view": 1}, {"consider": 1, "process": 1, "task": 2, "c": 3, "2": 1, "sample": 1, "distribution": 1, "dc": 1, "": 1, "rl": 1, "agent": 2, "learn": 1, "solve": 1, "advisor": 1, "also": 1, "update": 1, "policy": 1, "guide": 1, "exploration": 1}, {"whenever": 1, "agent": 1, "decide": 1, "explore": 1, "use": 1, "action": 1, "provide": 1, "advisor": 1, "accord": 1, "policy": 1, "": 1}, {"4": 1, "": 7, "problem": 1, "statement": 1, "weh": 1, "define": 1, "performance": 1, "advisors": 1, "policy": 1, "specific": 1, "task": 1, "c": 4, "2": 1, "pi": 1, "pt": 1, "th": 1, "e": 1, "i0": 1, "t0": 1, "rt": 2, "reward": 1, "time": 1, "step": 1, "episode": 1}, {"let": 1, "c": 1, "random": 1, "variable": 1, "denote": 1, "task": 1, "sample": 1, "dc": 1, "": 1}, {"goal": 1, "advisor": 1, "find": 1, "optimal": 1, "3": 1, "": 7, "exploration": 1, "policy": 2, "define": 1, "satisfy": 1, "2": 1, "arg": 1, "max": 1, "e": 1, "c": 1}, {"": 3, "1": 1, "intuitive": 1, "term": 1, "objective": 1, "seek": 1, "maximize": 1, "area": 1, "learn": 1, "curve": 1, "agent": 1}, {"assume": 1, "stable": 1, "policy": 2, "": 1, "whose": 1, "performance": 2, "improve": 1, "train": 1, "collapse": 1, "maximize": 1, "objective": 1, "imply": 1, "agent": 1, "able": 1, "learn": 1, "quickly": 1}, {"single": 1, "policy": 3, "solve": 2, "every": 1, "task": 2, "metaagent": 1, "learn": 2, "help": 1, "agent": 1, "obtain": 1, "optimal": 1, "particular": 1}, {"unfortunately": 1, "cannot": 1, "directly": 1, "optimize": 1, "objective": 1, "know": 1, "transition": 1, "reward": 1, "function": 1, "mdp": 1, "sample": 1, "task": 1, "dc": 1, "": 1}, {"next": 1, "section": 1, "show": 1, "search": 1, "exploration": 1, "policy": 1, "formulate": 1, "rl": 2, "problem": 1, "advisor": 1, "agent": 2, "solve": 2, "mdp": 1, "whose": 1, "environment": 1, "contain": 1, "current": 2, "task": 2, "c": 1}, {"5": 1, "": 2, "general": 1, "solution": 1, "framework": 2, "view": 1, "metamdp": 1, "mdp": 2, "within": 1}, {"point": 1, "view": 1, "agent": 1, "environment": 1, "current": 1, "task": 1, "c": 1, "mdp": 1}, {"however": 1, "point": 1, "view": 1, "advisor": 1, "environment": 1, "contain": 1, "task": 1, "c": 1, "agent": 1}, {"every": 1, "timestep": 1, "advisor": 1, "select": 1, "action": 2, "u": 1, "agent": 1}, {"select": 1, "action": 3, "go": 1, "selection": 1, "mechanism": 1, "execute": 1, "probability": 2, "1": 1, "u": 1, "episode": 1}, {"formulation": 1, "point": 1, "view": 1, "advisor": 1, "action": 1, "u": 1, "always": 1, "execute": 1, "selection": 1, "mechanism": 1, "simply": 1, "another": 1, "source": 1, "uncertainty": 1, "environment": 1}, {"figure": 1, "1": 1, "depict": 1, "propose": 1, "framework": 1, "action": 1, "exploitation": 1, "select": 1}, {"even": 1, "though": 1, "one": 3, "time": 2, "step": 2, "agent": 2, "correspond": 1, "advisor": 2, "episode": 1, "constitute": 1, "lifetime": 1}, {"perspective": 1, "wherein": 1, "advisor": 1, "merely": 1, "another": 1, "reinforcement": 1, "learn": 1, "algorithm": 1, "take": 1, "advantage": 1, "exist": 1, "body": 1, "work": 1, "rl": 1, "optimize": 1, "exploration": 1, "policy": 1, "": 1}, {"figure": 1, "1": 1, "mdp": 1, "view": 1, "interaction": 1, "advisor": 1, "agent": 1}, {"timestep": 1, "advisor": 1, "select": 1, "action": 2, "u": 1, "agent": 1}, {"probability": 2, "": 2, "agent": 1, "execute": 2, "action": 2, "u": 1, "1": 1}, {"action": 1, "agent": 2, "advisor": 2, "receive": 1, "reward": 1, "r": 1, "environment": 1, "transition": 1, "state": 1, "x": 1, "respectively": 1}, {"experiment": 1, "train": 1, "advisor": 1, "policy": 2, "use": 1, "two": 1, "different": 1, "rl": 1, "algorithms": 1, "reinforce": 1, "25": 1, "proximal": 1, "optimization": 1, "ppo": 1, "17": 1}, {"use": 1, "montercarlo": 1, "methods": 1, "reinforce": 1, "result": 1, "simpler": 1, "implementation": 1, "expense": 1, "large": 1, "computation": 1, "time": 1, "update": 1, "advisor": 1, "would": 1, "require": 1, "train": 1, "agent": 1, "entire": 1, "lifetime": 1}, {"hand": 1, "use": 1, "temporal": 1, "difference": 1, "method": 1, "ppo": 1, "overcome": 1, "computational": 1, "bottleneck": 1, "expense": 1, "larger": 1, "variance": 1, "performance": 1, "advisor": 1}, {"pseudocode": 1, "implementations": 1, "use": 2, "framework": 1, "reinforce": 1, "ppo": 1, "show": 2, "appendix": 1, "c": 1, "51": 1, "theoretical": 1, "result": 1, "formally": 1, "define": 1, "metamdp": 2, "face": 1, "advisor": 1, "optimal": 1, "policy": 1, "optimize": 1, "objective": 1, "1": 1}, {"recall": 1, "rc": 1, "": 2, "pc": 1, "dc0": 1, "denote": 1, "reward": 1, "function": 2, "transition": 1, "initial": 1, "state": 1, "distribution": 1, "mdp": 1, "c": 2, "2": 1, "formally": 1, "describe": 1, "metamdp": 1, "must": 1, "capture": 1, "property": 1, "agent": 1, "implement": 1, "arbitrary": 1, "rl": 1, "algorithm": 1}, {"assume": 1, "agent": 1, "maintain": 1, "memory": 2, "mk": 3, "": 2, "update": 1, "learn": 1, "rule": 1, "l": 1, "rl": 1, "algorithm": 1, "time": 1, "step": 1, "write": 1, "denote": 1, "agents": 1, "policy": 1, "give": 1}, {"word": 1, "mk": 2, "provide": 1, "information": 1, "need": 1, "determine": 1, "update": 2, "form": 1, "mk1": 1, "": 7, "lmk": 1, "sk": 1, "ak": 1, "rk": 1, "sk1": 1, "rule": 1, "represent": 1, "popular": 1, "rl": 1, "algorithms": 1, "4": 1, "like": 1, "qlearning": 1, "actorcritics": 1}, {"make": 1, "assumptions": 1, "learn": 2, "algorithm": 1, "agent": 1, "use": 2, "eg": 1, "sarsa": 1, "qlearning": 1, "reinforce": 1, "even": 1, "batch": 1, "methods": 1, "like": 1, "fit": 1, "qiteration": 1, "consider": 1, "rule": 1, "unknown": 1, "source": 1, "uncertainty": 1}, {"proposition": 1, "1": 1}, {"consider": 1, "advisor": 1, "policy": 1, "": 5, "episodic": 1, "task": 1, "c": 3, "2": 1, "belong": 1, "problem": 2, "class": 1, "learn": 1, "formulate": 1, "mdp": 1, "mmeta": 1, "x": 2, "u": 2, "d00": 2, "state": 2, "space": 2, "action": 1, "transition": 1, "function": 2, "reward": 1, "initial": 1, "distribution": 1}, {"proof": 1}, {"see": 1, "appendix": 1, "": 2, "give": 1, "formulate": 1, "metamdp": 1, "mmeta": 1, "able": 1, "show": 1, "optimal": 2, "policy": 2, "new": 1, "mdp": 1, "correspond": 1, "exploration": 1}, {"theorem": 1, "1": 1}, {"hoptimal": 1, "policyi": 1, "mmeta": 1, "optimal": 1, "exploration": 1, "policy": 1, "": 2, "define": 1, "1": 1}, {"pk": 1, "e": 2, "": 4, "c": 1, "k0": 1, "yk": 1}, {"proof": 1}, {"see": 1, "appendix": 1, "b": 1}, {"since": 1, "mmeta": 1, "mdp": 1, "optimal": 3, "exploration": 2, "policy": 3, "follow": 1, "convergence": 1, "properties": 1, "reinforcement": 1, "learn": 1, "algorithms": 1, "apply": 1, "search": 1}, {"example": 1, "experiment": 1, "advisor": 1, "use": 1, "reinforce": 1, "algorithm": 1, "25": 1, "convergence": 1, "properties": 1, "wellstudied": 1, "13": 1}, {"although": 1, "conceptually": 1, "simple": 1, "framework": 1, "present": 1, "thus": 1, "far": 1, "may": 1, "require": 1, "solve": 1, "large": 1, "number": 1, "task": 1, "episodes": 1, "metamdp": 1, "one": 1, "potentially": 1, "expensive": 1, "procedure": 1}, {"address": 1, "issue": 1, "sample": 1, "small": 1, "number": 1, "task": 1, "c1": 1, "": 2}, {"": 1}, {"": 1}, {"": 3, "cn": 1, "ci": 1, "dc": 1, "train": 1, "many": 1, "episodes": 1, "task": 1, "parallel": 1}, {"take": 1, "approach": 2, "every": 1, "update": 1, "advisor": 1, "influence": 1, "several": 1, "simultaneous": 1, "task": 1, "result": 1, "scalable": 1, "obtain": 1, "general": 1, "exploration": 1, "policy": 1}, {"difficult": 1, "task": 1, "might": 1, "require": 1, "agent": 2, "train": 2, "long": 1, "time": 1, "use": 1, "td": 1, "techniques": 1, "allow": 1, "advisor": 1, "improve": 1, "policy": 1, "still": 1}, {"6": 1, "": 2, "empirical": 1, "result": 1, "section": 1, "present": 1, "experiment": 1, "discrete": 1, "continuous": 1, "control": 1, "task": 1}, {"figure": 1, "8a": 1, "8b": 1, "depict": 1, "task": 1, "variations": 1, "animat": 1, "case": 1, "discrete": 1, "action": 1, "set": 1}, {"figure": 1, "11a": 1, "11b": 1, "show": 1, "task": 1, "variations": 1, "ant": 1, "problem": 1, "case": 1, "continuous": 1, "action": 1, "set": 1}, {"implementations": 1, "use": 1, "discrete": 1, "case": 1, "polebalancing": 1, "continuous": 1, "control": 1, "problems": 1, "take": 1, "openai": 1, "gym": 1, "roboschool": 1, "benchmarks": 1, "2": 1}, {"drive": 1, "task": 1, "experiment": 1, "use": 1, "simulator": 1, "implement": 1, "unity": 1, "tawn": 1, "kramer": 1, "donkey": 1, "car": 1, "community": 1, "1": 1, "": 1}, {"demonstrate": 1, "1": 1, "practice": 1, "metamdp": 1, "mmeta": 1, "": 1, "solve": 1, "use": 1, "exist": 2, "reinforcement": 1, "learn": 4, "methods": 2, "2": 2, "exploration": 3, "policy": 5, "advisor": 3, "improve": 1, "performance": 1, "rl": 1, "average": 1, "3": 1, "differ": 1, "optimal": 1, "exploitation": 2, "task": 1, "c": 2, "ie": 1, "necessarily": 1, "good": 1}, {"intuitively": 1, "method": 1, "work": 1, "well": 1, "common": 1, "pattern": 1, "across": 1, "task": 1, "action": 1, "take": 1, "give": 1, "state": 1}, {"example": 1, "simple": 1, "gridworld": 1, "method": 1, "would": 1, "able": 2, "learn": 2, "good": 1, "exploration": 1, "policy": 2, "case": 1, "animat": 1, "show": 1, "figure": 1, "8a": 1, "8b": 1, "metaagent": 1, "certain": 1, "action": 1, "pattern": 1, "never": 1, "lead": 1, "optimal": 1}, {"animat": 1, "task": 1, "1": 1}, {"b": 1, "animat": 1, "task": 1, "2": 1}, {"c": 1, "ant": 1, "task": 1, "1": 1}, {"ant": 1, "task": 1, "2": 1}, {"figure": 1, "2": 1, "example": 1, "task": 1, "variations": 1}, {"problem": 1, "class": 1, "correspond": 1, "animat": 1, "leave": 1, "discrete": 1, "action": 2, "space": 2, "ant": 1, "right": 1, "continuous": 1}, {"1": 1, "": 3, "unity": 1, "simulator": 1, "selfdriving": 2, "task": 1, "find": 1, "httpsgithubcomtawnkramer": 1, "sdsandbox": 1, "5": 1, "show": 1, "algorithm": 1, "behave": 1, "desire": 1, "first": 1, "study": 1, "behavior": 1, "method": 1, "two": 1, "simple": 1, "problem": 1, "class": 1, "discrete": 1, "actionspaces": 1, "polebalancing": 1, "19": 1, "animat": 1, "21": 1, "realistic": 1, "application": 1, "control": 1, "tune": 1, "vehicles": 1}, {"baseline": 1, "metalearning": 1, "method": 2, "contrast": 1, "framework": 1, "choose": 1, "model": 1, "agnostic": 1, "meta": 2, "learn": 2, "maml": 1, "4": 1, "general": 1, "adapt": 1, "previously": 1, "train": 1, "neural": 1, "network": 1, "novel": 1, "relate": 1, "task": 1}, {"worth": 1, "note": 1, "although": 1, "method": 1, "specifically": 1, "design": 1, "rl": 1, "author": 1, "describe": 1, "promise": 1, "result": 1, "adapt": 1, "behavior": 1, "learn": 1, "previous": 1, "task": 1, "novel": 1, "ones": 1}, {"61": 1, "empirical": 1, "evaluation": 2, "propose": 1, "framework": 1, "begin": 1, "assess": 1, "behavior": 1, "algorithm": 1, "two": 1, "different": 1, "problems": 1, "discrete": 1, "action": 1, "space": 1, "polebalancing": 1, "animat": 1}, {"choose": 1, "problems": 1, "present": 1, "structural": 1, "pattern": 1, "intuitive": 1, "understand": 1, "exploit": 1, "agent": 1}, {"polebalancing": 1, "agent": 1, "task": 1, "apply": 1, "force": 1, "cart": 1, "prevent": 1, "pole": 1, "balance": 1, "fall": 1}, {"distinct": 1, "task": 1, "construct": 1, "modify": 1, "length": 1, "mass": 3, "pole": 1, "cart": 1, "force": 1, "magnitude": 1}, {"state": 1, "represent": 1, "4d": 1, "vectors": 1, "describe": 1, "": 4, "position": 1, "velocity": 2, "cart": 1, "angle": 1, "angular": 1, "pendulum": 1, "ie": 1, "x": 1, "v": 1}, {"agent": 1, "2": 1, "action": 1, "disposal": 1, "apply": 1, "force": 1, "positive": 1, "negative": 1, "x": 1, "direction": 1}, {"figure": 1, "3a": 1, "contrast": 1, "cumulative": 1, "return": 1, "agent": 1, "use": 1, "advisor": 1, "random": 1, "exploration": 1, "train": 1, "6": 1, "task": 1, "show": 1, "blue": 1, "red": 1, "respectively": 1}, {"policies": 1, "": 5, "train": 1, "use": 1, "reinforce": 1, "1000": 1, "episodes": 1, "500": 1, "iterations": 1}, {"figure": 1, "horizontal": 1, "axis": 1, "correspond": 1, "episodes": 1, "advisor": 1}, {"horizontal": 1, "red": 1, "line": 1, "denote": 1, "estimate": 1, "standard": 1, "error": 1, "bar": 1, "expect": 1, "cumulative": 1, "reward": 1, "agents": 1, "lifetime": 1, "sample": 1, "action": 1, "uniformly": 1, "explore": 1}, {"notice": 1, "function": 1, "train": 1, "iteration": 1, "random": 1, "exploration": 1, "update": 1}, {"blue": 1, "curve": 1, "standard": 1, "error": 1, "bar": 1, "15": 1, "trials": 1, "show": 1, "expect": 1, "cumulative": 1, "reward": 1, "agent": 1, "obtain": 1, "lifetime": 1, "change": 1, "advisor": 1, "improve": 1, "policy": 1}, {"advisor": 1, "train": 1, "agent": 1, "obtain": 1, "roughly": 1, "30": 1, "reward": 1, "lifetime": 1, "use": 1, "random": 1, "exploration": 1}, {"visualize": 1, "difference": 1, "figure": 1, "3b": 1, "show": 1, "mean": 1, "learn": 1, "curve": 1, "episodes": 1, "agents": 1, "lifetime": 1, "horizontal": 1, "axis": 2, "average": 1, "return": 1, "episode": 1, "vertical": 1, "first": 1, "last": 1, "50": 1, "iterations": 1}, {"performance": 1, "curve": 1, "train": 1, "compare": 1, "advisor": 1, "policy": 2, "blue": 1, "random": 1, "exploration": 1, "red": 1}, {"b": 1, "average": 1, "learn": 1, "curve": 1, "train": 1, "task": 1, "first": 1, "50": 2, "advisor": 2, "episodes": 2, "blue": 1, "last": 1, "orange": 1}, {"animat": 1, "environments": 1, "agent": 1, "circular": 1, "creature": 1, "live": 1, "continuous": 1, "state": 1, "space": 1}, {"8": 1, "independent": 1, "actuators": 1, "angle": 1, "around": 1, "increments": 1, "45": 1, "degrees": 1}, {"actuator": 1, "either": 1, "time": 1, "step": 1, "action": 2, "set": 1, "0": 1, "18": 1, "": 1, "total": 1, "256": 1}, {"actuator": 1, "produce": 1, "small": 1, "force": 1, "direction": 1, "point": 1}, {"result": 2, "action": 1, "move": 1, "agent": 1, "direction": 1, "force": 1, "perturb": 1, "0mean": 1, "unit": 1, "variance": 1, "gaussian": 1, "noise": 1}, {"agent": 1, "task": 1, "move": 1, "goal": 2, "location": 1, "receive": 1, "reward": 2, "1": 1, "timestep": 1, "100": 1, "state": 1}, {"different": 2, "variations": 1, "task": 1, "correspond": 1, "randomize": 1, "start": 1, "goal": 1, "position": 1, "environments": 1}, {"figure": 1, "4a": 1, "show": 1, "clear": 1, "performance": 1, "improvement": 1, "average": 1, "advisor": 1, "improve": 1, "policy": 1, "50": 1, "train": 1, "iterations": 1}, {"curve": 2, "show": 2, "average": 1, "obtain": 1, "first": 1, "last": 1, "10": 1, "iteration": 1, "train": 1, "advisor": 1, "blue": 1, "orange": 1, "respectively": 1}, {"individual": 1, "task": 1, "train": 1, "": 1, "800": 1, "episodes": 1}, {"interest": 1, "pattern": 1, "share": 1, "across": 1, "variations": 1, "problem": 1, "class": 1, "actuator": 1, "combinations": 1, "useful": 1, "reach": 1, "goal": 1}, {"example": 1, "activate": 1, "actuators": 1, "opposite": 1, "6": 1, "": 1, "angle": 1, "would": 1, "leave": 1, "agent": 1, "position": 1, "ignore": 1, "effect": 1, "noise": 1}, {"presence": 1, "poor": 1, "perform": 1, "action": 1, "provide": 1, "common": 1, "pattern": 1, "leverage": 1}, {"test": 2, "intuition": 1, "exploration": 3, "policy": 2, "would": 1, "exploit": 1, "presence": 1, "poorperforming": 1, "action": 1, "record": 1, "frequency": 1, "execute": 1, "unseen": 1, "task": 2, "use": 2, "learn": 1, "train": 1, "random": 1, "strategy": 1, "5": 1, "different": 1}, {"figure": 1, "4b": 1, "help": 1, "explain": 1, "improvement": 1, "performance": 1}, {"depict": 1, "yaxis": 1, "percentage": 1, "time": 1, "poorperforming": 1, "action": 1, "select": 1, "give": 1, "episode": 2, "xaxis": 1, "agent": 1, "number": 1, "current": 1, "task": 1}, {"agent": 1, "use": 1, "advisor": 1, "policy": 1, "blue": 1, "encourage": 1, "reduce": 1, "selection": 1, "know": 1, "poorperforming": 1, "action": 1, "compare": 1, "random": 1, "actionselection": 1, "exploration": 1, "strategy": 1, "red": 1}, {"animat": 1, "result": 1, "average": 1, "learn": 1, "curve": 1, "train": 1, "task": 1, "first": 1, "10": 2, "iterations": 2, "blue": 1, "last": 1, "orange": 1}, {"b": 1, "animat": 1, "result": 1, "frequency": 1, "poorperforming": 1, "action": 1, "agents": 1, "lifetime": 1, "learn": 1, "blue": 1, "random": 1, "red": 1, "exploration": 1}, {"vehicle": 1, "control": 2, "pragmatic": 1, "application": 1, "framework": 1, "quickly": 1, "adapt": 1, "policy": 1, "one": 1, "system": 1, "another": 1}, {"experiment": 1, "test": 1, "advisor": 1, "control": 1, "problem": 1, "use": 1, "selfdriving": 1, "car": 1, "simulator": 1, "implement": 1, "unity": 1}, {"assume": 1, "agent": 1, "constant": 1, "acceleration": 1, "maximum": 1, "velocity": 1, "action": 1, "consist": 1, "15": 1, "possible": 1, "steer": 1, "angle": 2, "min": 1, "": 2, "0": 2, "max": 1}, {"state": 1, "represent": 1, "stack": 1, "last": 1, "4": 1, "80": 2, "": 2, "image": 1, "sense": 1, "frontfacing": 1, "camera": 1, "task": 1, "vary": 1, "body": 1, "mass": 1, "car": 1, "value": 1, "min": 1, "max": 1}, {"test": 1, "ability": 1, "advisor": 1, "improve": 1, "finetuning": 1, "control": 1, "specific": 1, "cars": 1}, {"first": 1, "learn": 1, "wellperforming": 1, "policy": 2, "one": 1, "car": 1, "use": 1, "start": 1, "point": 1, "finetune": 1, "policies": 1, "8": 1, "different": 1, "cars": 1}, {"experiment": 1, "depict": 1, "figure": 1, "5": 1, "compare": 1, "agent": 2, "able": 1, "use": 1, "advisor": 2, "exploration": 1, "finetuning": 1, "blue": 1, "vs": 1, "access": 1, "red": 1}, {"figure": 1, "show": 1, "number": 1, "episodes": 1, "finetuning": 1, "need": 1, "reach": 1, "predefined": 1, "performance": 1, "threshold": 1, "1": 1, "000": 1, "timesteps": 1, "without": 1, "leave": 1, "correct": 1, "lane": 1}, {"first": 2, "second": 2, "group": 1, "figure": 1, "show": 1, "average": 1, "number": 1, "episodes": 1, "need": 1, "finetune": 1, "half": 1, "task": 1, "respectively": 1}, {"first": 1, "half": 1, "task": 1, "leave": 1, "advisor": 1, "seem": 1, "make": 1, "finetuning": 1, "difficult": 1, "since": 1, "train": 1, "deal": 1, "specific": 1, "problem": 1}, {"use": 1, "advisor": 1, "take": 2, "average": 2, "42": 1, "episodes": 3, "figure": 1, "5": 1, "number": 1, "need": 1, "achieve": 1, "finetune": 1, "12": 1, "threshold": 1, "performance": 1, "lower": 1, "better": 1}, {"finetune": 1, "without": 1}, {"benefit": 1, "however": 1, "see": 1, "second": 1, "half": 1, "train": 1, "task": 1}, {"advisor": 2, "train": 1, "take": 1, "average": 2, "5": 1, "episodes": 2, "finetune": 1, "use": 1, "need": 1, "18": 1, "reach": 1, "require": 1, "performance": 1, "threshold": 1}, {"number": 1, "task": 1, "large": 1, "enough": 1, "episode": 1, "timeconsuming": 1, "costly": 1, "process": 1, "framework": 1, "could": 1, "result": 1, "important": 1, "time": 1, "cost": 1, "save": 1}, {"62": 1, "exploration": 1, "policy": 2, "simply": 1, "general": 1, "exploitation": 1}, {"one": 1, "might": 2, "tempt": 1, "think": 1, "learn": 1, "policy": 2, "exploration": 1, "simply": 1, "work": 1, "well": 1, "general": 1}, {"know": 1, "advisor": 1, "learn": 1, "policy": 2, "exploration": 1, "simply": 1, "exploitation": 1}, {"answer": 1, "question": 1, "generate": 1, "three": 1, "distinct": 1, "unseen": 1, "task": 2, "7": 1, "": 1, "polebalancing": 1, "animat": 1, "problem": 1, "class": 1, "compare": 1, "performance": 2, "use": 1, "learn": 1, "exploration": 1, "policy": 2, "obtain": 1, "exploitation": 1, "train": 1, "solve": 1, "specific": 1}, {"figure": 1, "6": 1, "show": 1, "two": 1, "bar": 1, "chart": 1, "contrast": 1, "performance": 1, "exploration": 1, "policy": 2, "blue": 1, "exploitation": 1, "green": 1, "task": 1, "variation": 1}, {"chart": 1, "first": 1, "three": 1, "group": 1, "bar": 1, "leave": 1, "correspond": 1, "performance": 1, "task": 2, "last": 1, "one": 1, "average": 1}, {"figure": 1, "6a": 1, "correspond": 1, "mean": 1, "performance": 1, "polebalancing": 1, "error": 1, "bar": 1, "standard": 1, "deviation": 1, "yaxis": 1, "denote": 1, "return": 1, "obtain": 1}, {"see": 1, "expect": 1, "exploration": 1, "policy": 2, "fail": 1, "achieve": 1, "comparable": 1, "performance": 1, "taskspecific": 1}, {"occur": 1, "animat": 1, "problem": 1, "class": 1, "show": 1, "figure": 1, "6b": 1}, {"case": 1, "yaxis": 1, "refer": 1, "number": 1, "step": 1, "need": 1, "reach": 1, "goal": 1, "smaller": 1, "bar": 1, "better": 1}, {"case": 1, "taskspecific": 1, "policy": 4, "perform": 1, "significantly": 1, "better": 1, "learn": 1, "exploration": 2, "indicate": 1, "general": 1, "exploitation": 1}, {"average": 1, "return": 1, "obtain": 1, "test": 2, "task": 2, "use": 1, "advisors": 1, "exploration": 1, "policy": 2, "blue": 2, "taskspecific": 1, "exploitation": 2, "green": 2, "": 1, "b": 1, "number": 1, "step": 1, "need": 1, "complete": 1, "advisor": 1}, {"figure": 1, "6": 1, "performance": 1, "comparison": 1, "exploration": 1, "exploitation": 1, "policies": 1}, {"63": 1, "": 2, "performance": 2, "evaluation": 1, "novel": 2, "task": 2, "examine": 1, "framework": 1, "learn": 1, "scratch": 1, "contrast": 1, "method": 1, "maml": 1, "train": 1, "use": 1, "ppo": 1}, {"case": 1, "discrete": 1, "action": 1, "set": 1, "train": 2, "task": 1, "500": 1, "episodes": 1, "compare": 1, "performance": 1, "agent": 1, "reinforce": 1, "r": 1, "ppo": 1, "without": 1, "advisor": 1}, {"case": 1, "continuous": 1, "task": 1, "restrict": 1, "experiment": 1, "agent": 1, "use": 1, "ppo": 1, "train": 1, "500": 1, "episodes": 1}, {"experiment": 1, "set": 1, "initial": 1, "value": 1, "": 1, "08": 1, "decrease": 1, "factor": 1, "0995": 1, "every": 1, "episode": 1}, {"result": 1, "show": 1, "table": 1, "1": 1, "obtain": 1, "train": 1, "5": 2, "time": 1, "novel": 1, "task": 1, "record": 1, "average": 1, "performance": 1, "standard": 1, "deviations": 1}, {"table": 1, "display": 1, "mean": 2, "average": 1, "standard": 1, "deviations": 1, "record": 1}, {"problem": 1, "class": 1, "polebalance": 2, "animat": 1, "correspond": 1, "discrete": 1, "action": 1, "space": 1, "c": 1, "hopper": 1, "ant": 1, "continuous": 1}, {"problem": 1, "class": 1, "polebalance": 2, "animat": 1, "c": 1, "hopper": 1, "ant": 1, "": 31, "r": 1, "2032": 1, "315": 1, "77962": 1, "11028": 1, "radvisor": 1, "2852": 1, "76": 1, "38727": 1, "16233": 1, "ppo": 1, "2787": 1, "617": 1, "75140": 1, "6873": 1, "2995": 1, "790": 1, "1382": 1, "1053": 1, "4275": 1, "2435": 1, "ppoadvisor": 1, "4629": 1, "630": 1, "63197": 1, "1555": 1, "43813": 1, "3554": 1, "16443": 1, "4854": 1, "8376": 1, "2041": 1, "maml": 1, "3929": 1, "574": 1, "66993": 1, "9232": 1, "26776": 1, "16305": 1, "3941": 1, "795": 1, "11333": 1, "6448": 1, "table": 1, "1": 1, "average": 1, "performance": 1, "standard": 1, "deviations": 1, "unseen": 1, "task": 1, "trials": 1, "discrete": 1, "continuous": 1, "control": 1, "last": 1, "50": 1, "episodes": 1}, {"7": 1, "": 2, "conclusion": 1, "work": 1, "develop": 1, "framework": 1, "leverage": 1, "experience": 1, "guide": 1, "agents": 1, "exploration": 2, "novel": 1, "task": 2, "advisor": 1, "learn": 1, "policy": 1, "use": 1, "agent": 1, "solve": 1}, {"show": 1, "sample": 1, "task": 2, "use": 2, "learn": 2, "exploration": 1, "policy": 1, "agent": 1, "improve": 1, "speed": 1, "novel": 1}, {"takeaway": 1, "work": 1, "oftentimes": 1, "agent": 1, "solve": 1, "new": 1, "task": 1, "may": 1, "experience": 2, "similar": 1, "problems": 1, "leverage": 1}, {"one": 1, "way": 1, "learn": 1, "better": 1, "approach": 1, "explore": 1, "face": 1, "uncertainty": 1}, {"natural": 1, "future": 1, "direction": 1, "work": 1, "use": 1, "past": 1, "experience": 1, "identify": 1, "exploration": 1, "need": 1, "action": 1, "take": 1, "explore": 1}, {"8": 1, "": 1, "reference": 1, "1": 1, "mohammad": 1, "gheshlaghi": 1, "azar": 1, "ian": 1, "osband": 1, "rmi": 1, "munos": 1}, {"minimax": 1, "regret": 1, "bound": 1, "reinforcement": 1, "learn": 1}, {"doina": 1, "precup": 1, "yee": 1, "whye": 1, "teh": 1, "editors": 1, "proceed": 1, "34th": 1, "international": 2, "conference": 1, "machine": 1, "learn": 1, "page": 1, "263272": 1, "convention": 1, "centre": 1, "sydney": 1, "australia": 1, "0611": 1, "aug": 1, "2017": 1}, {"pmlr": 1}, {"2": 1, "greg": 1, "brockman": 1, "vicki": 1, "cheung": 1, "ludwig": 1, "pettersson": 1, "jonas": 1, "schneider": 1, "john": 1, "schulman": 1, "jie": 1, "tang": 1, "wojciech": 1, "zaremba": 1}, {"openai": 1, "gym": 1, "2016": 1}, {"3": 1, "fernando": 1, "fernandez": 1, "manuela": 1, "veloso": 1}, {"probabilistic": 1, "policy": 1, "reuse": 1, "reinforcement": 1, "learn": 1, "agent": 1}, {"proceed": 1, "fifth": 1, "international": 1, "joint": 1, "conference": 1, "autonomous": 1, "agents": 1, "multiagent": 1, "systems": 1, "aamas": 1, "06": 1, "page": 1, "720727": 1, "new": 1, "york": 1, "ny": 1, "usa": 1, "2006": 1}, {"acm": 1}, {"4": 1, "chelsea": 1, "finn": 1, "pieter": 1, "abbeel": 1, "sergey": 1, "levine": 1}, {"modelagnostic": 1, "metalearning": 1, "fast": 1, "adaptation": 1, "deep": 1, "network": 1}, {"doina": 1, "precup": 1, "yee": 1, "whye": 1, "teh": 1, "editors": 1, "proceed": 2, "34th": 1, "international": 2, "conference": 1, "machine": 2, "learn": 2, "volume": 1, "70": 1, "research": 1, "page": 1, "11261135": 1, "convention": 1, "centre": 1, "sydney": 1, "australia": 1, "0611": 1, "aug": 1, "2017": 1}, {"pmlr": 1}, {"5": 1, "aurlien": 1, "garivier": 1, "eric": 1, "moulines": 1}, {"upperconfidence": 1, "bind": 1, "policies": 1, "switch": 1, "bandit": 1, "problems": 1}, {"proceed": 1, "22nd": 1, "international": 1, "conference": 1, "algorithmic": 1, "learn": 1, "theory": 1, "alt11": 1, "page": 1, "174188": 1, "berlin": 1, "heidelberg": 1, "2011": 1}, {"springerverlag": 1}, {"6": 1, "rein": 1, "houthooft": 1, "xi": 1, "chen": 1, "yan": 1, "duan": 1, "john": 1, "schulman": 1, "filip": 1, "de": 1, "turck": 1, "pieter": 1, "abbeel": 1}, {"curiositydriven": 1, "exploration": 1, "deep": 1, "reinforcement": 1, "learn": 1, "via": 1, "bayesian": 1, "neural": 1, "network": 1}, {"corr": 1, "abs160509674": 1, "2016": 1}, {"7": 1, "william": 1, "koch": 1, "renato": 1, "mancuso": 1, "richard": 1, "west": 1, "azer": 1, "bestavros": 1}, {"reinforcement": 1, "learn": 1, "uav": 1, "attitude": 1, "control": 1}, {"corr": 1, "abs180404154": 1, "2018": 1}, {"8": 1, "romain": 1, "laroche": 1, "mehdi": 1, "fatemi": 1, "harm": 1, "van": 1, "seijen": 1, "joshua": 1, "romoff": 1}, {"multiadvisor": 1, "reinforcement": 1, "learn": 1}, {"april": 1, "2017": 1}, {"9": 1, "bingyao": 1, "liu": 1, "satinder": 1, "p": 1, "singh": 1, "richard": 1, "l": 1, "lewis": 1, "shiyin": 1, "qin": 1}, {"optimal": 1, "reward": 1, "multiagent": 1, "team": 1}, {"2012": 4, "ieee": 1, "international": 1, "conference": 1, "development": 1, "learn": 1, "epigenetic": 1, "robotics": 1, "icdlepirob": 1, "san": 1, "diego": 1, "ca": 1, "usa": 1, "november": 1, "79": 1, "page": 1, "18": 1}, {"10": 1, "jarryd": 1, "martin": 1, "suraj": 1, "narayanan": 1, "sasikumar": 1, "tom": 1, "everitt": 1, "marcus": 1, "hutter": 1}, {"countbased": 1, "exploration": 1, "feature": 1, "space": 1, "reinforcement": 1, "learn": 1}, {"corr": 1, "abs170608090": 1, "2017": 1}, {"11": 1, "volodymyr": 1, "mnih": 1, "koray": 1, "kavukcuoglu": 1, "david": 1, "silver": 1, "andrei": 1, "rusu": 1, "joel": 1, "veness": 1, "marc": 1, "g": 1, "bellemare": 1, "alex": 1, "grave": 1, "martin": 1, "riedmiller": 1, "andreas": 1, "k": 1, "fidjeland": 1, "georg": 1, "ostrovski": 1, "stig": 1, "petersen": 1, "charles": 1, "beattie": 1, "amir": 1, "sadik": 1, "ioannis": 1, "antonoglou": 1, "helen": 1, "king": 1, "dharshan": 1, "kumaran": 1, "daan": 1, "wierstra": 1, "shane": 1, "legg": 1, "demis": 1, "hassabis": 1}, {"humanlevel": 1, "control": 1, "deep": 1, "reinforcement": 1, "learn": 1}, {"nature": 1, "5187540529533": 1, "february": 1, "2015": 1}, {"12": 1, "deepak": 1, "pathak": 1, "pulkit": 1, "agrawal": 1, "alexei": 1, "efros": 1, "trevor": 1, "darrell": 1}, {"curiositydriven": 1, "exploration": 1, "selfsupervised": 1, "prediction": 1}, {"corr": 1, "abs170505363": 1, "2017": 1}, {"13": 1, "v": 2, "phansalkar": 1, "l": 1, "thathachar": 1}, {"local": 1, "global": 1, "optimization": 1, "algorithms": 1, "generalize": 1, "learn": 1, "automata": 1}, {"neural": 1, "comput": 1, "75950973": 1, "september": 1, "1995": 1}, {"14": 1, "jrgen": 1, "schmidhuber": 1, "jieyu": 1, "zhao": 1, "nicol": 1, "n": 1, "schraudolph": 1}, {"learn": 2}, {"chapter": 1, "reinforcement": 1, "learn": 1, "selfmodifying": 1, "policies": 1, "page": 1, "293309": 1}, {"kluwer": 1, "academic": 1, "publishers": 1, "norwell": 1, "usa": 1, "1998": 1}, {"15": 1, "jrgen": 1, "schmidhuber": 1}, {"learn": 3, "strategies": 1}, {"technical": 1, "report": 1, "1995": 1}, {"16": 1, "john": 1, "schulman": 1, "philipp": 1, "moritz": 1, "sergey": 1, "levine": 1, "michael": 1, "jordan": 1, "pieter": 1, "abbeel": 1}, {"highdimensional": 1, "continuous": 1, "control": 1, "use": 1, "generalize": 1, "advantage": 1, "estimation": 1}, {"proceed": 1, "international": 1, "conference": 1, "learn": 1, "representations": 1, "iclr": 1, "2016": 1}, {"9": 1, "": 1, "17": 1, "john": 1, "schulman": 1, "filip": 1, "wolski": 1, "prafulla": 1, "dhariwal": 1, "alec": 1, "radford": 1, "oleg": 1, "klimov": 1}, {"proximal": 1, "policy": 1, "optimization": 1, "algorithms": 1}, {"corr": 1, "abs170706347": 1, "2017": 1}, {"18": 1, "alexander": 1, "l": 1, "strehl": 1}, {"probably": 1, "approximately": 1, "correct": 1, "pac": 1, "exploration": 1, "reinforcement": 1, "learn": 1}, {"isaim": 1, "2008": 1}, {"19": 1, "richard": 1, "sutton": 1, "andrew": 1, "g": 1, "barto": 1}, {"introduction": 1, "reinforcement": 1, "learn": 1}, {"mit": 1, "press": 1, "cambridge": 1, "usa": 1, "1st": 1, "edition": 1, "1998": 1}, {"20": 1, "haoran": 1, "tang": 1, "rein": 1, "houthooft": 1, "davis": 1, "foote": 1, "adam": 1, "stooke": 1, "xi": 1, "chen": 1, "yan": 1, "duan": 1, "john": 1, "schulman": 1, "filip": 1, "de": 1, "turck": 1, "pieter": 1, "abbeel": 1}, {"exploration": 2, "study": 1, "countbased": 1, "deep": 1, "reinforcement": 1, "learn": 1}, {"corr": 1, "abs161104717": 1, "2016": 1}, {"21": 1, "philip": 1, "thomas": 1, "andrew": 1, "g": 1, "barto": 1}, {"conjugate": 1, "markov": 1, "decision": 1, "process": 1}, {"proceed": 1, "28th": 1, "international": 1, "conference": 1, "machine": 1, "learn": 1, "icml": 1, "2011": 3, "bellevue": 1, "washington": 1, "usa": 1, "june": 1, "28": 1, "": 1, "july": 1, "2": 1, "page": 1, "137144": 1}, {"22": 1, "harm": 1, "van": 1, "seijen": 1, "mehdi": 1, "fatemi": 1, "joshua": 1, "romoff": 1, "romain": 1, "laroche": 1, "tavian": 1, "barnes": 1, "jeffrey": 1, "tsang": 1}, {"hybrid": 1, "reward": 1, "architecture": 1, "reinforcement": 1, "learn": 1}, {"june": 1, "2017": 1}, {"23": 1, "christopher": 1, "j": 1, "c": 1, "h": 1, "watkins": 1, "peter": 1, "dayan": 1}, {"qlearning": 1}, {"machine": 1, "learn": 1, "page": 1, "279292": 1, "1992": 1}, {"24": 1, "steven": 1, "whitehead": 1}, {"complexity": 1, "cooperation": 1, "qlearning": 1}, {"proceed": 1, "eighth": 1, "international": 1, "workshop": 1, "ml91": 1, "northwestern": 1, "university": 1, "evanston": 1, "illinois": 1, "usa": 1, "page": 1, "363367": 1, "1991": 1}, {"25": 1, "ronald": 1, "j": 1, "williams": 1}, {"simple": 1, "statistical": 1, "gradientfollowing": 1, "algorithms": 1, "connectionist": 1, "reinforcement": 1, "learn": 1}, {"machine": 1, "learn": 1, "page": 1, "229256": 1, "1992": 1}, {"26": 1, "q": 1, "yang": 1, "jagannathan": 1}, {"reinforcement": 1, "learn": 1, "controller": 1, "design": 1, "affine": 1, "nonlinear": 1, "discretetime": 1, "systems": 1, "use": 1, "online": 1, "approximators": 1}, {"ieee": 1, "transactions": 1, "systems": 1, "man": 1, "cybernetics": 2, "part": 1, "b": 1, "422377390": 1, "april": 1, "2012": 1}, {"27": 1, "tianhao": 1, "zhang": 1, "gregory": 1, "kahn": 1, "sergey": 1, "levine": 1, "pieter": 1, "abbeel": 1}, {"learn": 1, "deep": 1, "control": 1, "policies": 1, "autonomous": 1, "aerial": 1, "vehicles": 1, "mpcguided": 1, "policy": 1, "search": 1}, {"corr": 1, "abs150906791": 1, "2015": 1}, {"10": 1}]
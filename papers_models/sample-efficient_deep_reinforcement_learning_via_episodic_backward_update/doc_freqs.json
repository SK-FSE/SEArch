[{"sampleefficient": 1, "deep": 2, "reinforcement": 2, "learn": 2, "via": 1, "episodic": 2, "backward": 2, "update": 2, "": 3, "su": 1, "young": 1, "lee": 1, "sungik": 1, "choi": 1, "saeyoung": 1, "chung": 1, "school": 1, "electrical": 1, "engineer": 1, "kaist": 1, "republic": 1, "korea": 1, "suyoungl": 1, "sichoi": 1, "schungkaistackr": 1, "abstract": 1, "propose": 1, "ebu": 1, "novel": 1, "algorithm": 1, "direct": 1, "value": 1, "propagation": 1}, {"contrast": 1, "conventional": 1, "use": 1, "experience": 1, "replay": 1, "uniform": 1, "random": 1, "sample": 2, "agent": 1, "whole": 1, "episode": 1, "successively": 1, "propagate": 1, "value": 1, "state": 2, "previous": 1}, {"computationally": 1, "efficient": 1, "recursive": 1, "algorithm": 1, "allow": 1, "sparse": 1, "delay": 1, "reward": 1, "propagate": 1, "directly": 1, "transition": 1, "sample": 1, "episode": 1}, {"theoretically": 1, "prove": 1, "convergence": 1, "ebu": 1, "method": 1, "experimentally": 1, "demonstrate": 1, "performance": 1, "deterministic": 1, "stochastic": 1, "environments": 1}, {"especially": 1, "49": 1, "game": 1, "atari": 1, "2600": 1, "domain": 1, "ebu": 1, "achieve": 1, "mean": 1, "median": 1, "human": 1, "normalize": 1, "performance": 1, "dqn": 1, "use": 1, "5": 1, "10": 1, "sample": 1, "respectively": 1}, {"1": 1, "": 2, "introduction": 1, "deep": 1, "reinforcement": 1, "learn": 2, "drl": 1, "successful": 1, "many": 1, "complex": 1, "environments": 1, "arcade": 1, "environment": 1, "2": 1, "go": 1, "18": 1}, {"despite": 1, "drls": 1, "impressive": 1, "achievements": 1, "still": 1, "impractical": 1, "term": 1, "sample": 1, "efficiency": 1}, {"achieve": 1, "humanlevel": 1, "performance": 1, "arcade": 1, "learn": 1, "environment": 1, "deep": 1, "qnetwork": 1, "dqn": 1, "14": 1, "require": 1, "200": 1, "million": 1, "frame": 1, "experience": 1, "train": 1, "correspond": 1, "39": 1, "days": 1, "gameplay": 1, "realtime": 1}, {"clearly": 1, "still": 1, "tremendous": 1, "gap": 1, "learn": 2, "process": 1, "humans": 1, "deep": 1, "reinforcement": 1, "agents": 1}, {"problem": 1, "even": 1, "crucial": 1, "task": 1, "autonomous": 1, "drive": 1, "cannot": 1, "risk": 1, "many": 1, "trials": 1, "errors": 1, "due": 1, "high": 1, "cost": 1, "sample": 1}, {"one": 1, "reason": 1, "dqn": 1, "suffer": 1, "low": 1, "sample": 2, "efficiency": 1, "method": 1, "replay": 1, "memory": 1}, {"many": 1, "practical": 1, "problems": 1, "rl": 1, "agent": 1, "observe": 1, "sparse": 1, "delay": 1, "reward": 1}, {"two": 1, "main": 1, "problems": 1, "sample": 1, "onestep": 1, "transition": 1, "uniformly": 1, "random": 1}, {"1": 1, "low": 1, "chance": 1, "sample": 1, "transition": 1, "reward": 1, "sparsity": 1}, {"transition": 1, "reward": 1, "always": 1, "update": 1, "assign": 1, "credit": 1, "action": 1, "maximize": 1, "expect": 1, "return": 1}, {"2": 1, "early": 1, "stag": 1, "train": 1, "value": 3, "initialize": 1, "zero": 2, "point": 1, "update": 2, "onestep": 1, "transition": 2, "reward": 2, "future": 1, "nonzero": 1, "yet": 1}, {"without": 1, "future": 1, "reward": 1, "signal": 1, "propagate": 1, "sample": 1, "transition": 1, "always": 1, "train": 1, "return": 1, "zero": 1, "value": 1}, {"work": 1, "propose": 1, "episodic": 1, "backward": 1, "update": 1, "ebu": 1, "present": 1, "solutions": 1, "problems": 1, "raise": 1}, {"observe": 1, "event": 2, "scan": 1, "memory": 1, "seek": 1, "past": 1, "cause": 1, "later": 1, "one": 1}, {"episodic": 1, "control": 1, "method": 1, "humans": 1, "normally": 1, "recognize": 1, "cause": 1, "effect": 1, "relationship": 1, "10": 1}, {"inspire": 1, "solve": 1, "first": 1, "problem": 1, "1": 1, "sample": 1, "transition": 1, "episodic": 1, "manner": 1}, {"assure": 1, "least": 1, "one": 1, "transition": 1, "nonzero": 1, "reward": 1, "use": 1, "value": 1, "update": 1}, {"solve": 1, "second": 1, "problem": 1, "2": 1, "update": 1, "value": 1, "transition": 2, "backward": 1, "manner": 1, "make": 1}, {"afterward": 1, "perform": 1, "33rd": 1, "conference": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "neurips": 1, "2019": 1, "vancouver": 1, "canada": 1}, {"efficient": 1, "reward": 1, "propagation": 1, "without": 1, "meaningless": 1, "update": 1}, {"method": 1, "faithfully": 1, "follow": 1, "principle": 1, "dynamic": 1, "program": 1}, {"mention": 1, "author": 1, "dqn": 1, "update": 1, "correlate": 1, "sample": 1, "sequence": 1, "vulnerable": 1, "overestimation": 1}, {"section": 1, "3": 1, "deal": 1, "issue": 1, "adopt": 1, "diffusion": 1, "factor": 1, "mediate": 1, "learn": 1, "value": 1, "future": 1, "transition": 1, "current": 1, "sample": 1, "reward": 1}, {"section": 1, "4": 1, "theoretically": 1, "prove": 1, "convergence": 1, "method": 1, "deterministic": 1, "stochastic": 1, "mdps": 1}, {"section": 1, "5": 1, "empirically": 1, "show": 1, "superiority": 1, "method": 1, "2d": 1, "mnist": 1, "maze": 1, "environment": 1, "49": 1, "game": 1, "atari": 1, "2600": 1, "domain": 1}, {"especially": 1, "49": 1, "game": 1, "atari": 1, "2600": 1, "domain": 1, "method": 1, "require": 1, "10m": 1, "frame": 2, "achieve": 2, "mean": 1, "humannormalized": 2, "score": 2, "report": 1, "nature": 1, "dqn": 1, "14": 1, "20m": 1, "median": 1}, {"remarkably": 1, "ebu": 1, "achieve": 1, "improvements": 1, "comparable": 1, "amount": 1, "computation": 1, "complexity": 1, "modify": 1, "target": 1, "generation": 1, "procedure": 1, "value": 1, "update": 1, "original": 1, "dqn": 1}, {"2": 1, "": 3, "background": 1, "goal": 1, "reinforcement": 1, "learn": 2, "rl": 1, "optimal": 1, "policy": 1, "maximize": 1, "expect": 1, "sum": 1, "reward": 1, "environment": 1, "often": 1, "model": 1, "markov": 1, "decision": 1, "process": 1, "mdp": 1, "p": 1, "r": 1}, {"denote": 4, "state": 1, "space": 2, "action": 1, "p": 1, "": 7, "r": 3, "transition": 1, "probability": 1, "distribution": 1, "reward": 1, "function": 1}, {"qlearning": 1, "22": 1, "one": 1, "widely": 1, "use": 1, "methods": 1, "solve": 1, "rl": 1, "task": 1}, {"objective": 1, "qlearning": 1, "estimate": 1, "stateaction": 1, "value": 1, "function": 1, "qs": 1, "qfunction": 1, "characterize": 1, "bellman": 1, "optimality": 1, "equation": 1}, {"q": 2, "st": 1, "": 6, "ert": 1, "maxa0": 1, "st1": 1, "a0": 1}, {"two": 1, "major": 1, "inefficiencies": 1, "traditional": 1, "online": 1, "qlearning": 1}, {"first": 1, "experience": 1, "use": 1, "update": 1, "qfunction": 1}, {"secondly": 1, "learn": 2, "experience": 1, "chronologically": 2, "forward": 1, "order": 2, "much": 1, "inefficient": 1, "backward": 1, "value": 2, "st1": 1, "require": 1, "update": 1, "st": 1, "": 1}, {"experience": 1, "replay": 1, "12": 1, "propose": 1, "overcome": 1, "inefficiencies": 1}, {"observe": 1, "transition": 2, "st": 1, "": 4, "rt": 1, "st1": 1, "agent": 1, "store": 1, "replay": 1, "buffer": 1}, {"order": 1, "learn": 1, "qvalues": 1, "agent": 1, "sample": 1, "transition": 1, "replay": 1, "buffer": 1}, {"practice": 1, "state": 1, "space": 1, "extremely": 1, "large": 1, "therefore": 1, "impractical": 1, "tabularize": 1, "qvalues": 1, "stateaction": 1, "pair": 1}, {"deep": 2, "qnetwork": 1, "14": 1, "overcome": 1, "issue": 1, "use": 1, "neural": 1, "network": 1, "approximate": 1, "qfunction": 1}, {"dqn": 1, "adopt": 1, "experience": 1, "replay": 1, "use": 1, "transition": 1, "multiple": 1, "update": 1}, {"since": 1, "dqn": 1, "use": 1, "function": 1, "approximator": 1, "consecutive": 1, "state": 1, "output": 1, "similar": 1, "qvalues": 1}, {"dqn": 1, "update": 1, "transition": 1, "chronologically": 1, "backward": 1, "order": 1, "often": 1, "overestimation": 1, "errors": 1, "cumulate": 1, "degrade": 1, "performance": 1}, {"therefore": 1, "dqn": 1, "sample": 1, "transition": 1, "backward": 1, "order": 1, "uniformly": 1, "random": 1}, {"process": 1, "break": 1, "correlations": 1, "consecutive": 1, "transition": 1, "reduce": 1, "variance": 1, "update": 1}, {"variety": 1, "methods": 1, "propose": 1, "improve": 1, "performance": 1, "dqn": 1, "term": 1, "stability": 1, "sample": 1, "efficiency": 1, "runtime": 1}, {"methods": 1, "propose": 1, "new": 1, "network": 1, "architectures": 1}, {"duel": 1, "network": 1, "architecture": 1, "21": 1, "contain": 1, "two": 1, "stream": 1, "separate": 1, "qnetworks": 1, "estimate": 1, "value": 1, "function": 2, "advantage": 1}, {"neural": 1, "episodic": 3, "control": 2, "16": 1, "modelfree": 1, "5": 1, "use": 1, "memory": 1, "modules": 1, "estimate": 1, "stateaction": 1, "value": 1}, {"rudder": 1, "1": 1, "introduce": 1, "lstm": 1, "network": 1, "contribution": 1, "analysis": 1, "efficient": 1, "return": 1, "decomposition": 1}, {"ephemeral": 1, "value": 3, "adjustments": 1, "eva": 1, "7": 1, "combine": 1, "two": 1, "separate": 1, "network": 2, "one": 1, "standard": 1, "dqn": 1, "another": 1, "trajectorybased": 1}, {"methods": 1, "tackle": 1, "uniform": 1, "random": 1, "sample": 1, "replay": 1, "strategy": 1, "dqn": 1}, {"prioritize": 1, "experience": 1, "replay": 1, "17": 1, "assign": 2, "nonuniform": 1, "probability": 2, "sample": 1, "transition": 2, "greater": 1, "higher": 1, "temporal": 1, "difference": 1, "td": 1, "error": 1}, {"inspire": 1, "lins": 1, "backward": 1, "use": 1, "replay": 1, "memory": 1, "methods": 1, "try": 1, "aggregate": 1, "td": 1, "value": 1, "montecarlo": 1, "mc": 1, "return": 1}, {"q": 2, "23": 1, "": 1, "6": 1, "retrace": 1, "15": 1, "modify": 1, "target": 1, "value": 1, "allow": 1, "onpolicy": 2, "sample": 1, "use": 1, "interchangeably": 1, "offpolicy": 1, "learn": 1}, {"countbased": 1, "exploration": 1, "method": 1, "combine": 1, "intrinsic": 1, "motivation": 1, "3": 1, "take": 1, "mixture": 1, "onestep": 1, "return": 2, "mc": 1, "set": 1, "target": 1, "value": 1}, {"optimality": 1, "tighten": 1, "8": 1, "apply": 1, "constraints": 1, "target": 1, "use": 1, "value": 1, "several": 1, "neighbor": 1, "transition": 1}, {"simply": 1, "add": 1, "penalty": 1, "term": 1, "loss": 1, "efficiently": 1, "propagate": 1, "reliable": 1, "value": 1, "achieve": 1, "fast": 1, "convergence": 1}, {"2": 14, "": 76, "1": 8, "3": 11, "4": 4, "episode": 1, "experience": 1, "update": 1, "value": 1, "0": 4, "probability": 1, "learn": 1, "optimal": 1, "path": 1, "environment": 1, "dynamics": 1, "10": 2, "08": 1, "06": 1, "04": 1, "02": 1, "uniform": 2, "sample": 3, "episodic": 1, "backward": 1, "00": 1, "20": 1, "30": 1, "40": 1, "50": 1, "transition": 1, "b": 1, "figure": 1, "motivate": 1, "example": 1, "method": 1, "fail": 1, "ebu": 1}, {"simple": 1, "navigation": 1, "domain": 1, "4": 1, "state": 1, "single": 1, "reward": 1, "transition": 1}, {"circle": 1, "number": 1, "indicate": 1, "order": 1, "sample": 1, "update": 1}, {"qu": 1, "qe": 1, "stand": 1, "qvalues": 1, "learn": 1, "uniform": 1, "random": 1, "sample": 1, "method": 2, "ebu": 1, "respectively": 1}, {"b": 1, "probability": 1, "learn": 1, "optimal": 1, "path": 1, "s1": 1, "": 4, "s2": 1, "s3": 1, "s4": 1, "update": 1, "qvalues": 1, "sample": 1, "transition": 1}, {"goal": 1, "improve": 1, "sample": 1, "efficiency": 1, "deep": 1, "reinforcement": 1, "learn": 1, "make": 1, "simple": 1, "yet": 1, "effective": 1, "modification": 1}, {"without": 1, "single": 1, "change": 1, "network": 1, "structure": 1, "train": 1, "scheme": 1, "hyperparameters": 1, "original": 1, "dqn": 1, "modify": 1, "target": 1, "generation": 1, "method": 1}, {"instead": 1, "use": 1, "limit": 1, "number": 1, "transition": 2, "method": 1, "sample": 2, "whole": 1, "episode": 2, "replay": 1, "memory": 1, "propagate": 1, "value": 1, "sequentially": 1, "throughout": 1, "entire": 1, "backward": 1, "manner": 1}, {"use": 1, "temporary": 1, "backward": 1, "qtable": 1, "diffusion": 1, "coefficient": 1, "novel": 1, "algorithm": 1, "effectively": 1, "reduce": 1, "errors": 1, "generate": 1, "consecutive": 1, "update": 1, "correlate": 1, "state": 1}, {"3": 1, "31": 1, "": 2, "propose": 1, "methods": 1, "episodic": 1, "backward": 1, "update": 1, "tabular": 2, "qlearning": 1, "let": 1, "us": 1, "imagine": 1, "simple": 1, "mdp": 1, "single": 1, "reward": 1, "transition": 1, "figure": 1, "1": 1, "agent": 1, "take": 1, "one": 1, "two": 1, "action": 1, "leave": 1, "right": 1}, {"example": 1, "s1": 1, "initial": 1, "state": 2, "s4": 1, "terminal": 1}, {"reward": 2, "1": 1, "gain": 2, "agent": 1, "reach": 1, "terminal": 1, "state": 1, "0": 1, "transition": 1}, {"make": 1, "simple": 1, "assume": 1, "one": 1, "episode": 1, "store": 1, "experience": 1, "memory": 1, "s1": 1, "": 6, "s2": 2, "s3": 2, "s4": 1}, {"qvalues": 1, "transition": 1, "initialize": 1, "zero": 1}, {"discount": 1, "": 2, "0": 1, "1": 1, "optimal": 1, "policy": 1, "take": 1, "action": 1, "right": 1, "state": 1}, {"sample": 2, "transition": 2, "uniformly": 1, "random": 1, "nature": 1, "dqn": 1, "key": 1, "s1": 1, "": 6, "s2": 2, "s3": 2, "s4": 1, "may": 1, "update": 1}, {"even": 1, "transition": 2, "sample": 1, "guarantee": 1, "update": 2, "s3": 2, "": 4, "s4": 1, "do": 1, "s2": 1}, {"speed": 1, "reward": 1, "propagation": 1, "update": 1, "transition": 1, "within": 1, "episode": 1, "backward": 1, "manner": 1}, {"recursive": 1, "update": 1, "also": 1, "computationally": 1, "efficient": 1}, {"calculate": 1, "probability": 1, "learn": 1, "optimal": 1, "path": 1, "s1": 1, "": 4, "s2": 1, "s3": 1, "s4": 1, "function": 1, "number": 1, "sample": 1, "transition": 1, "train": 1}, {"tabular": 1, "episodic": 1, "backward": 1, "update": 2, "state": 1, "algorithm": 2, "1": 1, "special": 1, "case": 1, "lins": 1, "11": 1, "recency": 1, "parameter": 1, "": 2, "0": 1, "agent": 1, "figure": 1, "optimal": 1, "policy": 1, "5": 1, "qvalues": 1}, {"however": 1, "see": 1, "uniform": 1, "sample": 1, "method": 1, "require": 1, "40": 1, "transition": 1, "learn": 1, "optimal": 1, "path": 1, "probability": 1, "close": 1, "1": 2, "figure": 1, "b": 1}, {"note": 1, "method": 1, "differ": 1, "standard": 1, "nstep": 1, "qlearning": 1, "22": 1}, {"nstep": 1, "qlearning": 1, "number": 1, "future": 2, "step": 1, "target": 1, "generation": 1, "fix": 1, "n": 1, "however": 1, "method": 1, "consider": 1, "value": 1, "length": 1, "sample": 1, "episode": 1}, {"n": 1, "step": 3, "qlearning": 1, "take": 2, "max": 2, "operator": 2, "nth": 1, "whereas": 1, "method": 1, "every": 1, "iterative": 1, "backward": 1, "propagate": 1, "high": 1, "value": 1, "faster": 1}, {"avoid": 1, "exponential": 1, "decay": 1, "qvalue": 1, "set": 1, "learn": 1, "rate": 1, "": 2, "1": 1, "within": 1, "single": 1, "episode": 1, "update": 1}, {"3": 1, "": 2, "algorithm": 1, "1": 2, "episodic": 1, "backward": 1, "update": 1, "tabular": 2, "qlearning": 1, "single": 1, "episode": 1, "initialize": 1, "q": 2, "table": 1, "rsa": 1, "allzero": 1, "matrix": 1}, {"qs": 1, "": 3, "0": 1, "state": 1, "action": 1, "pair": 1}, {"2": 1, "experience": 1, "episode": 1, "e": 1, "": 6, "s1": 1, "a1": 1, "r1": 1, "s2": 1}, {"": 1}, {"": 1}, {"": 58, "st": 4, "rt": 5, "1": 8, "3": 2, "4": 2, "qst": 1, "maxa0": 1, "qst1": 1, "a0": 1, "5": 2, "end": 4, "algorithm": 1, "2": 3, "episodic": 1, "backward": 1, "update": 1, "initialize": 2, "replay": 1, "memory": 1, "capacity": 1, "n": 1, "online": 1, "actionvalue": 2, "function": 3, "q": 13, "target": 3, "episode": 2, "terminal": 1, "probability": 1, "select": 2, "random": 2, "action": 2, "otherwise": 1, "argmaxa": 1, "execute": 1, "observe": 1, "reward": 1, "next": 1, "state": 1, "st1": 2, "6": 1, "store": 1, "transition": 1, "7": 1, "sample": 1, "e": 1, "r": 1, "0": 2, "set": 1, "lengthe": 1, "8": 1, "generate": 1, "temporary": 1, "qtable": 1, "9": 1, "vector": 1, "zerost": 1, "yt": 1, "10": 1, "k": 4, "11": 1, "ak1": 2, "yk1": 1, "12": 1, "yk": 1, "rk": 1, "maxa": 1, "13": 1, "14": 1, "perform": 1, "gradient": 1, "descent": 1, "step": 2, "respect": 1, "15": 1, "every": 1, "c": 1, "reset": 1, "16": 1, "17": 1, "multistep": 1, "methods": 1, "converge": 1, "optimal": 1, "stateaction": 1, "value": 1}, {"however": 1, "algorithm": 1, "neither": 1, "cut": 1, "trace": 1, "trajectories": 1, "q": 2, "require": 1, "parameter": 1, "": 2, "small": 1, "enough": 1, "guarantee": 1, "convergence": 1}, {"present": 1, "detail": 1, "discussion": 1, "relationship": 1, "ebu": 1, "multistep": 1, "methods": 1, "appendix": 1, "f": 1, "episodic": 1, "backward": 2, "update": 2, "deep": 2, "qlearning1": 1, "": 2, "32": 1, "directly": 1, "apply": 1, "algorithm": 1, "reinforcement": 1, "learn": 1, "know": 1, "show": 1, "highly": 1, "unstable": 1, "result": 1, "due": 1, "high": 1, "correlation": 1, "consecutive": 1, "sample": 1}, {"show": 1, "fundamental": 1, "ideas": 1, "tabular": 1, "version": 2, "backward": 1, "update": 1, "algorithm": 1, "may": 1, "apply": 1, "deep": 1, "modifications": 1}, {"full": 1, "algorithm": 2, "introduce": 1, "2": 1, "closely": 1, "resemble": 1, "nature": 1, "dqn": 1, "14": 1}, {"contributions": 1, "lie": 1, "recursive": 1, "backward": 1, "target": 1, "generation": 1, "diffusion": 1, "factor": 1, "": 1, "start": 1, "line": 1, "number": 1, "7": 1, "algorithm": 1, "2": 1, "prevent": 1, "overestimation": 1, "errors": 1, "correlate": 1, "state": 1, "cumulate": 1}, {"instead": 1, "sample": 2, "transition": 2, "uniformly": 1, "random": 1, "make": 1, "use": 1, "within": 1, "episode": 1, "e": 1, "": 2, "r": 1, "0": 1}, {"let": 1, "sample": 1, "episode": 1, "start": 1, "state": 1, "s1": 1, "": 1, "contain": 1, "transition": 1}, {"e": 1, "denote": 1, "set": 1, "four": 1, "lengtht": 1, "vectors": 1, "": 4, "s1": 1, "s2": 1}, {"": 1}, {"": 1}, {"": 6, "st": 1, "a1": 1, "a2": 1}, {"": 1}, {"": 1}, {"": 6, "r": 1, "r1": 1, "r2": 1}, {"": 1}, {"": 1}, {"": 6, "rt": 1, "0": 1, "s2": 1, "s3": 1}, {"": 1}, {"": 1}, {"": 2, "st": 1, "1": 1}, {"temporary": 1, "target": 2, "qtable": 1, "q": 1, "": 2, "matrix": 1, "store": 1, "qvalues": 1, "state": 1, "0": 1, "valid": 1, "action": 2, "space": 1, "mdp": 1}, {"therefore": 1, "jth": 1, "column": 2, "q": 3, "vector": 1, "contain": 1, "sj1": 1, "": 7, "valid": 1, "action": 1, "target": 1, "qfunction": 1, "parametrized": 1}, {"initialization": 1, "temporary": 1, "qtable": 1, "perform": 1, "recursive": 1, "backward": 1, "update": 1}, {"adopt": 1, "backward": 1, "update": 1, "idea": 1, "one": 1, "element": 1, "q": 2, "ak1": 1, "": 2, "k": 1, "kth": 1, "column": 1, "replace": 1, "use": 1, "next": 1, "transition": 1, "target": 1, "yk1": 1}, {"yk": 1, "estimate": 1, "maximum": 1, "value": 1, "newly": 1, "modify": 1, "kth": 1, "column": 1, "q": 1}, {"repeat": 1, "procedure": 1, "recursive": 1, "manner": 1, "start": 1, "episode": 1, "1": 1, "": 3, "code": 1, "available": 1, "httpsgithubcomsuyoungleeepisodicbackwardupdate": 1, "4": 1, "successfully": 1, "apply": 1, "backward": 1, "update": 1, "algorithm": 1, "deep": 1, "qnetwork": 1}, {"process": 1, "describe": 1, "detail": 1, "supplementary": 1, "diagram": 1, "appendix": 1, "e": 1, "use": 1, "function": 1, "approximator": 1, "update": 1, "correlate": 1, "state": 1, "sequence": 1}, {"result": 1, "observe": 1, "overestimate": 1, "value": 1, "propagate": 1, "compound": 1, "recursive": 1, "max": 1, "operations": 1}, {"solve": 1, "problem": 1, "introduce": 1, "diffusion": 1, "factor": 1, "": 1}, {"set": 1, "": 2, "0": 1, "1": 1, "take": 1, "weight": 1, "sum": 1, "new": 1, "backpropagated": 1, "value": 2, "preexist": 1, "estimate": 1}, {"one": 1, "regard": 1, "": 1, "learn": 1, "rate": 1, "temporary": 1, "qtable": 1, "level": 1, "backwardness": 1, "update": 1}, {"process": 2, "stabilize": 1, "learn": 1, "exponentially": 1, "decrease": 1, "overestimation": 1, "error": 1}, {"note": 1, "algorithm": 3, "2": 1, "": 2, "1": 2, "identical": 1, "tabular": 1, "backward": 1, "state": 1}, {"": 2, "0": 1, "algorithm": 1, "identical": 1, "episodic": 1, "onestep": 1, "dqn": 1}, {"role": 1, "": 1, "investigate": 1, "detail": 1, "experiment": 1, "section": 1, "53": 1}, {"33": 1, "": 3, "adaptive": 1, "episodic": 1, "backward": 1, "update": 1, "deep": 1, "qlearning": 1, "optimal": 1, "diffusion": 1, "factor": 1, "vary": 1, "depend": 1, "type": 1, "environment": 1, "degree": 1, "much": 1, "network": 1, "train": 1}, {"may": 1, "improve": 1, "ebu": 1, "develop": 1, "adaptive": 1, "tune": 1, "scheme": 1, "": 1}, {"without": 1, "increase": 1, "sample": 1, "complexity": 1, "propose": 1, "adaptive": 1, "single": 1, "actor": 1, "multiple": 1, "learner": 1, "version": 1, "ebu": 1}, {"generate": 1, "k": 1, "learner": 1, "network": 1, "different": 1, "diffusion": 1, "factor": 1, "single": 1, "actor": 1, "output": 1, "policy": 1}, {"episode": 1, "single": 1, "actor": 1, "select": 1, "one": 1, "learner": 1, "network": 1, "regular": 1, "sequence": 1}, {"learner": 1, "train": 1, "parallel": 1, "use": 1, "episode": 1, "sample": 1, "share": 1, "experience": 1, "replay": 1}, {"even": 1, "train": 1, "data": 1, "learners": 1, "show": 1, "different": 2, "interpretations": 1, "sample": 1, "base": 1, "level": 1, "trust": 1, "backwardly": 1, "propagate": 1, "value": 1}, {"record": 1, "episode": 1, "score": 1, "learner": 1, "train": 1}, {"every": 1, "fix": 1, "step": 1, "synchronize": 1, "learner": 2, "network": 2, "parameters": 1, "best": 1, "train": 1, "score": 1}, {"adaptive": 1, "version": 1, "ebu": 1, "present": 1, "pseudocode": 1, "appendix": 1}, {"section": 1, "52": 1, "compare": 1, "two": 1, "versions": 1, "ebu": 1, "one": 1, "constant": 1, "": 2, "another": 1, "adaptive": 1}, {"4": 1, "41": 1, "": 4, "theoretical": 1, "convergence": 1, "deterministic": 2, "mdps": 2, "prove": 1, "episodic": 1, "backward": 1, "update": 1, "0": 1, "1": 1, "define": 1, "contraction": 1, "operator": 1, "converge": 1, "optimal": 1, "qfunction": 1, "finite": 1}, {"theorem": 1, "1": 1}, {"give": 1, "finite": 1, "deterministic": 1, "tabular": 1, "mdp": 1, "": 1, "p": 1, "r": 1, "episodic": 1, "backward": 1, "update": 1, "algorithm": 2, "2": 1, "converge": 1, "optimal": 1, "qfunction": 1, "wp": 1}, {"1": 1, "long": 1, "": 5, "step": 1, "size": 1, "satisfy": 1, "robbinsmonro": 1, "condition": 1, "sample": 1, "trajectories": 1, "finite": 1, "lengths": 1, "l": 1, "el": 1, "every": 1, "state": 1, "action": 1, "pair": 1, "visit": 1, "infinitely": 1, "often": 1}, {"state": 1, "proof": 1, "theorem": 1, "1": 1, "appendix": 1, "g": 1, "furthermore": 1, "even": 1, "stochastic": 1, "environments": 1, "guarantee": 1, "convergence": 1, "episodic": 1, "backward": 1, "algorithm": 1, "sufficiently": 1, "small": 1, "": 1}, {"42": 1, "": 2, "stochastic": 1, "mdps": 1, "sto": 1, "theorem": 1, "2": 1}, {"give": 1, "finite": 1, "tabular": 1, "stochastic": 1, "mdp": 1, "": 3, "p": 1, "r": 1, "define": 1, "rmax": 1, "maximal": 1, "return": 1, "trajectory": 1, "start": 1, "state": 1, "action": 1}, {"similar": 1, "way": 1, "sto": 2, "define": 3, "rmin": 1, "rmean": 1, "minimum": 1, "mean": 1, "possible": 1, "reward": 1, "select": 1, "action": 2, "state": 2, "asub": 1, "": 6, "a0": 2, "aq": 1, "maxaa": 1, "q": 1, "set": 1, "suboptimal": 1, "aopt": 1, "aasub": 1}, {"condition": 1, "theorem": 1, "1": 2, "": 24, "q": 5, "a0": 6, "sto": 3, "ss": 2, "asub": 2, "aaopt": 2, "rmax": 1, "r": 1, "rmean": 1, "min": 1, "2": 2, "inf": 6, "episodic": 1, "backward": 1, "update": 1, "algorithm": 2, "converge": 1, "optimal": 1, "qfunction": 1, "wp": 1}, {"1": 1}, {"5": 1, "": 32, "ebu": 7, "10": 6, "onestep": 3, "dqn": 6, "nstep": 3, "relative": 4, "length": 3, "50": 6, "40": 3, "30": 3, "20": 4, "0": 3, "60": 3, "deterministic": 2, "25k": 3, "50k": 3, "75k": 3, "100k": 3, "125k": 3, "150k": 3, "175k": 3, "200k": 3, "step": 3, "wall": 3, "density": 3, "stochastic": 1, "075": 1, "05": 1, "025": 1, "b": 2, "c": 1, "figure": 1, "2": 1, "median": 1, "lengths": 1, "baselines": 1}, {"ebu": 1, "outperform": 1, "baselines": 1, "significantly": 1, "low": 1, "sample": 1, "regime": 1, "high": 1, "wall": 1, "density": 1}, {"c": 1, "median": 1, "relative": 1, "lengths": 1, "ebu": 1, "baseline": 1, "algorithms": 1, "mnist": 1, "maze": 1, "stochastic": 1, "transition": 1}, {"main": 1, "intuition": 1, "theorem": 1, "": 1, "act": 1, "learn": 1, "rate": 1, "backward": 1, "target": 1, "therefore": 1, "mitigate": 1, "collision": 1, "max": 1, "operator": 1, "stochastic": 1, "transition": 1}, {"5": 1, "51": 1, "": 10, "experimental": 1, "result": 1, "2d": 2, "mnist": 1, "maze": 2, "deterministicstochastic": 1, "mdps": 1, "test": 1, "algorithm": 1, "environment": 1}, {"start": 1, "initial": 1, "position": 2, "0": 2, "agent": 1, "navigate": 1, "maze": 1, "reach": 1, "goal": 1, "9": 2}, {"minimize": 1, "correlation": 1, "neighbor": 1, "figure": 1, "3": 1, "2d": 1, "mnist": 2, "state": 2, "use": 1, "dataset": 1, "9": 1, "representation": 1}, {"agent": 1, "maze": 1, "receive": 1, "coordinate": 1, "position": 1, "two": 1, "mnist": 1, "image": 1, "state": 1, "representation": 1}, {"train": 1, "environments": 1, "10": 2, "mazes": 1, "randomly": 1, "place": 1, "wall": 1}, {"assign": 1, "reward": 2, "1000": 1, "reach": 1, "goal": 1, "1": 1, "bump": 1, "wall": 1}, {"wall": 2, "density": 1, "indicate": 1, "probability": 1, "position": 1}, {"wall": 2, "density": 1, "generate": 1, "50": 1, "random": 1, "mazes": 1, "different": 1, "locations": 1}, {"train": 1, "total": 1, "50": 1, "independent": 1, "agents": 1, "one": 1, "maze": 1, "200000": 1, "step": 1}, {"performance": 1, "metric": 1, "relative": 1, "length": 3, "define": 1, "lrel": 1, "": 2, "lagent": 2, "loracle": 2, "ratio": 1, "agents": 1, "path": 2, "grind": 1, "truth": 1, "shortest": 1, "reach": 1, "goal": 1}, {"detail": 1, "hyperparameters": 1, "network": 1, "structure": 1, "describe": 1, "appendix": 1, "": 1, "compare": 1, "ebu": 1, "uniform": 1, "random": 1, "sample": 1, "onestep": 1, "dqn": 2, "nstep": 1}, {"nstep": 1, "dqn": 1, "set": 1, "value": 1, "n": 1, "length": 1, "episode": 1}, {"since": 1, "three": 1, "algorithms": 1, "eventually": 1, "achieve": 1, "median": 1, "relative": 2, "lengths": 2, "1": 2, "end": 1, "train": 1, "report": 1, "100000": 1, "step": 1, "table": 1}, {"onestep": 1, "dqn": 1, "perform": 1, "worst": 1, "configurations": 1, "imply": 1, "inefficiency": 1, "uniform": 1, "sample": 1, "update": 1, "environments": 1, "sparse": 1, "delay": 1, "reward": 1}, {"wall": 1, "density": 1, "increase": 1, "become": 1, "important": 1, "agent": 1, "learn": 1, "correct": 1, "decisions": 1, "bottleneck": 1, "position": 1}, {"n": 1, "step": 1, "dqn": 2, "show": 1, "best": 1, "performance": 1, "low": 1, "wall": 2, "density": 2, "increase": 1, "ebu": 1, "significantly": 1, "outperform": 1, "nstep": 1}, {"addition": 1, "run": 1, "experiment": 1, "stochastic": 1, "transition": 1}, {"assign": 1, "10": 1, "probability": 1, "side": 1, "action": 2, "four": 1, "valid": 1}, {"example": 1, "agent": 1, "take": 1, "action": 1, "10": 2, "chance": 2, "transit": 2, "leave": 1, "state": 2, "right": 1}, {"figure": 1, "2": 1, "c": 1, "see": 1, "ebu": 1, "agent": 1, "outperform": 1, "baselines": 1, "stochastic": 1, "environment": 1, "well": 1}, {"table": 1, "1": 1, "relative": 1, "lengths": 1, "mean": 1, "": 9, "median": 1, "50": 2, "deterministic": 1, "mnist": 1, "maze": 1, "100000": 1, "step": 2, "wall": 1, "density": 1, "20": 1, "30": 1, "40": 2, "ebu": 1, "544": 1, "814": 1, "861": 1, "551": 1, "10": 1, "242": 1, "303": 1, "252": 1, "234": 1, "onestep": 1, "dqn": 2, "1440": 1, "925": 2, "2563": 1, "2103": 1, "2545": 1, "2271": 1, "2236": 1, "1662": 1, "6": 1, "n": 1, "326": 1, "224": 1, "888": 1, "332": 2, "896": 1, "350": 1, "1132": 1, "312": 2, "39182": 1, "7882": 1, "4159": 1, "683": 1, "367": 1, "304": 1, "92": 1, "72": 1, "56": 1, "49": 1, "14": 2, "01": 1, "00": 1, "07": 1, "25": 1, "31": 1, "32": 1, "52": 1, "58": 1, "60": 1, "64": 1, "78": 1, "83": 1, "103": 1, "117": 1, "151": 1, "183": 1, "201": 1, "204": 1, "221": 1, "253": 1, "286": 1, "306": 1, "310": 1, "358": 1, "475": 1, "511": 1, "567": 1, "632": 1, "639": 1, "748": 1, "890": 1, "953": 1, "1235": 1, "krull": 1, "enduro": 1, "name": 1, "game": 1, "qbert": 1, "robotank": 1, "asterix": 1, "tennis": 1, "ms": 1, "pacman": 1, "montezumas": 1, "rev": 1}, {"amidar": 1, "kangaroo": 1, "chopper": 1, "command": 1, "hero": 1, "alien": 1, "private": 1, "eye": 1, "river": 1, "raid": 1, "seaquest": 1, "asteroids": 1, "battle": 1, "zone": 1, "demon": 1, "attack": 1, "venture": 1, "gravitar": 1, "frostbite": 1, "ice": 1, "hockey": 1, "kungfu": 1, "master": 1, "space": 1, "invaders": 1, "beam": 1, "rider": 1, "centipede": 1, "freeway": 1, "bowl": 1, "zaxxon": 1, "crazy": 1, "climber": 1, "bank": 1, "heist": 1, "star": 1, "gunner": 1, "wizard": 1, "wor": 1, "tutankham": 1, "fish": 1, "derby": 1, "assault": 1, "time": 1, "pilot": 1, "pong": 1, "road": 1, "runner": 1, "jamesbond": 1, "gopher": 1, "box": 1, "double": 1, "dunk": 1, "atlantis": 1, "breakout": 1, "video": 1, "pinball": 1, "figure": 1, "4": 2, "relative": 1, "score": 1, "adaptive": 1, "ebu": 1, "random": 2, "seed": 2, "compare": 1, "nature": 1, "dqn": 1, "8": 1, "percents": 1, "": 1, "train": 1, "10m": 1, "frame": 1}, {"52": 1, "": 2, "49": 1, "game": 1, "atari": 1, "2600": 1, "environment": 2, "deterministic": 1, "mdps": 1, "arcade": 1, "learn": 1, "2": 1, "one": 1, "popular": 1, "rl": 1, "benchmarks": 1, "diverse": 1, "set": 1, "challenge": 1, "task": 1}, {"use": 1, "set": 1, "49": 1, "atari": 1, "2600": 1, "game": 1, "evaluate": 1, "nature": 1, "dqn": 1, "paper": 1, "14": 1}, {"select": 1, "": 2, "05": 1, "ebu": 1, "constant": 1, "diffusion": 1, "factor": 1}, {"adaptive": 1, "ebu": 1, "train": 1, "k": 1, "": 2, "11": 1, "parallel": 1, "learners": 1, "diffusion": 1, "factor": 1, "00": 1, "01": 1}, {"": 1}, {"": 1, "10": 1}, {"synchronize": 1, "learners": 1, "end": 1, "epoch": 1, "025m": 1, "frame": 1}, {"compare": 1, "algorithm": 1, "four": 1, "baselines": 1, "nature": 1, "dqn": 1, "14": 1, "prioritize": 1, "experience": 1, "replay": 1, "per": 1, "17": 1, "retrace": 1, "15": 1, "optimality": 1, "tighten": 1, "ot": 1, "8": 1}, {"train": 1, "ebu": 2, "baselines": 1, "10m": 1, "frame": 2, "additional": 1, "20m": 1, "adaptive": 1, "49": 1, "atari": 1, "game": 1, "network": 1, "structure": 1, "hyperparameters": 1, "evaluation": 1, "methods": 1, "use": 1, "nature": 1, "dqn": 1}, {"choice": 1, "small": 1, "number": 1, "train": 1, "step": 1, "make": 1, "investigate": 1, "sample": 1, "efficiency": 1, "algorithm": 1, "follow": 1, "16": 1, "8": 1}, {"report": 1, "mean": 1, "result": 1, "4": 1, "random": 2, "seed": 2, "adaptive": 1, "ebu": 1, "8": 1, "baselines": 1}, {"detail": 1, "specifications": 1, "baseline": 1, "describe": 1, "appendix": 1, "first": 1, "show": 1, "improvement": 1, "adaptive": 1, "ebu": 1, "nature": 1, "dqn": 1, "10m": 1, "frame": 1, "49": 1, "game": 1, "figure": 1, "4": 1}, {"compare": 1, "performance": 1, "agent": 1, "baselines": 1, "use": 1, "follow": 1, "relative": 1, "score": 2, "scorebaseline": 2, "maxscorehumanagent": 1, "": 1, "scorerandom": 1, "21": 1}, {"measure": 1, "show": 1, "well": 1, "agent": 1, "perform": 1, "task": 2, "compare": 1, "level": 1, "difficulty": 1}, {"ebu": 2, "": 2, "05": 1, "adaptive": 1, "outperform": 1, "nature": 1, "dqn": 1, "33": 1, "39": 1, "game": 2, "49": 1, "respectively": 1}, {"large": 1, "amount": 1, "improvements": 1, "game": 2, "atlantis": 1, "breakout": 1, "video": 1, "pinball": 1, "highly": 1, "surpass": 1, "minor": 1, "fail": 1}, {"score": 3, "": 3, "agent": 1, "random": 1, "use": 2, "humannormalized": 1, "scorehuman": 1, "scorerandom": 1, "20": 1, "widely": 1, "metric": 1, "make": 1, "appletoapple": 1, "comparison": 1, "atari": 1, "domain": 1}, {"report": 1, "mean": 1, "median": 1, "humannormalized": 1, "score": 1, "49": 1, "game": 1, "table": 1, "2": 1}, {"result": 1, "signify": 1, "algorithm": 1, "outperform": 1, "baselines": 1, "mean": 1, "median": 1, "humannormalized": 1, "score": 1}, {"per": 1, "retrace": 1, "show": 1, "lot": 1, "improvements": 1, "small": 1, "number": 1, "train": 1, "step": 1, "10m": 1, "frame": 1}, {"since": 1, "ot": 1, "calculate": 1, "qvalues": 1, "neighbor": 1, "state": 1, "compare": 1, "generate": 1, "penalty": 1, "term": 1, "require": 1, "3": 1, "time": 2, "train": 1, "nature": 1, "dqn": 1}, {"however": 1, "ebu": 2, "perform": 1, "iterative": 1, "episodic": 1, "update": 1, "use": 1, "temporary": 1, "qtable": 1, "share": 1, "transition": 1, "episode": 1, "almost": 1, "computational": 1, "cost": 1, "nature": 1, "dqn": 1}, {"7": 1, "": 14, "gopher": 2, "mean": 2, "qvalues": 3, "state": 2, "test": 4, "episode": 5, "25": 1, "20": 1, "15": 1, "10": 3, "5": 2, "0": 4, "breakout": 2, "8": 1, "6": 1, "4": 1, "2": 1, "500": 1, "1000": 1, "1500": 1, "2000": 1, "2500": 1, "3000": 1, "score": 3, "ebu": 2, "05": 1, "50": 1, "100": 1, "150": 1, "nature": 1, "dqn": 1, "200": 1, "figure": 1, "average": 1, "stateaction": 1, "pair": 1}, {"significant": 1, "result": 1, "ebu": 1, "": 2, "05": 1, "require": 1, "10m": 1, "frame": 2, "train": 2, "achieve": 1, "mean": 1, "humannormalized": 1, "score": 1, "report": 1, "nature": 1, "dqn": 1, "200m": 1}, {"although": 1, "10m": 1, "frame": 2, "enough": 1, "achieve": 2, "median": 2, "score": 2, "adaptive": 1, "ebu": 1, "train": 1, "20m": 1, "normalize": 1}, {"result": 1, "signify": 1, "efficacy": 1, "backward": 1, "value": 1, "propagation": 1, "early": 1, "stag": 1, "train": 1}, {"raw": 1, "score": 1, "49": 1, "game": 1, "summarize": 1, "appendix": 1, "b": 1}, {"learn": 1, "curve": 1, "adaptive": 1, "ebu": 1, "49": 1, "game": 1, "report": 1, "appendix": 1, "c": 1, "table": 1, "2": 1, "summary": 1, "train": 1, "time": 1, "humannormalized": 1, "performance": 1}, {"train": 2, "time": 2, "refer": 1, "total": 1, "require": 1, "49": 1, "game": 1, "10m": 1, "frame": 1, "use": 1, "single": 2, "nvidia": 1, "titan": 1, "xp": 1, "random": 1, "seed": 1}, {"use": 1, "multigpus": 1, "train": 1, "learners": 1, "adaptive": 1, "ebu": 1, "parallel": 1}, {"": 1, "result": 2, "ot": 1, "differ": 1, "report": 1, "8": 1, "due": 1, "different": 1, "evaluation": 1, "methods": 1, "ie": 1}, {"limit": 1, "maximum": 2, "number": 1, "step": 1, "test": 1, "episode": 1, "take": 1, "score": 1, "random": 1, "seed": 1}, {"": 1, "report": 1, "score": 1, "nature": 1, "dqn": 1, "200m": 1, "14": 1}, {"algorithm": 1, "frame": 1, "ebu": 4, "": 14, "05": 1, "10m": 6, "adaptive": 2, "nature": 2, "dqn": 2, "per": 1, "retrace": 1, "ot": 1, "20m": 1, "200m": 1, "53": 1, "train": 1, "time": 1, "hours": 1, "152": 1, "203": 1, "138": 1, "146": 1, "154": 1, "407": 1, "450": 1, "mean": 1, "25355": 1, "27578": 1, "13395": 1, "15657": 1, "9377": 1, "16266": 1, "34799": 1, "24106": 1, "median": 1, "5155": 1, "6380": 1, "4042": 1, "4086": 1, "4199": 1, "4942": 1, "9250": 1, "9352": 1, "analysis": 1, "role": 1, "diffusion": 1, "factor": 1, "section": 1, "make": 1, "comparisons": 1, "algorithms": 1}, {"ebu": 1, "": 2, "10": 1, "work": 1, "best": 1, "mnist": 2, "maze": 1, "environment": 1, "use": 1, "image": 1, "state": 2, "representation": 1, "allow": 1, "consecutive": 1, "exhibit": 1, "little": 1, "correlation": 1}, {"however": 1, "atari": 1, "domain": 1, "consecutive": 1, "state": 1, "often": 1, "different": 1, "scale": 1, "pixels": 1}, {"consequence": 1, "ebu": 2, "": 4, "10": 1, "underperform": 1, "05": 1, "atari": 1, "game": 1}, {"order": 1, "analyze": 1, "phenomenon": 1, "evaluate": 1, "qvalues": 1, "learn": 1, "end": 1, "train": 1, "epoch": 1}, {"report": 1, "test": 2, "episode": 2, "score": 1, "correspond": 1, "mean": 1, "qvalues": 1, "transition": 1, "within": 1, "figure": 1, "5": 1}, {"notice": 1, "ebu": 1, "": 2, "10": 1, "train": 1, "output": 1, "highly": 1, "overestimate": 1, "qvalues": 1, "compare": 1, "actual": 1, "return": 1}, {"since": 1, "ebu": 2, "method": 1, "perform": 1, "recursive": 1, "max": 1, "operations": 1, "output": 1, "higher": 1, "possibly": 1, "overestimate": 1, "qvalues": 1, "nature": 1, "dqn": 1}, {"result": 1, "indicate": 1, "sequentially": 1, "update": 1, "correlate": 1, "state": 1, "8": 1, "": 1, "overestimate": 1, "value": 1, "may": 1, "destabilize": 1, "learn": 1, "process": 1}, {"however": 1, "result": 1, "clearly": 1, "imply": 1, "ebu": 1, "": 2, "05": 1, "relatively": 1, "free": 1, "overestimation": 1, "problem": 1}, {"next": 1, "investigate": 1, "efficacy": 1, "use": 1, "adaptive": 1, "diffusion": 1, "factor": 1}, {"figure": 1, "6": 1, "present": 1, "adaptive": 1, "ebu": 1, "adapt": 1, "diffusion": 1, "factor": 1, "course": 1, "train": 1, "breakout": 1}, {"early": 1, "stage": 1, "train": 1, "agent": 1, "barely": 1, "succeed": 1, "break": 1, "single": 1, "brick": 1}, {"high": 1, "": 1, "close": 1, "1": 1, "value": 1, "directly": 1, "propagate": 1, "reward": 1, "state": 2, "agent": 1, "bounce": 1, "ball": 1}, {"note": 1, "performance": 1, "adaptive": 1, "ebu": 2, "follow": 1, "": 2, "10": 1, "5m": 1, "frame": 1}, {"train": 1, "proceed": 1, "agent": 1, "encounter": 1, "reward": 1, "various": 1, "trajectories": 1, "may": 1, "cause": 1, "overestimation": 1}, {"consequence": 1, "discover": 1, "agent": 1, "anneal": 1, "diffusion": 1, "factor": 1, "lower": 1, "value": 1, "05": 1}, {"trend": 1, "diffusion": 1, "factor": 1, "adapt": 1, "differ": 1, "game": 2}, {"refer": 1, "diffusion": 2, "factor": 2, "curve": 1, "49": 1, "game": 1, "appendix": 1, "c": 1, "check": 1, "adaptive": 1, "ebu": 1, "select": 1, "best": 1}, {"500": 1, "": 25, "raw": 2, "score": 3, "400": 1, "breakout": 3, "meanstd": 2, "4": 2, "seed": 2, "adaptive": 2, "diffusion": 1, "factor": 1, "10": 2, "ebu": 2, "dqn": 2, "200m": 1, "08": 1, "300": 1, "06": 1, "200": 3, "04": 1, "100": 3, "02": 1, "0": 1, "00": 3, "25": 2, "50": 2, "75": 2, "125": 2, "150": 2, "175": 2, "million": 2, "frame": 2, "b": 1, "figure": 1, "6": 1, "test": 1}, {"mean": 1, "standard": 1, "deviation": 1, "4": 1, "random": 1, "seed": 1, "plot": 1}, {"b": 1, "adaptive": 2, "diffusion": 1, "factor": 1, "ebu": 1, "breakout": 1}, {"6": 1, "": 2, "conclusion": 1, "work": 1, "propose": 1, "episodic": 1, "backward": 2, "update": 2, "sample": 1, "transition": 1, "episode": 2, "value": 1, "recursively": 1, "manner": 1}, {"algorithm": 1, "achieve": 1, "fast": 1, "stable": 1, "learn": 1, "due": 1, "efficient": 1, "value": 1, "propagation": 1}, {"theoretically": 1, "prove": 1, "convergence": 1, "method": 1, "experimentally": 1, "show": 1, "algorithm": 1, "outperform": 1, "baselines": 1, "many": 1, "complex": 1, "domains": 1, "require": 1, "10": 1, "sample": 1}, {"since": 1, "work": 1, "differ": 1, "dqn": 1, "term": 1, "target": 1, "generation": 1, "hope": 1, "make": 1, "improvements": 1, "combine": 1, "successful": 1, "deep": 1, "reinforcement": 1, "learn": 1, "methods": 1}, {"acknowledgments": 1, "work": 1, "support": 1, "ict": 1, "rd": 1, "program": 1, "msipiitp": 1}, {"2016000563": 1, "research": 1, "adaptive": 1, "machine": 1, "learn": 1, "technology": 1, "development": 1, "intelligent": 1, "autonomous": 1, "digital": 1, "companion": 1, "": 1, "reference": 1, "1": 1, "arjonamedina": 1, "j": 1}, {"gillhofer": 1, "widrich": 1, "unterthiner": 1, "hochreiter": 1, "rudder": 1, "return": 1, "decomposition": 1, "delay": 1, "reward": 1}, {"arxiv": 1, "preprint": 1, "arxiv180607857": 1, "2018": 1}, {"2": 1, "bellemare": 1, "g": 1, "naddaf": 1, "veness": 1, "j": 1, "bowl": 1, "arcade": 1, "learn": 1, "environment": 1, "evaluation": 1, "platform": 1, "general": 1, "agents": 1}, {"journal": 1, "artificial": 1, "intelligence": 1, "research": 1, "47253279": 1, "2013": 1}, {"9": 1, "": 1, "3": 1, "bellemare": 1, "g": 2, "srinivasan": 1, "ostrovski": 1, "schaul": 1, "saxton": 1, "munos": 1, "r": 1, "unify": 1, "countbased": 1, "exploration": 1, "intrinsic": 1, "motivation": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "nip": 1, "14711479": 1, "2016": 1}, {"4": 1, "bertsekas": 1, "p": 1, "tsitsiklis": 1, "j": 1, "n": 1, "neurodynamic": 1, "program": 1}, {"athena": 1, "scientific": 1, "1996": 1}, {"5": 1, "blundell": 1, "c": 1, "uria": 1, "b": 1, "pritzel": 1, "li": 1, "ruderman": 1, "leibo": 1, "j": 1}, {"z": 1, "rae": 1, "jwierstra": 1, "hassabis": 1, "modelfree": 1, "episodic": 1, "control": 1}, {"arxiv": 1, "preprint": 1, "arxiv160604460": 1, "2016": 1}, {"6": 1, "harutyunyan": 1, "bellemare": 1, "g": 1, "stepleton": 1, "munos": 1, "r": 1, "q": 1, "offpolicy": 1, "corrections": 1}, {"international": 1, "conference": 1, "algorithmic": 1, "learn": 1, "theory": 1, "alt": 1, "305320": 1, "2016": 1}, {"7": 1, "hansen": 1, "pritzel": 1, "sprechmann": 1, "p": 1, "barreto": 1, "blundell": 1, "c": 1, "fast": 1, "deep": 1, "reinforcement": 1, "learn": 1, "use": 1, "online": 1, "adjustments": 1, "past": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "nip": 1, "1059010600": 1, "2018": 1, "8": 1, "f": 1, "liu": 1, "schwing": 1, "g": 1, "peng": 1, "j": 1}, {"learn": 2, "play": 1, "day": 1, "faster": 1, "deep": 1, "reinforcement": 1, "optimality": 1, "tighten": 1}, {"international": 1, "conference": 1, "learn": 1, "representations": 1, "iclr": 1, "2017": 1}, {"9": 1, "lecun": 1, "bottou": 1, "l": 1, "bengio": 1, "haffner": 1, "p": 1, "gradientbased": 1, "learn": 1, "apply": 1, "document": 1, "recognition": 1}, {"institute": 1, "electrical": 1, "electronics": 1, "engineer": 1, "ieee": 1, "86": 1, "22782324": 1, "1998": 1}, {"10": 1, "lengyel": 1, "dayan": 1, "p": 1, "hippocampal": 1, "contributions": 1, "control": 1, "third": 1, "way": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "nip": 1, "889896": 1, "2007": 1}, {"11": 1, "lin": 1, "lj": 1}, {"program": 1, "robots": 1, "use": 1, "reinforcement": 1, "learn": 1, "teach": 1}, {"association": 1, "advancement": 1, "artificial": 1, "intelligence": 1, "aaai": 1, "781786": 1, "1991": 1}, {"12": 1, "lin": 1, "lj": 1}, {"selfimproving": 1, "reactive": 1, "agents": 1, "base": 1, "reinforcement": 1, "learn": 1, "plan": 1, "teach": 1}, {"machine": 1, "learn": 1, "293321": 1, "1992": 1}, {"13": 1, "melo": 1, "f": 1, "convergence": 1, "qlearning": 1, "simple": 1, "proof": 1, "institute": 1, "systems": 1, "robotics": 1, "tech": 1}, {"rep": 1, "2001": 1}, {"14": 1, "mnih": 1, "v": 1, "kavukcuoglu": 1, "k": 1, "silver": 1, "rusu": 1}, {"veness": 1, "j": 1, "bellemare": 1, "g": 2, "grave": 1, "riedmiller": 1, "fidjeland": 1, "k": 1, "ostrovski": 1, "petersen": 1, "beattie": 1, "c": 1, "sadik": 1, "antonoglou": 1, "king": 1, "h": 1, "kumaran": 1, "wierstra": 1, "legg": 1, "hassabis": 1, "humanlevel": 1, "control": 1, "deep": 1, "reinforcement": 1, "learn": 1}, {"nature": 1, "5187540529533": 1, "2015": 1}, {"15": 1, "munos": 1, "r": 1, "stepleton": 1, "harutyunyan": 1, "bellemare": 1, "g": 1, "safe": 1, "efficient": 1, "offpolicy": 1, "reinforcement": 1, "learn": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "nip": 1, "10461054": 1, "2016": 1}, {"16": 1, "pritzel": 1, "uria": 1, "b": 1, "srinivasan": 1, "puigdomenech": 1, "vinyals": 1, "hassabis": 1, "wierstra": 1, "blundell": 1, "c": 1, "neural": 1, "episodic": 1, "control": 1}, {"international": 1, "conference": 1, "machine": 1, "learn": 1, "icml": 1, "28272836": 1, "2017": 1}, {"17": 1, "schaul": 1, "quan": 1, "j": 1, "antonoglou": 1, "silver": 1, "prioritize": 1, "experience": 1, "replay": 1}, {"international": 1, "conference": 1, "learn": 1, "representations": 1, "iclr": 1, "2016": 1}, {"18": 1, "silver": 1, "huang": 1, "maddison": 1, "c": 1, "j": 3, "guez": 1, "sifre": 1, "l": 1, "van": 1, "den": 1, "driessche": 1, "g": 1, "schrittwieser": 1, "antonoglou": 1, "panneershelvam": 1, "v": 1, "lanctot": 1, "dieleman": 1, "grewe": 1, "nham": 1, "kalchbrenner": 1, "n": 1, "sutskever": 1, "lillicrap": 1, "leach": 1, "kavukcuoglu": 1, "k": 1, "graepel": 1, "hassabis": 1, "master": 1, "game": 1, "go": 1, "deep": 1, "neural": 1, "network": 1, "tree": 1, "search": 1}, {"nature": 1, "529484489": 1, "2016": 1}, {"19": 1, "sutton": 1, "r": 1, "barto": 1, "g": 1, "reinforcement": 1, "learn": 1, "introduction": 1}, {"mit": 1, "press": 1, "1998": 1}, {"20": 1, "van": 1, "hasselt": 1, "h": 1, "guez": 1, "silver": 1, "deep": 1, "reinforcement": 1, "learn": 1, "double": 1, "qlearning": 1}, {"association": 1, "advancement": 1, "artificial": 1, "intelligence": 1, "aaai": 1, "20942100": 1, "2016": 1}, {"21": 1, "wang": 1, "z": 1, "schaul": 1, "hessel": 1, "van": 1, "hasselt": 1, "h": 1, "lanctot": 1, "de": 1, "freitas": 1, "n": 1, "duel": 1, "network": 1, "architectures": 1, "deep": 1, "reinforcement": 1, "learn": 1}, {"international": 1, "conference": 1, "machine": 1, "learn": 1, "icml": 1, "19952003": 1, "2016": 1}, {"22": 1, "watkins": 1, "c": 2, "j": 1, "h": 1, "learn": 1, "delay": 1, "reward": 1}, {"phd": 1, "thesis": 1, "university": 1, "cambridge": 1, "england": 1, "1989": 1}, {"23": 1, "watkins": 1, "c": 2, "j": 1, "h": 1, "dayan": 1, "p": 1, "qlearning": 1}, {"machine": 1, "learn": 1, "272292": 1, "1992": 1}, {"10": 1}]
[{"thompson": 1, "sample": 2, "information": 2, "relaxation": 2, "penalties": 1, "seungki": 1, "min": 1, "columbia": 3, "business": 3, "school": 3, "": 3, "costis": 1, "maglaras": 1, "ciamac": 1, "c": 1, "moallemi": 1, "abstract": 1, "consider": 1, "finitehorizon": 1, "multiarmed": 1, "bandit": 1, "mab": 1, "problem": 1, "bayesian": 1, "set": 1, "propose": 1, "framework": 1}, {"framework": 1, "define": 1, "intuitive": 1, "family": 1, "control": 1, "policies": 1, "include": 1, "thompson": 1, "sample": 1, "ts": 1, "bayesian": 1, "optimal": 1, "policy": 1, "endpoints": 1}, {"analogous": 1, "ts": 1, "decision": 1, "epoch": 1, "pull": 1, "arm": 1, "best": 2, "respect": 1, "randomly": 1, "sample": 2, "parameters": 1, "algorithms": 1, "entire": 1, "future": 1, "reward": 1, "realizations": 1, "take": 1, "correspond": 1, "action": 1}, {"however": 1, "do": 1, "presence": 1, "penalties": 1, "seek": 1, "compensate": 1, "availability": 1, "future": 1, "information": 1}, {"develop": 1, "several": 1, "novel": 1, "policies": 1, "performance": 2, "bound": 1, "mab": 1, "problems": 1, "vary": 1, "term": 1, "improve": 1, "increase": 1, "computational": 1, "complexity": 1, "two": 1, "endpoints": 1}, {"policies": 1, "view": 1, "natural": 1, "generalizations": 1, "ts": 1, "simultaneously": 1, "incorporate": 1, "knowledge": 1, "time": 1, "horizon": 1, "explicitly": 1, "consider": 1, "explorationexploitation": 1, "tradeoff": 1}, {"prove": 1, "associate": 1, "structural": 1, "result": 1, "performance": 1, "bound": 1, "suboptimality": 1, "gap": 1}, {"numerical": 1, "experiment": 1, "suggest": 1, "new": 1, "class": 1, "policies": 1, "perform": 1, "well": 1, "particular": 1, "settings": 1, "finite": 1, "time": 1, "horizon": 1, "introduce": 1, "significant": 1, "explorationexploitation": 1, "tension": 1, "problem": 1}, {"1": 1, "": 2, "introduction": 1, "date": 1, "back": 1, "earliest": 1, "work": 1, "2": 1, "10": 1, "multiarmed": 1, "bandit": 1, "mab": 1, "problems": 1, "consider": 1, "within": 1, "bayesian": 1, "framework": 1, "unknown": 1, "parameters": 1, "model": 1, "random": 1, "variables": 1, "draw": 1, "know": 1, "prior": 1, "distribution": 1}, {"set": 1, "problem": 1, "view": 1, "markov": 1, "decision": 1, "process": 1, "mdp": 1, "state": 2, "information": 1, "describe": 1, "beliefs": 1, "unknown": 1, "parameters": 1, "evolve": 1, "stochastically": 1, "upon": 1, "play": 1, "arm": 1, "accord": 1, "bay": 1, "rule": 1}, {"objective": 1, "expect": 1, "performance": 1, "expectation": 1, "take": 1, "respect": 1, "prior": 1, "distribution": 1, "unknown": 1, "parameters": 1, "bayesian": 1, "optimal": 1, "policy": 1, "pt": 1, "characterize": 1, "bellman": 1, "equations": 1, "immediately": 1, "follow": 1, "mdp": 1, "formulation": 1}, {"discount": 1, "infinitehorizon": 1, "set": 1, "celebrate": 1, "gittins": 1, "index": 1, "10": 1, "determine": 1, "optimal": 1, "policy": 1, "despite": 1, "fact": 1, "computation": 1, "still": 1, "challenge": 1}, {"nondiscounted": 1, "finitehorizon": 1, "set": 1, "consider": 1, "problem": 1, "become": 1, "difficult": 1, "1": 1, "except": 1, "special": 1, "case": 1, "bellman": 1, "equations": 1, "neither": 1, "analytically": 1, "numerically": 1, "tractable": 1, "due": 1, "curse": 1, "dimensionality": 1}, {"paper": 1, "focus": 1, "bayesian": 1, "set": 1, "attempt": 1, "apply": 1, "ideas": 1, "dynamic": 1, "program": 1, "dp": 1, "develop": 1, "tractable": 1, "policies": 1, "good": 1, "performance": 1}, {"end": 1, "apply": 1, "idea": 1, "information": 1, "relaxation": 1, "4": 1, "technique": 1, "provide": 1, "systematic": 1, "way": 1, "obtain": 1, "performance": 1, "bound": 1, "optimal": 1, "policy": 1}, {"multiperiod": 1, "stochastic": 1, "dp": 1, "33rd": 1, "conference": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "neurips": 1, "2019": 1, "vancouver": 1, "canada": 1}, {"problems": 1, "admissible": 1, "policies": 1, "require": 1, "make": 1, "decisions": 1, "base": 1, "previously": 1, "reveal": 1, "information": 1}, {"idea": 1, "information": 1, "relaxation": 2, "consider": 1, "nonanticipativity": 1, "constraint": 1, "impose": 1, "policy": 1, "space": 1, "relax": 1, "simultaneously": 1, "introduce": 1, "penalty": 1, "objective": 1, "usual": 1, "lagrangian": 1, "relaxations": 1, "convex": 1, "duality": 1, "theory": 1}, {"relaxation": 1, "decision": 1, "maker": 1, "dm": 1, "allow": 1, "access": 1, "future": 1, "information": 1, "ask": 1, "solve": 1, "optimization": 1, "problem": 1, "maximize": 1, "total": 1, "reward": 1, "presence": 1, "penalties": 1, "punish": 1, "violation": 1, "nonanticipativity": 1, "constraint": 1}, {"penalties": 2, "satisfy": 1, "condition": 1, "dual": 1, "feasibility": 1, "formally": 1, "define": 1, "3": 1, "expect": 2, "value": 1, "maximal": 1, "reward": 1, "adjust": 1, "provide": 1, "upper": 1, "bind": 1, "performance": 1, "nonanticipating": 1, "optimal": 1, "policy": 1}, {"idea": 1, "relax": 1, "nonanticipativity": 1, "constraint": 1, "study": 1, "different": 1, "contexts": 1, "17": 1, "6": 1, "18": 1, "11": 1, "later": 1, "formulate": 1, "formal": 1, "framework": 1, "4": 1, "upon": 1, "methodology": 1, "develop": 1}, {"framework": 1, "apply": 1, "variety": 1, "applications": 1, "include": 1, "optimal": 1, "stop": 1, "problems": 1, "7": 1, "linearquadratic": 1, "control": 1, "12": 1, "dynamic": 1, "portfolio": 1, "execution": 1, "13": 1, "see": 1, "3": 1}, {"typically": 1, "application": 1, "method": 1, "specific": 1, "class": 1, "mdps": 1, "require": 1, "custom": 1, "analysis": 1}, {"particular": 1, "always": 1, "easy": 1, "determine": 1, "penalty": 1, "function": 1, "1": 1, "yield": 1, "relaxation": 1, "tractable": 1, "solve": 1, "2": 1, "provide": 1, "tight": 1, "upper": 1, "bound": 1, "performance": 1, "optimal": 1, "policy": 1}, {"moreover": 1, "establish": 1, "information": 1, "relaxation": 1, "theory": 1, "focus": 1, "upper": 1, "bound": 1, "provide": 1, "guidance": 1, "development": 1, "tractable": 1, "policies": 1}, {"contribution": 1, "apply": 1, "information": 1, "relaxation": 1, "techniques": 1, "finitehorizon": 1, "stochastic": 1, "mab": 1, "problem": 1, "explicitly": 1, "exploit": 1, "structure": 1, "bayesian": 1, "learn": 1, "process": 1}, {"particular": 1, "1": 1, "propose": 1, "series": 1, "information": 1, "relaxations": 1, "penalties": 1, "increase": 1, "computational": 2, "complexity": 2, "2": 1, "systematically": 1, "obtain": 1, "upper": 1, "bound": 1, "best": 1, "achievable": 1, "expect": 1, "performance": 1, "trade": 1, "tightness": 1, "3": 1, "develop": 1, "associate": 1, "randomize": 1, "policies": 1, "generalize": 1, "thompson": 1, "sample": 1, "ts": 1, "finitehorizon": 1, "set": 1}, {"framework": 1, "call": 1, "information": 2, "relaxation": 1, "sample": 1, "penalty": 1, "function": 1, "relaxations": 1, "determine": 1, "one": 2, "policy": 1, "performance": 1, "bind": 1, "give": 1, "particular": 1, "problem": 1, "instance": 1, "specify": 1, "time": 1, "horizon": 1, "prior": 1, "beliefs": 1}, {"base": 1, "case": 1, "algorithms": 1, "ts": 1, "21": 1, "conventional": 1, "regret": 2, "benchmark": 1, "use": 1, "bayesian": 1, "analysis": 1, "since": 1, "15": 1}, {"extreme": 1, "optimal": 1, "policy": 1, "pt": 1, "expect": 1, "performance": 1, "follow": 1, "ideal": 1, "penalty": 1, "surprisingly": 1, "intractable": 1, "compute": 1}, {"pick": 1, "increasingly": 1, "strict": 1, "information": 1, "penalties": 1, "improve": 1, "policy": 1, "associate": 1, "bind": 1, "two": 1, "extremes": 1, "ts": 1, "pt": 1}, {"example": 1, "one": 1, "algorithms": 1, "rs": 1, "fh": 1, "provide": 1, "simple": 1, "modification": 1, "ts": 1, "naturally": 1, "incorporate": 1, "time": 1, "horizon": 1, "": 1}, {"recall": 1, "ts": 1, "make": 1, "decision": 1, "base": 1, "sample": 1, "parameters": 2, "posterior": 1, "distribution": 1, "epoch": 1, "focus": 1, "fact": 1, "know": 1, "informative": 1, "infinite": 1, "number": 1, "future": 1, "reward": 1, "observations": 1, "term": 1, "best": 1, "arm": 1, "identification": 1}, {"contrast": 1, "rs": 1, "fh": 1, "make": 1, "decision": 1, "base": 2, "future": 2, "bayesian": 1, "estimate": 1, "update": 1, "": 1, "1": 1, "reward": 2, "realizations": 1, "arm": 1, "sample": 1, "prior": 1, "belief": 1, "moment": 1}, {"": 1, "1": 1, "equivalently": 1, "last": 1, "decision": 2, "epoch": 1, "policy": 1, "take": 1, "myopically": 1, "best": 1, "action": 1, "base": 1, "current": 1, "estimate": 1, "indeed": 1, "optimal": 1, "whereas": 1, "ts": 1, "would": 1, "still": 1, "explore": 1, "unnecessarily": 1}, {"keep": 1, "recursive": 1, "structure": 1, "sequential": 1, "decisionmaking": 1, "process": 1, "ts": 2, "rs": 1, "fh": 1, "naturally": 1, "perform": 1, "less": 1, "exploration": 1, "remain": 1, "time": 1, "horizon": 1, "diminish": 1}, {"mitigate": 1, "common": 1, "practical": 1, "criticism": 1, "ts": 1, "explore": 1, "much": 1}, {"beyond": 1, "propose": 1, "algorithms": 1, "explicitly": 2, "quantify": 1, "benefit": 1, "exploration": 2, "trade": 1, "exploitation": 1, "cost": 1, "additional": 1, "computational": 1, "complexity": 1}, {"increase": 1, "complexity": 1, "achieve": 1, "policies": 1, "improve": 1, "performance": 2, "separately": 1, "provide": 1, "tighter": 1, "tractable": 1, "computational": 1, "upper": 1, "bound": 1, "expect": 1, "policy": 1, "particular": 1, "problem": 1, "instance": 1}, {"provide": 2, "natural": 1, "generalizations": 1, "ts": 2, "work": 1, "deeper": 1, "understand": 1, "improve": 1, "policies": 1, "require": 1, "tune": 1}, {"since": 1, "ts": 1, "show": 1, "asymptotically": 2, "regret": 1, "optimal": 1, "5": 1, "improvements": 2, "best": 1, "constant": 1, "factor": 1, "metric": 1}, {"hand": 1, "ts": 1, "extremely": 1, "": 2, "2": 1, "popular": 1, "practice": 1, "demonstrate": 1, "numerical": 1, "examples": 1, "improvements": 1, "significant": 1, "likely": 1, "practical": 1, "interest": 1}, {"moreover": 1, "develop": 1, "upper": 1, "bound": 1, "performance": 1, "useful": 1, "right": 1}, {"suppose": 1, "decision": 1, "maker": 1, "face": 1, "particular": 2, "problem": 1, "instance": 1, "consider": 1, "mab": 1, "policy": 1, "one": 1, "suggest": 1, "otherwise": 1}, {"simulate": 1, "policy": 2, "lower": 1, "bind": 1, "performance": 1, "optimal": 1, "find": 1}, {"introduce": 1, "series": 1, "upper": 1, "bound": 1, "also": 1, "evaluate": 1, "problem": 1, "instance": 1, "via": 1, "simulation": 1}, {"pair": 1, "lower": 1, "bind": 1, "provide": 1, "computational": 1, "simulationbased": 1, "confidence": 1, "interval": 1, "helpful": 1, "decision": 1, "maker": 1}, {"example": 1, "upper": 1, "bind": 2, "lower": 1, "close": 1, "suboptimality": 1, "gap": 1, "policy": 1, "consideration": 1, "guarantee": 1, "small": 1, "worth": 1, "invest": 1, "better": 1, "policies": 1}, {"2": 1, "": 2, "notation": 1, "preliminaries": 1, "problem": 1}, {"consider": 1, "classical": 1, "stochastic": 1, "mab": 1, "problem": 1, "k": 1, "independent": 1, "arm": 1, "finitehorizon": 1, "": 1}, {"decision": 1, "epoch": 1, "": 2, "1": 1}, {"": 1}, {"": 1}, {"": 5, "decision": 1, "maker": 1, "dm": 1, "pull": 1, "arm": 1, "1": 1}, {"": 1}, {"": 1}, {"": 2, "k": 1, "earn": 1, "stochastic": 1, "reward": 1, "associate": 1, "arm": 1}, {"formally": 1, "reward": 1, "nth": 1, "pull": 1, "arm": 2, "denote": 1, "run": 1, "independently": 1, "draw": 1, "unknown": 1, "distribution": 1, "ra": 1, "": 2, "parameter": 1, "associate": 1}, {"also": 1, "prior": 2, "distribution": 2, "pa": 2, "ya": 4, "": 10, "unknown": 1, "parameter": 1, "call": 1, "belief": 1, "hyperparameter": 1, "describe": 1, "run": 1, "ra": 1, "n": 2}, {"define": 1, "outcome": 1, "": 4, "aa": 1, "run": 1, "aann": 1, "incorporate": 1, "uncertainties": 1, "dm": 1, "encounter": 1}, {"give": 1, "prior": 1, "belief": 1, "vector": 1, "": 3, "y1": 1}, {"": 1}, {"": 1}, {"": 4, "yk": 1, "let": 1, "iy": 1, "prior": 1, "distribution": 1, "outcome": 1, "would": 1, "describe": 1, "pa": 1, "ra": 1}, {"additionally": 1, "define": 1, "true": 1, "mean": 1, "reward": 1, "bayesian": 1, "estimate": 1, "follow": 1, "": 10, "e": 2, "run": 1, "ya": 1, "ra1": 1}, {"": 1}, {"": 1}, {"": 3, "run": 1}, {"1": 1, "": 8, "paper": 1, "assume": 1, "reward": 2, "absolutely": 1, "integrable": 1, "prior": 1, "distribution": 2, "ie": 1, "e": 1, "run": 2, "explicitly": 1, "erra": 1, "pa": 2, "ya": 2, "r": 1, "ra": 1, "denote": 1, "unconditional": 1, "doubly": 1, "stochastic": 1, "random": 1, "variable": 1}, {"policy": 1}, {"give": 1, "action": 1, "sequence": 1, "time": 1, "a1t": 1, "": 3, "a1": 1}, {"": 1}, {"": 1}, {"": 15, "define": 1, "pt": 1, "number": 1, "pull": 1, "nt": 2, "a1t": 3, "s1": 1, "1as": 1, "arm": 1, "correspond": 1, "reward": 1, "realization": 1, "r": 2, "natural": 1, "filtration": 1, "ft": 1, "1t": 1}, {"": 5, "rs": 1, "a1s": 1, "st": 1, "encode": 1, "observations": 1, "reveal": 1, "time": 1, "inclusive": 1}, {"let": 1, "a1t": 1, "action": 1, "sequence": 1, "take": 1, "policy": 1, "": 1}, {"policy": 1, "": 1, "call": 1, "nonanticipating": 2, "every": 1, "action": 1, "ft1": 1, "measurable": 1, "define": 1, "f": 1, "set": 1, "policies": 1, "include": 1, "randomize": 1, "ones": 1}, {"bayesian": 1, "performance": 1, "policy": 1, "": 9, "define": 1, "expect": 1, "total": 1, "reward": 1, "randomness": 1, "associate": 1, "outcome": 1, "ie": 1, "x": 1, "v": 1, "eiy": 1, "rt": 1, "a1t": 1}, {"2": 1, "t1": 1, "": 1, "mdp": 1, "formulation": 1}, {"assume": 1, "equip": 1, "bayesian": 1, "update": 3, "function": 1, "u": 1, "": 4, "r": 3, "7": 1, "observe": 1, "ra1": 1, "arm": 1, "belief": 1, "vector": 1, "uy": 1, "accord": 1, "bay": 1, "rule": 1, "ath": 1, "component": 1, "step": 1}, {"bayesian": 1, "framework": 1, "mab": 1, "problem": 1, "recursive": 1, "structure": 1}, {"give": 1, "time": 2, "horizon": 1, "prior": 1, "belief": 1, "suppose": 1, "dm": 1, "earn": 1, "r": 1, "pull": 1, "arm": 1, "": 1, "1": 1}, {"remain": 1, "problem": 2, "dm": 1, "equivalent": 1, "time": 1, "horizon": 1, "": 1, "1": 1, "prior": 1, "belief": 1, "uy": 1, "r": 1}, {"follow": 1, "markovian": 1, "structure": 1, "obtain": 1, "bellman": 1, "equations": 1, "mab": 1, "problem": 1, "q": 2, "": 13, "erra": 1, "pa": 1, "ya": 1, "r": 2, "v": 3, "1": 1, "uy": 1, "max": 1, "3": 1, "aa": 1, "0": 2}, {"bellman": 1, "equation": 1, "intractable": 1, "analyze": 1, "offer": 1, "characterization": 1, "bayesian": 1, "optimal": 1, "policy": 1, "pt": 2, "best": 1, "achievable": 1, "performance": 1, "v": 4, "": 6, "ie": 1, "supf": 1}, {"3": 2, "": 6, "information": 2, "relaxation": 2, "sample": 2, "propose": 1, "general": 1, "framework": 1, "refer": 1, "irs": 1, "take": 1, "input": 1, "penalty": 1, "function": 1, "zt": 1, "produce": 1, "output": 1, "policy": 1, "z": 2, "associate": 1, "performance": 1, "bind": 1, "w": 1}, {"information": 1, "relaxation": 1, "penalties": 1, "inner": 1, "problem": 1}, {"relax": 1, "nonanticipativity": 1, "constraint": 1, "impose": 1, "policy": 1, "space": 1, "f": 1, "ie": 2, "ft1": 1, "measurable": 2, "dm": 1, "allow": 1, "first": 1, "observe": 1, "future": 1, "outcomes": 1, "advance": 1, "pick": 1, "action": 1}, {"compensate": 1, "relaxation": 1, "impose": 1, "penalty": 1, "dm": 1, "violate": 1, "nonanticipativity": 1, "constraint": 1}, {"introduce": 1, "penalty": 2, "function": 1, "zt": 1, "a1t": 2, "": 3, "denote": 1, "dm": 1, "incur": 1, "time": 1, "take": 1, "action": 1, "sequence": 1, "give": 1, "particular": 1, "instance": 1, "specify": 1}, {"clairvoyant": 1, "dm": 1, "find": 1, "best": 1, "action": 1, "sequence": 1, "optimal": 1, "particular": 1, "outcome": 1, "": 9, "presence": 1, "penalties": 1, "zt": 2, "solve": 1, "follow": 1, "deterministic": 1, "optimization": 1, "problem": 2, "refer": 1, "inner": 1, "maximizea1t": 1, "x": 1, "rt": 1, "a1t": 2}, {"": 3, "t1": 1, "definition": 1, "1": 1, "dual": 1, "feasibility": 1}, {"penalty": 1, "function": 1, "zt": 2, "dual": 1, "feasible": 1, "exante": 1, "zeromean": 1, "ie": 1, "e": 1, "a1t": 2, "": 12, "ft1": 1, "a1t1": 1, "0": 1}, {"4": 1, "": 4, "clarify": 1, "notion": 1, "conditional": 1, "expectation": 2, "remark": 1, "map": 1, "a1t": 3, "7": 1, "zt": 1, "stochastic": 1, "function": 1, "action": 1, "sequence": 1, "since": 1, "outcome": 1, "random1": 1, "dual": 1, "feasibility": 1, "condition": 1, "require": 1, "dm": 1, "make": 1, "decisions": 1, "natural": 1, "filtration": 1, "receive": 1, "zero": 1, "penalties": 1}, {"irs": 1, "performance": 1, "bind": 1}, {"let": 1, "w": 1, "z": 2, "expect": 2, "maximal": 1, "value": 1, "inner": 1, "problem": 1, "": 11, "outcome": 1, "randomly": 1, "draw": 1, "prior": 1, "distribution": 1, "iy": 1, "ie": 1, "total": 1, "payoff": 1, "clairvoyant": 1, "dm": 1, "achieve": 1, "presence": 1, "penalties": 1, "x": 1, "rt": 1, "a1t": 2, "zt": 1}, {"5": 1, "w": 1, "": 8, "eiy": 1, "max": 1, "a1t": 1, "t1": 1, "obtain": 1, "value": 1, "numerically": 1, "via": 1, "simulation": 1, "draw": 1, "outcomes": 1, "1": 1, "2": 1}, {"": 1}, {"": 1}, {"": 2, "independently": 1, "iy": 1, "solve": 1, "inner": 1, "problem": 1, "outcome": 1, "separately": 1, "take": 1, "average": 1, "maximal": 1, "value": 1, "across": 1, "sample": 1}, {"follow": 1, "theorem": 1, "show": 1, "w": 1, "z": 1, "indeed": 1, "valid": 1, "performance": 1, "bind": 1, "stochastic": 1, "mab": 1, "problem": 1}, {"theorem": 1, "1": 1, "weak": 1, "duality": 2, "strong": 1}, {"penalty": 1, "function": 1, "zt": 1, "dual": 1, "feasible": 1, "w": 2, "z": 2, "upper": 1, "bind": 1, "optimal": 1, "value": 1, "v": 2, "": 5, "weak": 1, "duality": 1}, {"6": 1, "": 6, "exist": 1, "dual": 1, "feasible": 1, "penalty": 3, "function": 2, "refer": 1, "ideal": 3, "ztideal": 2, "strong": 1, "duality": 1, "w": 1, "v": 1}, {"7": 1, "": 27, "follow": 1, "functional": 1, "form": 1, "ztideal": 1, "a1t": 5, "rt": 2, "e": 2, "ft1": 2, "a1t1": 2, "8": 1, "v": 2, "yt": 2}, {"good": 1, "penalty": 1, "function": 1, "precisely": 1, "penalize": 1, "additional": 1, "profit": 1, "extract": 1, "use": 1, "future": 1, "information": 1, "": 1}, {"extreme": 1, "ideal": 1, "penalty": 1, "ztideal": 1, "": 1, "intractable": 1, "however": 1, "remove": 1, "incentive": 1, "deviate": 1, "pt": 1, "result": 1, "strong": 1, "duality": 1}, {"8": 1, "yt": 1, "a1t": 2, "": 3, "represent": 1, "posterior": 1, "belief": 1, "dm": 1, "would": 1, "time": 1, "observe": 1, "reward": 1, "realizations": 1, "associate": 1, "give": 1}, {"1": 1, "": 5, "usual": 1, "probability": 1, "theory": 1, "z": 2, "exy": 1, "represent": 1, "expect": 1, "value": 1, "random": 2, "variable": 2, "x": 1, "give": 1, "information": 1, "dependency": 1}, {"4": 1, "": 1, "irs": 1, "policy": 1}, {"give": 1, "penalty": 1, "function": 1, "zt": 1, "": 3, "characterize": 1, "randomize": 1, "nonanticipating": 1, "irs": 1, "policy": 1, "z": 1, "f": 1, "follow": 1}, {"policy": 1, "": 4, "z": 1, "specify": 1, "arm": 1, "pull": 1, "remain": 1, "time": 1, "current": 1, "belief": 1, "give": 1, "first": 2, "sample": 1, "outcome": 1, "iy": 1, "randomly": 1, "ii": 1, "solve": 1, "inner": 1, "problem": 1, "find": 1, "best": 1, "action": 2, "sequence": 1, "a1t": 2, "respect": 1, "presence": 1, "penalties": 1, "zt": 1, "iii": 1, "take": 1, "a1": 1, "clairvoyant": 1, "optimal": 1, "solution": 1, "suggest": 1}, {"analogous": 1, "thompson": 1, "sample": 1, "repeat": 1, "step": 1, "iiii": 1, "every": 1, "decision": 1, "epoch": 1, "update": 1, "remain": 1, "time": 1, "belief": 1, "upon": 1, "reward": 1, "realization": 1}, {"1": 4, "2": 3, "": 23, "3": 2, "4": 1, "algorithm": 1, "information": 1, "relaxation": 1, "sample": 2, "irs": 1, "policy": 1, "function": 1, "irst": 1, "z": 2, "iy": 1, "equivalently": 1, "pa": 1, "ya": 1, "run": 1, "ra": 1, "n": 1, "find": 1, "best": 1, "action": 1, "sequence": 1, "npwith": 1, "respect": 1, "penalties": 1, "zo": 1, "a1t": 3, "argmaxa1t": 1, "t1": 1, "rt": 1, "zt": 1, "return": 1, "a1": 1, "procedure": 1, "irsoutert": 1, "y0": 1}, {"": 1}, {"": 1}, {"": 9, "play": 1, "irst": 1, "1": 2, "yt1": 1, "z": 1, "earn": 1, "observe": 1, "reward": 1, "rt": 2, "update": 1, "belief": 1, "yt": 1, "uyt1": 1, "end": 1, "remark": 1}, {"ideal": 2, "penalty": 1, "yield": 1, "bayesian": 1, "optimal": 1, "policy": 1, "ie": 1, "v": 2, "": 4}, {"choice": 1, "penalty": 1, "function": 1}, {"irs": 1, "policies": 1, "include": 1, "thompson": 1, "sample": 1, "bayesian": 1, "optimal": 1, "policy": 1, "two": 1, "extremal": 1, "case": 1}, {"propose": 1, "set": 1, "penalty": 1, "function": 1, "span": 1, "two": 1}, {"defer": 1, "detail": 1, "explanations": 1, "31": 1, "": 11, "34": 1, "briefly": 1, "list": 1, "penalty": 1, "function": 1, "ztts": 1, "a1t": 3, "rt": 2, "e": 1, "1": 1}, {"": 1}, {"": 1}, {"": 18, "k": 1, "zti": 3, "rs": 3, "fh": 1, "a1t": 5, "vz": 1, "ero": 1, "vem": 1, "ax": 1, "9": 1, "rt": 2, "e": 1, "1t": 1, "1": 1}, {"": 1}, {"": 1}, {"": 48, "kt": 1, "1": 2, "10": 1, "rt": 5, "a1t": 7, "e": 5, "ft1": 4, "a1t1": 4, "11": 1, "12": 1, "ts": 2, "w": 2, "yt": 2, "help": 1, "understand": 1, "provide": 1, "identity": 1, "example": 1, "rat": 1}, {"": 1}, {"": 1}, {"": 6, "rat": 1, "nt1": 2, "a1t1": 2, "represent": 1, "mean": 1, "reward": 1, "dm": 1, "expect": 1, "get": 1, "arm": 1, "right": 1, "make": 1, "decision": 1, "time": 1, "remark": 1, "2": 1}, {"penalty": 1, "function": 1, "812": 1, "dual": 1, "feasible": 1}, {"sequentially": 1, "increase": 1, "complexity": 1, "z": 2, "ts": 1, "ideal": 1, "": 1, "penalty": 1, "function": 1, "accurately": 1, "penalize": 1, "benefit": 1, "know": 1, "future": 2, "outcomes": 1, "explicitly": 1, "prevent": 1, "dm": 1, "exploit": 1, "information": 1}, {"summarize": 1, "table": 1, "1": 1, "make": 1, "inner": 1, "problem": 2, "closer": 1, "original": 1, "stochastic": 1, "optimization": 1, "result": 1, "better": 1, "perform": 1, "policy": 1, "tighter": 1, "performance": 1, "bind": 1}, {"result": 1, "achieve": 1, "family": 1, "algorithms": 1, "intuitive": 1, "tractable": 1, "exhibit": 1, "tradeoff": 1, "quality": 1, "computational": 1, "efficiency": 1}, {"31": 1, "": 24, "thompson": 1, "sample": 1, "penalty": 1, "function": 1, "ztts": 1, "a1t": 4, "rt": 2, "inner": 1, "problem": 1, "reduce": 1, "x": 2, "ts": 1, "max": 3, "zt": 1}, {"13": 1, "a1t": 2, "": 7, "t1": 2, "aa": 1, "result": 1, "performance": 1, "bind": 1, "w": 1, "ts": 1, "e": 1, "maxaa": 1, "conventional": 1, "benchmark": 1, "bayesian": 1, "set": 1, "15": 1, "19": 1}, {"correspond": 1, "irs": 1, "policy": 1, "": 5, "ts": 1, "restore": 1, "thompson": 1, "sample": 3, "outcome": 1, "use": 1, "instead": 1, "play": 1, "arm": 1, "a1": 1, "argmaxa": 1, "pa": 1, "ya": 1}, {"recall": 1, "samplingbased": 1, "decision": 1, "make": 1, "repeat": 1, "epoch": 1, "update": 1, "belief": 1, "sequentially": 1, "describe": 1, "irso": 1, "uter": 1, "algorithm": 1, "1": 1}, {"5": 1, "": 13, "penalty": 1, "function": 1, "policy": 1, "ztts": 1, "rs": 9, "fh": 3, "zt": 1, "zti": 2, "vz": 3, "ero": 3, "vem": 3, "ax": 3, "ztideal": 1, "ts": 2, "pt": 1, "performance": 1, "bind": 1, "w": 4, "v": 1, "inner": 1, "problem": 1, "run": 1, "time": 1, "find": 1, "best": 1, "arm": 1, "give": 1, "parameters": 1}, {"ok": 1, "find": 1, "best": 1, "arm": 1, "give": 1, "finite": 1, "observations": 1}, {"ok": 1, "okt": 1, "": 1, "find": 1, "optimal": 1, "allocation": 1, "pull": 1}, {"okt": 1, "2": 1, "": 1, "find": 1, "optimal": 1, "action": 1, "sequence": 1}, {"okt": 1, "k": 1, "": 1, "solve": 1, "bellman": 1, "equations": 1}, {"": 2, "table": 1, "1": 1, "list": 1, "algorithms": 1, "associate": 1, "penalty": 1, "function": 1, "812": 1}, {"run": 1, "time": 3, "represent": 1, "complexity": 1, "solve": 1, "one": 2, "instance": 1, "inner": 1, "problem": 1, "require": 1, "obtain": 1, "sample": 1, "performance": 1, "bind": 1, "w": 1, "z": 2, "make": 1, "single": 1, "decision": 1, "policy": 1, "": 2}, {"": 2, "rs": 1, "fh": 1, "ok": 1, "achievable": 1, "prior": 2, "distribution": 2, "pa": 1, "conjugate": 1, "reward": 1, "ra": 1}, {"32": 1, "": 6, "rs": 1, "fh": 1, "recall": 1, "1": 2, "bayesian": 1, "estimate": 1, "mean": 1, "reward": 2, "arm": 1, "infer": 1, "first": 1, "realizations": 1, "ra1": 1}, {"": 1}, {"": 1}, {"": 2, "rat": 1, "1": 1}, {"give": 1, "10": 1, "optimal": 1, "solution": 1, "inner": 1, "problem": 1, "": 15, "pull": 1, "arm": 1, "highest": 1, "1": 3, "begin": 1, "end": 1, "x": 2, "max": 3, "rt": 1, "a1t": 2, "zti": 1, "rs": 1, "fh": 1}, {"a1t": 2, "": 7, "t1": 2, "aa": 1, "14": 1, "rs": 1, "fh": 1, "almost": 1, "identical": 1, "ts": 1, "except": 1, "replace": 1, "1": 1}, {"note": 1, "1": 1, "": 3, "less": 1, "informative": 1, "dm": 1, "since": 1, "never": 1, "able": 1, "learn": 1, "perfectly": 1, "within": 1, "finite": 1, "horizon": 1}, {"term": 1, "estimation": 1, "know": 1, "parameters": 1, "equivalent": 1, "infinite": 1, "number": 1, "observations": 1}, {"inner": 1, "problem": 1, "ts": 1, "ask": 2, "dm": 1, "identify": 2, "best": 2, "arm": 2, "base": 2, "infinite": 1, "number": 2, "sample": 2, "whereas": 1, "rs": 1, "fh": 1, "finite": 1, "take": 1, "account": 1, "length": 1, "time": 1, "horizon": 1, "explicitly": 1}, {"focus": 1, "policies": 1, "": 9, "rs": 1, "fh": 1, "ts": 1, "randomly": 1, "generate": 1, "1": 2, "use": 1, "observe": 1, "distribution": 1, "concentrate": 1, "mean": 1, "ea": 1, "a0": 1}, {"since": 1, "variance": 1, "1": 2, "": 5, "govern": 1, "degree": 1, "random": 1, "exploration": 1, "deviate": 1, "myopic": 1, "decision": 1, "pull": 1, "arm": 1, "largest": 1, "rs": 1, "fh": 1, "naturally": 1, "explore": 1, "less": 1, "ts": 1, "particular": 1, "approach": 1, "end": 1, "horizon": 1}, {"performance": 2, "bound": 1, "reason": 1, "w": 2, "rs": 2, "fh": 2, "": 5, "et": 2, "maxa": 2, "1": 1, "ts": 1, "mean": 1, "yield": 1, "bind": 1, "tighter": 1, "conventional": 1, "regret": 1, "benchmark": 1}, {"sample": 1, "1": 1, "": 1}, {"order": 1, "obtain": 1, "1": 1, "": 3, "synthesize": 1, "outcome": 1, "one": 1, "may": 1, "apply": 1, "bay": 1, "rule": 1, "sequentially": 1, "reward": 1, "realization": 1, "take": 1, "okt": 1, "computations": 1, "total": 1}, {"do": 1, "ok": 1, "belief": 1, "update": 1, "batch": 1, "use": 1, "sufficient": 1, "statistics": 1}, {"betabernoulli": 2, "gaussian": 2, "mabs": 1, "example": 1, "1": 8, "": 9, "represent": 2, "convex": 1, "pt": 2, "combination": 1, "current": 1, "estimate": 1, "sample": 1, "mean": 1, "n1": 2, "run": 2, "distribute": 1, "binomialt": 1, "case": 2, "n": 1, "a2": 2, "noise": 1, "variance": 1}, {"sample": 2, "parameter": 1, "": 2, "pt": 1, "1": 2, "n1": 1, "run": 1, "directly": 1, "know": 1, "distribution": 1, "use": 1, "compute": 1, "without": 1, "sequentially": 1, "update": 1, "belief": 1}, {"case": 1, "single": 1, "decision": 1, "": 1, "rs": 1, "fh": 1, "make": 1, "within": 1, "ok": 1, "operations": 1, "similar": 1, "complexity": 1, "ts": 1}, {"33": 1, "": 10, "rs": 2, "vz": 2, "ero": 2, "penalty": 1, "zti": 1, "dm": 2, "time": 3, "earn": 2, "e": 1, "rt": 1, "a1t": 1, "ft1": 1, "a1t1": 1, "expect": 1, "mean": 1, "reward": 2, "infer": 1, "observations": 1, "prior": 1, "define": 1, "run": 1, "nth": 2, "pull": 4, "arm": 4, "n": 1, "posterior": 1, "belief": 1, "associate": 1, "determine": 1, "number": 1, "past": 2, "an1": 1, "irrespective": 1, "detail": 1, "sequence": 1, "action": 1}, {"6": 1, "": 5, "follow": 1, "observation": 1, "solve": 1, "inner": 1, "problem": 1, "equivalent": 1, "find": 1, "optimal": 1, "allocation": 1, "n1": 1, "n2": 1}, {"": 1}, {"": 1}, {"": 23, "nk": 1, "among": 1, "remain": 1, "opportunities": 1, "omit": 1, "brevity": 1, "reduce": 1, "k": 4, "nt": 3, "1t": 1, "x": 4, "max": 3, "nt1": 1, "a1t1": 1, "an1": 1, "sana": 1, "n1k": 1, "a1t": 2, "t1": 1, "a1": 2, "n1": 2, "15": 1, "pn": 1, "san": 1, "cumulative": 1, "payoff": 1, "first": 1, "n": 1, "pull": 1, "arm": 1, "m1": 1, "am1p": 1}, {"": 1}, {"": 1}, {"": 6, "nk": 1, "z": 1, "a1": 1, "na": 1, "set": 1, "feasible": 1, "allocations": 1}, {"san": 1, "compute": 1, "inner": 1, "problem": 1, "solve": 1, "within": 1, "okt": 1, "2": 1, "": 1, "operations": 1, "sequentially": 1, "apply": 1, "sup": 1, "convolution": 1, "k": 1, "time": 1}, {"detail": 1, "implementation": 1, "provide": 1, "appendix": 1, "b1": 1}, {"give": 1, "optimal": 1, "allocation": 1, "n": 1, "": 2, "policy": 1, "rs": 1, "vz": 1, "ero": 1, "need": 1, "select": 1, "arm": 1, "pull": 1, "next": 1}, {"principle": 1, "arm": 2, "include": 1, "solution": 1, "inner": 1, "problem": 1, "na": 2, "": 2, "0": 1, "would": 1, "fine": 1, "suggest": 1, "selection": 1, "rule": 1, "need": 1, "pull": 1, "choose": 1, "ie": 1, "argmaxa": 1}, {"34": 1, "": 2, "rs": 1, "vem": 1, "ax": 1, "perfect": 1, "information": 1, "relaxation": 1, "dm": 1, "perfectly": 1, "know": 1, "earn": 1, "future": 1, "time": 1, "also": 1, "ii": 1, "belief": 1, "evolve": 1, "result": 1, "action": 1, "sequence": 1}, {"previous": 1, "algorithms": 1, "focus": 1, "former": 1, "component": 1, "make": 1, "dm": 1, "adjust": 1, "future": 1, "reward": 1, "condition": 1, "eg": 1, "ert": 3, "": 3, "1kt": 1, "1": 1, "ft1": 1}, {"rs": 1, "vem": 1, "ax": 1, "also": 1, "focus": 1, "second": 1, "component": 1, "well": 1, "charge": 1, "additional": 1, "cost": 1, "use": 1, "information": 1, "future": 1, "belief": 1, "transition": 1}, {"specifically": 1, "penalty": 1, "function": 1, "zti": 1, "rs": 1, "vem": 1, "ax": 1, "obtain": 1, "ztideal": 1, "8": 1, "replace": 1, "v": 1, "": 1, "w": 1, "ts": 1, "tractable": 1, "alternative": 1}, {"use": 1, "w": 3, "ts": 3, "lead": 1, "simple": 1, "expression": 1, "conditional": 1, "expectation": 1, "since": 1, "ft1": 3, "distribute": 1, "pyt1": 1, "": 18, "h": 2, "e": 2, "yt": 1, "max": 2, "16": 1, "epyt1": 1, "yt1": 1}, {"17": 1, "": 6, "observe": 1, "give": 1, "future": 2, "belief": 1, "yt": 1, "a1t": 1, "depend": 1, "many": 1, "time": 1, "arm": 1, "pull": 2, "irrespective": 1, "sequence": 1, "hence": 1, "number": 1, "possible": 1, "beliefs": 1, "ot": 1, "k": 1, "ok": 1}, {"give": 1, "observations": 1, "solve": 1, "inner": 1, "problem": 1, "within": 1, "okt": 1, "k": 1, "": 1, "computations": 1, "dynamic": 1, "program": 1, "ie": 1, "find": 1, "best": 1, "action": 1, "future": 1, "belief": 1, "iterate": 1, "beliefs": 1, "appropriate": 1, "order": 1}, {"see": 1, "b2": 1, "detail": 1}, {"4": 1, "": 2, "analysis": 1, "remark": 1, "3": 1, "single": 1, "period": 1, "optimality": 1}, {"": 8, "1": 1, "rs": 3, "fh": 1, "vz": 1, "ero": 1, "max": 1, "take": 1, "optimal": 1, "action": 1, "pull": 1, "myopically": 1, "best": 1, "arm": 1, "argmaxa": 1, "ea": 1}, {"proposition": 1, "1": 1, "asymptotic": 1, "behavior": 1}, {"assume": 1, "": 2, "6": 2, "j": 3, "almost": 1, "surely": 1, "two": 1, "distinct": 1, "arm": 1}, {"": 9, "distribution": 1, "rs": 4, "fhs": 1, "vz": 2, "eros": 1, "action2": 1, "converge": 1, "thompson": 1, "sample": 1, "lim": 2, "p": 3, "fht": 1, "erot": 1, "tsy": 1}, {"18": 1, "": 9, "tsy": 1, "rs": 4, "fht": 1, "vz": 2, "erot": 1, "denote": 1, "action": 1, "take": 1, "policies": 1, "ts": 1, "fh": 1, "ero": 1, "repsectively": 1, "remain": 1, "time": 1, "prior": 1, "belief": 1}, {"random": 1, "variables": 1, "since": 1, "policies": 1, "use": 1, "randomly": 1, "sample": 1, "outcome": 1, "": 1}, {"remark": 1, "3": 1, "proposition": 1, "1": 1, "state": 1, "rs": 2, "fh": 1, "vz": 1, "ero": 1, "behave": 1, "like": 1, "ts": 2, "initial": 1, "decision": 2, "epochs": 1, "gradually": 1, "shift": 1, "toward": 1, "myopic": 1, "scheme": 1, "end": 1, "optimal": 1, "contrast": 1, "continue": 1, "explore": 1, "throughout": 1}, {"transition": 1, "exploration": 1, "exploitation": 1, "irs": 1, "policies": 1, "occur": 1, "smoothly": 1, "without": 1, "rely": 1, "auxiliary": 1, "control": 1, "parameter": 1}, {"maintain": 1, "recursive": 1, "structure": 1, "irs": 1, "policies": 1, "take": 1, "account": 1, "horizon": 1, "": 1, "naturally": 1, "balance": 1, "exploitation": 1, "exploration": 1}, {"2": 1, "": 3, "rs": 1, "vz": 1, "ero": 1, "assume": 1, "particular": 1, "selection": 1, "rule": 1, "argmaxa": 1, "na": 1}, {"7": 1, "": 1, "theorem": 1, "2": 1, "monotonicity": 1, "performance": 1, "bound": 1}, {"rs": 4, "fh": 2, "vz": 2, "ero": 2, "monotonically": 1, "improve": 1, "performance": 1, "bind": 1, "w": 3, "ts": 1, "": 2}, {"19": 1, "": 4, "note": 1, "w": 1, "ts": 1, "epy": 1, "maxa": 1, "conventional": 1, "benchmark": 1}, {"addition": 1, "w": 3, "rs": 1, "vem": 1, "ax": 1, "": 1, "ideal": 2, "since": 1, "lowest": 1, "attainable": 1, "upper": 1, "bind": 1, "theorem": 1, "1": 1}, {"empirically": 1, "also": 1, "observe": 1, "w": 2, "rs": 2, "vz": 1, "ero": 1, "": 2, "vem": 1, "ax": 1}, {"theorem": 1, "3": 1, "suboptimality": 1, "gap": 1}, {"betabernoulli": 1, "mab": 1, "p": 4, "": 25, "log": 3, "2": 4, "kt": 3, "1p": 1, "w": 2, "rs": 4, "fh": 2, "v": 2, "3k": 1, "k": 2, "3": 1, "1": 1, "vz": 2, "ero": 2, "2k": 1}, {"3": 1, "w": 1, "ts": 2, "": 9, "v": 2, "3k": 1, "2": 1, "20": 1, "21": 1, "22": 1, "theoretical": 1, "guarantee": 1, "monotonicity": 1, "actual": 1, "performance": 1, "z": 1, "among": 1, "irs": 1, "policies": 1}, {"instead": 1, "theorem": 1, "3": 1, "indirectly": 1, "show": 1, "improvements": 1, "suboptmality": 1, "gap": 1, "": 4, "w": 1, "z": 2, "yv": 1, "although": 1, "bound": 1, "asymptotic": 1, "order": 1, "kt": 1, "log": 1, "irs": 1, "policies": 1, "improve": 1, "lead": 1, "coefficient": 1, "additional": 1, "term": 1}, {"theorem": 1, "2": 1, "3": 1, "highlight": 1, "better": 2, "choice": 1, "penalty": 1, "function": 1, "zt": 1, "lead": 1, "tighter": 1, "performance": 1, "bind": 1, "w": 1, "z": 2, "perform": 1, "policy": 1, "": 2}, {"recall": 1, "penalties": 1, "design": 1, "penalize": 1, "gain": 1, "additional": 1, "future": 1, "information": 1}, {"irs": 1, "algorithms": 1, "basically": 1, "optimistic": 1, "sense": 1, "dm": 2, "make": 1, "decision": 1, "believe": 1, "inform": 1, "outcome": 1, "": 2, "realize": 1, "better": 1, "penalty": 1, "function": 1, "prevent": 1, "pick": 1, "action": 1, "overly": 1, "optimize": 1, "particular": 1, "future": 1, "realization": 1}, {"5": 2, "": 3, "numerical": 1, "experiment": 1, "visualize": 1, "effectiveness": 1, "irs": 1, "policies": 1, "performance": 1, "bound": 1, "case": 1, "gaussian": 1, "mab": 1, "five": 1, "arm": 1, "k": 1, "different": 1, "noise": 1, "variances": 1}, {"specifically": 1, "arm": 1, "": 11, "unknown": 1, "mean": 1, "reward": 2, "n": 2, "0": 1, "12": 1, "yield": 1, "stochastic": 1, "run": 1, "a2": 1, "1": 1, "01": 1, "2": 1, "04": 1, "3": 1, "10": 1, "4": 1, "40": 1, "5": 1, "100": 1}, {"experiment": 1, "include": 1, "stateoftheart": 1, "algorithms": 1, "particularly": 1, "suitable": 1, "bayesian": 2, "framework": 1, "upper": 1, "confidence": 1, "bind": 1, "14": 1, "bay": 1, "ucb": 1, "quantile": 1, "1": 2, "": 5, "1t": 2, "information": 1, "direct": 1, "sample": 1, "20": 1, "ids": 1, "optimistic": 1, "gittins": 1, "index": 1, "9": 1, "ogi": 1, "onestep": 1, "look": 1, "ahead": 1, "approximation": 1, "discount": 1, "factor": 1}, {"simulation": 1, "randomly": 1, "generate": 1, "set": 1, "outcomes": 1, "": 3, "1": 1}, {"": 1}, {"": 1}, {"": 5, "measure": 1, "performance": 2, "policy": 1, "v": 1, "bound": 1, "w": 1, "z": 1, "via": 1, "sample": 2, "average": 1, "approximation": 1, "across": 1, "outcomes": 1, "20": 1, "000": 1}, {"figure": 1, "1": 1, "plot": 1, "regret": 2, "policies": 1, "solid": 1, "line": 2, "w": 2, "ts": 2, "": 4, "v": 1, "bound": 1, "dash": 1, "yw": 1, "z": 1, "yhthat": 1, "measure": 1, "different": 1, "value": 1, "5": 1, "10": 1}, {"": 1}, {"": 1}, {"": 16, "500": 1, "pt": 1, "ts": 4, "regret": 3, "measure": 2, "w": 6, "v": 2, "e": 1, "t1": 1, "maxa": 1, "equivalent": 1, "conventional": 1, "bayesian": 1, "19": 1, "z": 2, "provide": 1, "lower": 1, "bind": 1, "achievable": 1, "since": 1, "policy": 1, "f": 1, "due": 1, "weak": 1, "duality": 1}, {"despite": 1, "fact": 1, "cannot": 1, "compute": 1, "bayesian": 1, "optimal": 1, "policy": 1, "directly": 1, "infer": 1, "regret": 1, "curve": 1, "locate": 1, "shade": 1, "region": 1, "plot": 1}, {"note": 1, "lower": 1, "regret": 1, "curve": 2, "better": 2, "higher": 1, "bind": 1}, {"incorporate": 1, "complicate": 1, "irs": 1, "algorithm": 1, "ts": 1, "rs": 1, "vz": 1, "ero": 1, "observe": 1, "clear": 1, "improvement": 1, "performances": 1, "bound": 1, "predict": 1, "theorem": 1, "2": 1, "3": 1}, {"particular": 1, "example": 1, "crucial": 1, "incorporate": 1, "much": 1, "learn": 1, "arm": 1, "remain": 1, "time": 2, "periods": 1, "heavily": 1, "depend": 1, "noise": 1, "level": 1, "horizon": 1, "": 1}, {"accordingly": 1, "irs": 1, "policies": 1, "outperform": 1, "others": 1, "since": 1, "explicitly": 1, "incorporate": 1, "exploitationexploration": 1, "tradeoff": 1}, {"8": 1, "": 22, "gaussian": 2, "mab": 2, "k": 1, "5": 1, "heteroscedastic": 1, "noise": 2, "bayesian": 1, "regret": 2, "w": 1, "tst": 1, "v": 1, "200": 2, "150": 1, "ts": 3, "34": 1, "ms": 4, "bayesucb": 2, "88": 1, "irsfh": 3, "128": 1, "ogi": 2, "829": 1, "ids": 2, "29": 1, "sec": 2, "irsvzero": 3, "74": 1, "100": 2, "50": 1, "0": 2, "300": 1, "time": 1, "horizon": 1, "400": 1, "500": 1, "600": 1, "figure": 1, "1": 1, "plot": 1, "different": 1, "variances": 1}, {"solid": 1, "line": 2, "represent": 2, "bayesian": 1, "regret": 2, "policies": 1, "w": 3, "ts": 2, "": 3, "v": 1, "dash": 1, "bound": 1, "irs": 1, "algorithms": 1, "produce": 1, "z": 1}, {"lowest": 1, "achievable": 1, "regret": 1, "w": 1, "ts": 1, "": 2, "v": 1, "within": 1, "shade": 1, "area": 1}, {"time": 2, "legend": 1, "represent": 1, "average": 1, "length": 1, "require": 1, "simulate": 1, "policy": 1, "single": 1, "problem": 1, "instance": 1, "": 1, "500": 1}, {"6": 1, "": 2, "discussion": 1, "develop": 1, "unify": 1, "framework": 1, "provide": 1, "principled": 1, "method": 1, "improve": 1, "ts": 1, "require": 1, "tune": 1, "additional": 1, "parameters": 1}, {"despite": 1, "fact": 1, "paper": 1, "focus": 1, "finitehorizon": 1, "mab": 1, "independent": 1, "arm": 1, "general": 1, "idea": 1, "information": 1, "relaxation": 1, "sample": 1, "restrict": 1, "set": 1, "briefly": 1, "illustrate": 1, "extend": 1, "framework": 1, "broader": 1, "class": 1, "problems": 1}, {"mab": 1, "unknown": 1, "time": 1, "horizon": 1}, {"framework": 1, "penalties": 1, "policies": 1, "upper": 1, "bound": 1, "naturally": 1, "incorporate": 1, "unknown": 1, "within": 1, "bayesian": 1, "set": 1, "ie": 1, "horizon": 1, "also": 1, "random": 1, "variable": 1, "whose": 1, "prior": 1, "distribution": 1, "know": 1}, {"simple": 1, "case": 1, "dms": 1, "action": 1, "pindependent": 1, "": 10, "reformulate": 1, "objective": 1, "function": 1, "inner": 1, "problem": 1, "t1": 1, "rt": 2, "a1t": 2, "zt": 2, "discount": 1, "factor": 1, "pt": 1, "survival": 1, "probability": 1, "reward": 1, "penalty": 1, "term": 1, "use": 1, "paper": 1}, {"alternatively": 1, "treat": 1, "random": 2, "variable": 1, "like": 1, "reward": 1, "realizations": 1, "": 3, "sample": 1, "prior": 1, "distribution": 1, "penalty": 1, "function": 1, "additionally": 1, "penalize": 1, "gain": 1, "know": 1, "one": 1, "imagine": 1, "outcome": 1, "include": 1, "realization": 1}, {"structural": 1, "result": 1, "weak": 1, "duality": 2, "strong": 1, "continue": 1, "hold": 1}, {"mab": 1, "complicate": 1, "settings": 1}, {"consider": 1, "follow": 1, "examples": 1, "finitehorizon": 1, "mab": 1, "2": 1, "correlate": 1, "arm": 3, "eg": 1, "run": 1, "": 6, "n": 1, "x": 1, "r": 2, "share": 1, "across": 1, "xa": 1, "feature": 1, "vector": 1, "rs": 1, "vz": 1, "ero": 1, "immediately": 1, "implement": 1, "adopt": 1, "dp": 1, "algorithm": 1, "discuss": 1, "b2": 1}, {"ii": 1, "mab": 1, "delay": 2, "reward": 1, "realization": 1, "rs": 1, "fh": 1, "immediately": 1, "implement": 1, "simulate": 1, "dms": 1, "learn": 1, "process": 1, "presence": 1}, {"iii": 1, "mab": 1, "budget": 3, "constraint": 1, "arm": 1, "consume": 1, "certain": 1, "amount": 1, "dm": 1, "want": 1, "maximize": 1, "total": 1, "reward": 1, "within": 1, "limit": 1}, {"see": 1, "8": 1, "irs": 1, "algorithms": 1, "implement": 1, "solve": 1, "budgetconstrained": 1, "optimization": 2, "problem": 2, "instead": 1, "horizonconstrained": 1}, {"extensions": 1, "obtain": 1, "online": 1, "decision": 1, "make": 1, "policies": 1, "also": 1, "performance": 1, "bound": 1, "paper": 1}, {"generally": 1, "speak": 1, "framework": 1, "provide": 1, "systemic": 1, "way": 1, "improve": 1, "ts": 1, "take": 1, "account": 1, "exploitationexploration": 1, "tradeoff": 1, "carefully": 1, "particularly": 1, "presence": 1, "constraint": 1, "incur": 1, "incomplete": 1, "learn": 1}, {"main": 1, "challenge": 1, "would": 1, "design": 1, "suitable": 1, "penalty": 1, "function": 1, "tractable": 1, "yet": 1, "capture": 1, "problemspecific": 1, "explorationexploitation": 1, "tradeoff": 1, "precisely": 1}, {"9": 1, "": 1, "reference": 1, "1": 1, "donald": 1, "berry": 1, "bert": 1, "fristedt": 1}, {"bandit": 1, "problems": 1, "sequential": 1, "allocation": 1, "experiment": 1}, {"chapman": 1, "hall": 1, "1985": 1}, {"2": 1, "rusell": 1, "n": 1, "bradt": 1, "johnson": 1, "samuel": 1, "karlin": 1}, {"sequential": 1, "design": 1, "maximize": 1, "sum": 1, "n": 1, "observations": 1}, {"annals": 1, "mathematical": 1, "statistics": 1, "27410601074": 1, "1956": 1}, {"3": 1, "david": 1, "b": 1}, {"brown": 1, "martin": 1, "b": 1, "haugh": 1}, {"information": 1, "relaxation": 1, "bound": 1, "infinite": 1, "horizon": 1, "markov": 1, "decision": 1, "process": 1}, {"operations": 1, "research": 1, "65513551379": 1, "2017": 1}, {"4": 1, "david": 1, "b": 1}, {"brown": 1, "jam": 1, "e": 1, "smith": 1, "peng": 1, "sun": 1}, {"information": 1, "relaxations": 1, "duality": 1, "stochastic": 1, "dynamic": 1, "program": 1}, {"operations": 1, "research": 1, "584785801": 1, "2010": 1}, {"5": 1, "sebastien": 1, "bubeck": 1, "cheyu": 1, "liu": 1}, {"priorfree": 1, "priordependent": 1, "regret": 1, "bound": 1, "thompson": 1, "sample": 1}, {"proceed": 1, "26th": 1, "international": 1, "conference": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "1638646": 1, "2013": 1}, {"6": 1, "h": 1, "davis": 1, "karatzas": 1}, {"deterministic": 1, "approach": 1, "optimal": 1, "stop": 1}, {"wiley": 1, "1994": 1}, {"7": 1, "vijay": 1, "v": 1, "desai": 1, "vivek": 1, "f": 1, "farias": 1, "ciamac": 1, "c": 1, "moallemi": 1}, {"pathwise": 1, "optimization": 1, "optimal": 1, "stop": 1, "problems": 1}, {"management": 1, "science": 1, "581222922308": 1, "2012": 1}, {"8": 1, "wenkui": 1, "ding": 1, "tao": 1, "qin": 1, "xudong": 1, "zhang": 1, "tieyan": 1, "liu": 1}, {"multiarmed": 1, "bandit": 1, "budget": 1, "constraint": 1, "variable": 1, "cost": 1}, {"proceed": 1, "27th": 1, "aaai": 1, "conference": 1, "artificial": 1, "intelligence": 1, "2013": 1}, {"9": 1, "vivek": 1, "f": 1, "farias": 1, "eli": 1, "gutin": 1}, {"optimistic": 1, "gittins": 1, "indices": 1}, {"proceed": 1, "30th": 1, "international": 1, "conference": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "31613169": 1, "2016": 1}, {"10": 1, "j": 1, "c": 1, "gittins": 1}, {"bandit": 1, "process": 1, "dynamic": 1, "allocation": 1, "indices": 1}, {"journal": 1, "royal": 1, "statistical": 1, "society": 1, "series": 1, "b": 1, "412148177": 1, "1979": 1}, {"11": 1, "martin": 1, "b": 1, "haugh": 1, "leonid": 1, "kogan": 1}, {"price": 1, "american": 1, "options": 1, "duality": 1, "approach": 1}, {"operations": 1, "research": 1, "522258270": 1, "2004": 1}, {"12": 1, "martin": 1, "b": 2, "haugh": 1, "andrew": 1, "e": 1, "lim": 1}, {"linearquadratic": 1, "control": 1, "information": 1, "relaxations": 1}, {"operations": 1, "research": 1, "letter": 1, "40521528": 1, "2012": 1}, {"13": 1, "martin": 1, "b": 1, "haugh": 1, "chun": 1, "wang": 1}, {"dynamic": 1, "portfolio": 1, "execution": 1, "information": 1, "relaxations": 1}, {"siam": 1, "journal": 1, "financial": 1, "math": 1, "5316359": 1, "2014": 1}, {"14": 1, "emilie": 1, "kaufmann": 1, "olivier": 1, "capp": 1, "aurlien": 1, "garivier": 1}, {"bayesian": 1, "upper": 1, "confidence": 1, "bound": 1, "bandit": 1, "problems": 1}, {"proceed": 1, "fifteenth": 1, "international": 1, "conference": 1, "artificial": 1, "intelligence": 1, "statistics": 1, "22592600": 1, "2012": 1}, {"15": 1, "tze": 1, "leueng": 1, "lai": 1, "herbert": 1, "robbins": 1}, {"asymptotically": 1, "efficient": 1, "adaptive": 1, "allocation": 1, "rule": 1}, {"advance": 1, "apply": 1, "mathematics": 1, "6422": 1, "1985": 1}, {"16": 1, "olivier": 1, "marchal": 1, "julyan": 1, "arbel": 1}, {"subgaussianity": 1, "beta": 1, "dirichlet": 1, "distributions": 1}, {"2017": 1}, {"17": 1, "r": 1, "rockafellar": 1, "roger": 1, "jb": 1}, {"wet": 1}, {"scenarios": 1, "policy": 1, "aggregation": 1, "optimization": 1, "uncertainty": 1}, {"mathematics": 1, "operations": 1, "research": 1, "161119147": 1, "1991": 1}, {"18": 1, "l": 1, "c": 1, "g": 1, "rogers": 1}, {"monte": 1, "carlo": 1, "valuation": 1, "american": 1, "options": 1}, {"mathematical": 1, "finance": 1, "123271": 1, "286": 1, "2002": 1}, {"19": 1, "daniel": 1, "russo": 1, "benjamin": 1, "van": 1, "roy": 1}, {"learn": 1, "optimize": 1, "via": 1, "posterior": 1, "sample": 1}, {"mathematics": 1, "operations": 1, "research": 1, "39412211243": 1, "2014": 1}, {"20": 1, "daniel": 1, "russo": 1, "benjamin": 1, "van": 1, "roy": 1}, {"learn": 1, "optimize": 1, "via": 1, "informationdirected": 1, "sample": 1}, {"operations": 1, "research": 1, "661230252": 1, "2017": 1}, {"21": 1, "w": 1, "thompson": 1}, {"likelihood": 1, "one": 1, "unknown": 1, "probability": 1, "exceed": 1, "another": 1, "view": 1, "evidence": 1, "two": 1, "sample": 1}, {"biometrika": 1, "2534285294": 1, "1933": 1}, {"10": 1}]
[{"use": 1, "parametric": 1, "model": 1, "reinforcement": 1, "learn": 1}, {"hado": 1, "van": 1, "hasselt": 1, "deepmind": 3, "london": 3, "uk": 3, "hadogooglecom": 1, "": 3, "matteo": 1, "hessel": 1, "mtthssgooglecom": 1, "john": 1, "aslanides": 1, "jaslanidesgooglecom": 1, "abstract": 1, "examine": 1, "question": 1, "parametric": 1, "model": 1, "useful": 1, "reinforcement": 1, "learn": 1}, {"particular": 1, "look": 1, "commonalities": 1, "differences": 1, "parametric": 1, "model": 1, "experience": 1, "replay": 1}, {"replaybased": 1, "learn": 1, "algorithms": 1, "share": 1, "important": 1, "traits": 1, "modelbased": 1, "approach": 1, "include": 1, "ability": 1, "plan": 1, "use": 1, "computation": 1, "without": 1, "additional": 1, "data": 1, "improve": 1, "predictions": 1, "behaviour": 1}, {"discuss": 1, "expect": 1, "benefit": 1, "either": 1, "approach": 1, "interpret": 1, "prior": 1, "work": 1, "context": 1}, {"hypothesise": 1, "suitable": 1, "condition": 1, "replaybased": 1, "algorithms": 2, "competitive": 1, "better": 1, "modelbased": 1, "model": 1, "use": 1, "generate": 1, "fictional": 1, "transition": 1, "observe": 1, "state": 1, "update": 1, "rule": 1, "otherwise": 1, "modelfree": 1}, {"validate": 1, "hypothesis": 1, "atari": 1, "2600": 1, "video": 1, "game": 1}, {"replaybased": 1, "algorithm": 1, "attain": 1, "stateoftheart": 1, "data": 1, "efficiency": 1, "improve": 1, "prior": 1, "result": 1, "parametric": 1, "model": 1}, {"additionally": 1, "discuss": 1, "different": 1, "ways": 1, "use": 1, "model": 1}, {"show": 1, "better": 1, "plan": 2, "backward": 1, "forward": 1, "use": 1, "model": 1, "perform": 1, "credit": 1, "assignment": 1, "eg": 1, "directly": 1, "learn": 1, "value": 1, "policy": 1, "even": 1, "though": 1, "latter": 1, "seem": 1, "common": 1}, {"finally": 1, "argue": 1, "demonstrate": 1, "beneficial": 1, "plan": 1, "forward": 1, "immediate": 1, "behaviour": 1, "rather": 1, "credit": 1, "assignment": 1}, {"general": 1, "set": 1, "consider": 1, "learn": 1, "make": 1, "decisions": 1, "finite": 1, "interactions": 1, "environment": 1}, {"although": 1, "distinction": 1, "fully": 1, "unambiguous": 1, "exist": 1, "two": 1, "prototypical": 1, "families": 1, "algorithms": 1, "learn": 2, "without": 1, "explicit": 1, "model": 4, "environment": 1, "free": 1, "first": 1, "use": 1, "plan": 1, "solution": 1, "base": 1}, {"good": 1, "reason": 1, "build": 1, "capability": 1, "learn": 1, "sort": 1, "model": 1, "world": 1, "artificial": 1, "agents": 1}, {"model": 1, "may": 2, "allow": 2, "transfer": 1, "knowledge": 3, "ways": 1, "policies": 1, "scalar": 1, "value": 1, "predictions": 1, "agents": 1, "acquire": 1, "rich": 1, "world": 1, "know": 1, "best": 1, "use": 1}, {"addition": 1, "model": 1, "use": 2, "plan": 1, "additional": 2, "computation": 1, "without": 1, "require": 1, "experience": 1, "improve": 1, "agents": 1, "predictions": 1, "decisions": 1}, {"paper": 1, "discuss": 1, "commonalities": 1, "differences": 1, "parametric": 1, "model": 1, "experience": 1, "replay": 1, "lin": 1, "1992": 1}, {"although": 1, "replaybased": 1, "agents": 1, "always": 1, "think": 1, "modelbased": 1, "replay": 1, "share": 1, "many": 1, "characteristics": 1, "often": 1, "associate": 1, "parametric": 1, "model": 1}, {"particular": 1, "plan": 1, "experience": 1, "store": 1, "replay": 1, "memory": 1, "sense": 1, "use": 1, "additional": 1, "computation": 1, "improve": 1, "agents": 1, "predictions": 1, "policies": 1, "interactions": 1, "real": 1, "environment": 1}, {"work": 2, "partially": 1, "inspire": 1, "recent": 1, "kaiser": 1, "et": 1, "al": 1}, {"2019": 1, "show": 1, "plan": 1, "parametric": 1, "model": 1, "allow": 1, "dataefficient": 1, "learn": 1, "several": 1, "atari": 1, "video": 1, "game": 1}, {"main": 1, "comparison": 1, "rainbow": 1, "dqn": 1, "hessel": 1, "et": 1, "al": 1, "2018a": 1, "use": 1, "replay": 1}, {"explain": 1, "result": 1, "may": 1, "perhaps": 1, "consider": 1, "surprise": 1, "show": 1, "likeforlike": 1, "comparison": 1, "rainbow": 1, "dqn": 1, "outperform": 1, "score": 1, "modelbased": 1, "agent": 1, "less": 1, "experience": 1, "computation": 1}, {"33rd": 1, "conference": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "neurips": 1, "2019": 1, "vancouver": 1, "canada": 1}, {"algorithm": 1, "1": 3, "modelbased": 1, "reinforcement": 1, "learn": 1, "input": 5, "state": 2, "sample": 1, "procedure": 1, "2": 2, "model": 1, "3": 1, "policy": 1, "": 4, "4": 1, "predictions": 1, "v": 1, "5": 1, "environment": 1, "e": 2, "6": 1, "get": 1, "initial": 1, "7": 1, "iteration": 1}, {"": 1}, {"": 1}, {"": 3, "k": 1, "8": 1, "interaction": 1, "1": 1, "2": 1}, {"": 1}, {"": 1}, {"": 12, "9": 1, "generate": 2, "action": 1, "10": 1, "reward": 1, "next": 1, "state": 2, "r": 3, "s0": 4, "ea": 1, "11": 1, "u": 2, "pdate": 2, "odels": 1, "12": 1, "v": 1, "agents": 1, "13": 1, "update": 1, "current": 1, "14": 1, "end": 1, "15": 1, "plan": 1, "step": 1, "1": 1, "2": 1}, {"": 1}, {"": 1}, {"": 7, "p": 1, "16": 1, "generate": 2, "state": 2, "action": 1, "17": 1, "reward": 1, "next": 1, "r": 2, "s0": 2, "ms": 1, "18": 1, "v": 1, "u": 1, "pdate": 1, "agents": 1, "19": 1, "end": 2, "20": 1, "discuss": 1, "context": 1, "broad": 1, "discussion": 1, "parametric": 1, "model": 1, "experience": 1, "replay": 1}, {"examine": 1, "equivalences": 1, "potential": 1, "failure": 1, "modes": 1, "plan": 1, "parametric": 2, "model": 2, "exploit": 1, "addition": 1, "instead": 1, "use": 1, "provide": 1, "imagine": 1, "experience": 1, "otherwise": 1, "modelfree": 1, "algorithm": 1}, {"particular": 1, "discuss": 1, "three": 1, "different": 1, "ways": 1, "use": 1, "learn": 1, "imperfect": 1, "model": 1}, {"first": 1, "plan": 1, "forward": 1, "credit": 1, "assignment": 1}, {"mean": 1, "roll": 1, "model": 2, "forward": 1, "real": 1, "state": 1, "instance": 1, "store": 1, "replay": 1, "buffer": 1, "use": 1, "result": 1, "transition": 1, "learn": 1, "predictions": 1, "policies": 1}, {"argue": 1, "worse": 1, "plan": 1, "backward": 1, "real": 2, "state": 3, "former": 1, "involve": 2, "update": 2, "fictional": 2, "experience": 1, "whereas": 1, "latter": 1, "seem": 1, "safer": 1}, {"hypothesis": 1, "validate": 1, "empirically": 1}, {"finally": 1, "third": 1, "use": 1, "model": 1, "plan": 1, "forward": 1, "current": 1, "state": 1, "help": 1, "determine": 1, "immediate": 1, "behaviour": 1}, {"believe": 1, "plan": 3, "backward": 1, "credit": 2, "assignment": 2, "forward": 2, "behaviour": 1, "may": 1, "beneficial": 1}, {"see": 1, "consider": 1, "inaccurate": 1, "model": 1, "instance": 1, "predict": 1, "transition": 1, "magical": 1, "world": 1, "truly": 1, "thereby": 1, "provide": 1, "fictional": 1, "path": 1, "high": 1, "reward": 1}, {"plan": 1, "forward": 1, "credit": 1, "assignment": 1, "may": 1, "result": 1, "incorrect": 1, "predictions": 1, "policies": 1, "assume": 1, "fiction": 1, "real": 1}, {"instead": 1, "plan": 1, "backward": 1, "model": 1, "inaccurate": 1, "may": 3, "lead": 1, "update": 2, "fictional": 2, "state": 2, "unreachable": 1, "useful": 1, "less": 1, "likely": 1, "harmful": 1, "real": 1, "transition": 1}, {"however": 1, "plan": 1, "forward": 1, "use": 1, "inaccurate": 1, "model": 1, "inform": 1, "behaviour": 1, "rather": 1, "trust": 1, "transition": 1, "real": 1, "might": 1, "expect": 1, "agent": 1, "go": 1, "see": 1, "whether": 1, "fact": 1, "magical": 1, "world": 1, "around": 1, "corner": 1}, {"may": 1, "result": 1, "useful": 2, "data": 1, "perhaps": 1, "even": 1, "exploration": 1, "regardless": 1, "whether": 1, "model": 1, "transition": 1, "fact": 1, "exist": 1}, {"1": 1, "": 2, "modelbased": 2, "reinforcement": 1, "learn": 1, "define": 1, "terminology": 1, "use": 1, "paper": 1, "present": 1, "generic": 1, "algorithm": 1, "encompass": 1, "replaybased": 1, "algorithms": 1}, {"consider": 1, "reinforcement": 1, "learn": 1, "set": 1, "sutton": 1, "barto": 1, "2018": 1, "agent": 2, "interact": 1, "environment": 2, "sense": 1, "output": 1, "action": 1, "obtain": 1, "observations": 1, "reward": 1}, {"consider": 1, "control": 1, "set": 1, "goal": 1, "optimise": 1, "accumulation": 1, "reward": 1, "time": 1, "pick": 1, "appropriate": 1, "sequence": 1, "action": 1}, {"action": 1, "agent": 1, "output": 1, "typically": 1, "depend": 1, "state": 1}, {"state": 3, "function": 1, "past": 1, "observations": 1, "case": 2, "sufficient": 1, "use": 1, "immediate": 1, "observation": 1, "sophisticate": 1, "agent": 1, "require": 1, "yield": 1, "suitable": 1, "decisions": 1}, {"state": 2, "agent": 2, "confuse": 1, "2": 1, "": 1, "environment": 1, "typically": 2, "fully": 1, "observable": 1, "also": 1, "much": 1, "large": 1, "reason": 1, "directly": 1}, {"use": 2, "word": 1, "plan": 1, "refer": 1, "algorithm": 1, "additional": 2, "computation": 1, "improve": 1, "predictions": 1, "behaviour": 1, "without": 1, "consume": 1, "data": 1}, {"conversely": 1, "reserve": 1, "term": 1, "learn": 1, "update": 1, "depend": 1, "newly": 1, "observe": 1, "experience": 1}, {"term": 1, "model": 1, "refer": 1, "function": 1, "take": 1, "state": 2, "action": 1, "input": 1, "output": 1, "reward": 1, "next": 1}, {"sometimes": 2, "may": 1, "perfect": 1, "model": 2, "board": 1, "game": 1, "eg": 1, "chess": 1, "go": 1, "need": 1, "learn": 1, "use": 1}, {"model": 2, "stochastic": 2, "approximate": 1, "inherently": 1, "transition": 1, "dynamics": 1, "agents": 1, "uncertainty": 1, "future": 1}, {"expectation": 1, "model": 1, "deterministic": 1, "output": 1, "approximation": 1, "expect": 1, "reward": 1, "state": 1}, {"true": 1, "dynamics": 1, "stochastic": 1, "iterate": 1, "expectation": 1, "model": 2, "multiple": 1, "step": 1, "may": 3, "unhelpful": 1, "expect": 2, "state": 4, "valid": 1, "output": 1, "useful": 1, "semantics": 1, "use": 1, "input": 1, "rather": 1, "real": 1, "cf": 1}, {"wan": 1, "et": 1, "al": 1, "2019": 1}, {"plan": 1, "associate": 1, "model": 2, "common": 1, "way": 1, "use": 2, "computation": 1, "improve": 1, "predictions": 1, "policies": 1, "search": 1}, {"instance": 1, "dyna": 1, "sutton": 1, "1990": 1, "learn": 2, "plan": 2, "combine": 1, "use": 1, "new": 1, "experience": 1, "model": 1, "agents": 1, "predictions": 2, "improve": 1}, {"experience": 1, "replay": 2, "lin": 1, "1992": 1, "refer": 1, "store": 1, "previously": 1, "observe": 1, "transition": 1, "later": 1, "additional": 1, "update": 1, "predictions": 1, "policy": 1}, {"replay": 2, "may": 2, "use": 1, "plan": 1, "query": 1, "stateaction": 1, "pair": 1, "observe": 1, "experience": 1, "indistinguishable": 1, "accurate": 1, "model": 1}, {"sometimes": 1, "may": 1, "practical": 1, "differences": 1, "replay": 1, "model": 1, "depend": 1, "use": 1}, {"hand": 1, "replay": 2, "memory": 2, "less": 1, "flexible": 1, "model": 1, "since": 1, "cannot": 1, "query": 1, "arbitrary": 1, "state": 1, "present": 1}, {"11": 1, "": 2, "generic": 2, "algorithm": 3, "1": 1, "modelbased": 1, "learn": 1}, {"run": 1, "k": 1, "iterations": 1, "interactions": 1, "environment": 1, "occur": 1}, {"total": 1, "number": 1, "interactions": 1, "thus": 1, "": 3, "k": 1}, {"experience": 1, "use": 1, "update": 1, "model": 1, "line": 2, "11": 1, "policy": 1, "predictions": 1, "agent": 1, "12": 1}, {"p": 1, "step": 1, "plan": 1, "perform": 1, "transition": 1, "sample": 1, "model": 1, "use": 1, "update": 1, "agent": 1, "line": 1, "18": 1}, {"p": 1, "": 1, "0": 1, "model": 1, "use": 1, "hence": 1, "algorithm": 1, "modelfree": 1, "could": 1, "also": 1, "skip": 1, "line": 1, "11": 1}, {"p": 1, "": 1, "0": 1, "agent": 1, "update": 1, "line": 1, "12": 1, "anything": 1, "purely": 1, "modelbased": 1, "algorithm": 1}, {"agent": 1, "update": 1, "line": 1, "12": 1, "18": 1, "could": 2, "differ": 1, "treat": 1, "real": 1, "model": 1, "transition": 1, "equivalently": 1}, {"many": 1, "know": 1, "algorithms": 1, "modelbased": 1, "literature": 1, "instance": 1, "algorithm": 1, "1": 1}, {"line": 1, "12": 1, "18": 1, "update": 2, "agents": 1, "predictions": 2, "way": 1, "result": 1, "algorithm": 1, "know": 1, "dyna": 1, "sutton": 2, "1990": 1, "": 1, "instance": 1, "v": 1, "include": 1, "action": 1, "value": 1, "normally": 1, "denote": 1, "q": 1, "use": 1, "qlearning": 1, "watkins": 2, "1989": 1, "dayan": 1, "1992": 1, "obtain": 1, "dynaq": 1, "barto": 1, "2018": 1}, {"one": 1, "extend": 1, "algorithm": 1, "1": 1, "instance": 1, "allow": 1, "plan": 1, "modelfree": 1, "learn": 1, "happen": 1, "simultaneously": 1}, {"extensions": 1, "orthogonal": 1, "discussion": 1, "discuss": 1}, {"algorithms": 1, "typically": 1, "think": 1, "modelfree": 1, "also": 1, "fit": 1, "framework": 1}, {"instance": 1, "dqn": 1, "mnih": 1, "et": 1, "al": 1, "2013": 1, "2015": 1, "neuralfitted": 1, "qiteration": 1, "riedmiller": 1, "2005": 1, "match": 1, "algorithm": 1, "1": 1, "stretch": 1, "definitions": 1, "model": 1, "include": 1, "limit": 1, "replay": 1, "buffer": 1}, {"dqn": 1, "learn": 1, "transition": 1, "sample": 1, "replay": 1, "buffer": 1, "use": 1, "qlearning": 1, "neural": 1, "network": 1}, {"algorithm": 1, "1": 1, "correspond": 1, "update": 1, "nonparametric": 1, "model": 1, "line": 2, "11": 1, "store": 1, "observe": 1, "transition": 3, "buffer": 2, "perhaps": 1, "overwrite": 1, "old": 1, "17": 1, "retrieve": 1}, {"policy": 1, "update": 1, "transition": 1, "sample": 1, "replay": 1, "buffer": 1, "ie": 1, "line": 1, "12": 1, "effect": 1}, {"2": 1, "": 2, "model": 2, "properties": 1, "main": 1, "advantage": 1, "use": 2, "ability": 1, "plan": 1, "additional": 1, "computation": 1, "new": 1, "data": 1, "improve": 1, "agents": 1, "policy": 1, "predictions": 1}, {"sutton": 1, "barto": 1, "2018": 1, "illustrate": 1, "benefit": 1, "plan": 1, "simple": 1, "grid": 1, "world": 1, "figure": 1, "1": 1, "leave": 1, "agent": 1, "must": 1, "learn": 1, "navigate": 1, "along": 1, "shortest": 1, "path": 1, "fix": 1, "goal": 1, "location": 1}, {"right": 1, "figure": 1, "1": 1, "use": 1, "domain": 1, "show": 1, "performance": 1, "replaybased": 1, "qlearning": 1, "agent": 2, "blue": 1, "dynaq": 1, "3": 1, "": 1, "total": 1, "step": 1, "log": 1}, {"scale": 1, "": 13, "scalability": 1, "3e4": 1, "forw": 1, "ard": 1, "1e4": 1, "rep": 1, "3e3": 1, "01": 1, "dyn": 1, "lay": 1, "03": 1, "10": 1, "update": 1, "per": 1, "real": 1, "step": 1, "30": 1, "figure": 1, "1": 1, "leave": 1, "layout": 1, "grid": 1, "world": 1, "sutton": 1, "barto": 1, "2018": 1, "g": 1, "denote": 1, "start": 1, "goal": 1, "state": 1, "respectively": 1}, {"right": 1, "qlearning": 1, "replay": 1, "blue": 1, "dynaq": 1, "parametric": 1, "model": 1, "red": 1, "yaxis": 1, "total": 1, "number": 2, "step": 2, "complete": 1, "25": 1, "episodes": 1, "experience": 1, "xaxis": 1, "update": 1, "per": 1, "environment": 1}, {"ax": 1, "logarithmic": 1, "scale": 1}, {"red": 1, "scale": 1, "similarly": 1, "amount": 1, "plan": 1, "measure": 1, "term": 1, "number": 1, "update": 1, "per": 1, "real": 1, "environment": 1, "step": 1}, {"agents": 1, "use": 2, "multilayer": 1, "perceptron": 1, "approximate": 1, "action": 1, "value": 1, "dynaq": 1, "also": 1, "identical": 1, "network": 1, "model": 1, "transition": 1, "terminations": 1, "reward": 1}, {"algorithm": 1, "call": 1, "forward": 2, "dyna": 1, "figure": 1, "sample": 1, "state": 1, "replay": 1, "step": 2, "one": 1, "use": 1, "model": 1}, {"later": 1, "consider": 1, "variant": 1, "instead": 1, "step": 1, "backward": 1, "inverse": 1, "model": 1}, {"appendix": 1, "contain": 1, "detail": 1, "experiment": 1}, {"21": 1, "": 2, "computational": 2, "properties": 1, "clear": 1, "differences": 1, "use": 1, "parametric": 1, "model": 1, "replay": 1}, {"instance": 1, "kaiser": 1, "et": 1, "al": 1}, {"2019": 1, "use": 1, "fairly": 1, "large": 1, "deep": 1, "neural": 1, "network": 1, "model": 1, "pixel": 1, "dynamics": 1, "atari": 1, "mean": 1, "predict": 1, "single": 1, "transition": 1, "require": 1, "nontrivial": 1, "computation": 1}, {"general": 1, "parametric": 1, "model": 1, "typically": 1, "require": 1, "computations": 1, "take": 1, "sample": 1, "replay": 1, "buffer": 1}, {"hand": 1, "replay": 1, "tightly": 1, "couple": 1, "model": 1, "capacity": 1, "memory": 2, "requirements": 1, "transition": 1, "store": 1, "take": 1, "certain": 1, "amount": 1}, {"remove": 1, "transition": 1, "memory": 1, "grow": 1, "unbounded": 1}, {"limit": 2, "memory": 1, "usage": 1, "imply": 1, "effective": 1, "capacity": 1, "replay": 1, "transition": 1, "replace": 1, "forget": 1, "completely": 1}, {"contrast": 1, "parametric": 1, "model": 1, "may": 1, "able": 1, "achieve": 1, "good": 1, "accuracy": 1, "fix": 1, "comparatively": 1, "small": 1, "memory": 1, "footprint": 1}, {"22": 1, "": 2, "equivalences": 1, "suppose": 1, "manage": 1, "learn": 1, "model": 1, "perfectly": 1, "match": 1, "transition": 1, "observe": 1, "thus": 1, "far": 1}, {"would": 2, "use": 1, "perfect": 1, "model": 1, "generate": 1, "experience": 2, "state": 1, "actually": 1, "observe": 1, "result": 1, "update": 1, "indistinguishable": 1, "replay": 1}, {"sense": 1, "replay": 1, "match": 1, "perfect": 1, "model": 2, "albeit": 1, "state": 2, "observed1": 1, "therefore": 1, "else": 1, "equal": 1, "would": 1, "expect": 1, "use": 1, "imperfect": 1, "eg": 1, "parametric": 1, "generate": 1, "fictional": 1, "experience": 1, "truly": 1, "observe": 1, "probably": 1, "result": 1, "better": 1, "learn": 1}, {"subtleties": 1, "argument": 1}, {"first": 1, "argument": 1, "make": 1, "even": 1, "stronger": 1, "case": 1}, {"make": 1, "linear": 1, "predictions": 1, "leastsquares": 1, "temporaldifference": 1, "learn": 1, "lstd": 1, "bradtke": 1, "barto": 1, "1996": 1, "boyan": 1, "1999": 1, "modelfree": 1, "algorithm": 1, "original": 1, "data": 2, "require": 1, "indeed": 1, "benefit": 1, "plan": 1, "solution": 1, "already": 1, "best": 1, "fit": 1, "least": 1, "square": 1, "sense": 1, "even": 1, "single": 1, "pass": 1}, {"fact": 1, "fit": 1, "linear": 1, "model": 2, "data": 1, "fully": 1, "solve": 1, "solution": 2, "equal": 1, "lstd": 1, "parr": 1, "et": 1, "al": 1, "2008": 1}, {"one": 1, "also": 1, "show": 1, "exhaustive": 1, "replay": 2, "linear": 1, "td": 1, "sutton": 2, "1988": 1, "equivalent": 1, "onetime": 1, "pass": 1, "data": 2, "lstd": 1, "van": 1, "seijen": 1, "2015": 1, "similarly": 1, "allow": 1, "us": 1, "solve": 1, "empirical": 1, "model": 1, "implicitly": 1, "define": 1, "observe": 1}, {"full": 1, "equivalences": 1, "however": 1, "limit": 1, "linear": 1, "prediction": 1, "extend": 1, "straightforwardly": 1, "nonlinear": 1, "function": 1, "control": 1, "set": 1}, {"leave": 1, "open": 1, "question": 1, "use": 1, "parametric": 1, "model": 1, "rather": 1, "replay": 1, "vice": 1, "versa": 1}, {"1": 1, "": 1, "one": 2, "could": 1, "go": 1, "step": 1, "extend": 1, "replay": 1, "full": 1, "nonparametric": 1, "model": 1}, {"instance": 1, "pan": 1, "et": 1, "al": 1}, {"2018": 1, "use": 1, "kernel": 1, "methods": 1, "allow": 1, "query": 1, "replaybased": 1, "model": 1, "state": 1, "store": 1, "buffer": 1}, {"4": 1, "": 5, "6e4": 1, "3e4": 1, "1": 2, "0": 1, "search": 1, "depth": 1, "2": 1, "episode": 1, "length": 1, "log": 1}, {"scale": 1, "": 1, "total": 1, "step": 1, "log": 1}, {"scale": 1, "": 10, "9e4": 1, "100": 1, "forward": 1, "dyna": 2, "30": 1, "backward": 1, "10": 1, "replay": 1, "0": 1, "1000": 1, "2000": 1, "3000": 1, "4000": 1, "episode": 2, "length": 1, "log": 1}, {"scale": 1, "": 10, "backward": 3, "vs": 2, "forward": 3, "deterministic": 1, "plan": 1, "stochastic": 1, "100": 1, "dyna": 2, "30": 1, "replay": 1, "10": 1, "0": 1, "1000": 1, "2000": 1, "3000": 1, "4000": 1, "episode": 1, "figure": 1, "2": 1, "leave": 1, "four": 1, "room": 1, "grid": 1, "world": 1, "sutton": 1, "et": 1, "al": 1, "1998": 1}, {"centerleft": 1, "plan": 1, "forward": 1, "current": 2, "state": 1, "update": 1, "behaviour": 1, "0": 1, "step": 2, "correspond": 1, "qlearning": 1, "yaxis": 1, "total": 1, "number": 1, "require": 1, "complete": 1, "100": 1, "episodes": 1, "xaxis": 1, "search": 1, "depth": 1}, {"centerright": 1, "compare": 1, "replay": 1, "blue": 1, "forward": 1, "dyna": 2, "red": 1, "backward": 1, "black": 1, "yaxis": 1, "episode": 1, "length": 1, "logarithmic": 1, "scale": 1, "xaxis": 1, "number": 1, "episodes": 1}, {"right": 1, "add": 1, "stochasticity": 1, "transition": 2, "dynamics": 1, "form": 1, "20": 1, "probability": 1, "random": 1, "adjacent": 1, "cell": 1, "irrespectively": 1, "action": 1, "compare": 1, "replay": 1, "blue": 1, "forward": 1, "dyna": 2, "red": 1, "backward": 1, "black": 1, "yaxis": 1, "episode": 1, "length": 1, "logarithmic": 1, "scale": 1, "xaxis": 1, "number": 1, "episodes": 1, "23": 1, "": 1, "parametric": 1, "model": 1, "help": 1, "learn": 1}, {"expect": 1, "benefit": 1, "learn": 1, "use": 2, "parametric": 1, "model": 1, "rather": 1, "actual": 1, "data": 1}, {"discuss": 1, "important": 1, "computational": 1, "differences": 1}, {"focus": 1, "learn": 2, "efficiency": 1, "parametric": 1, "model": 1, "help": 1}, {"first": 1, "parametric": 1, "model": 1, "may": 1, "useful": 1, "plan": 1, "future": 1, "help": 1, "determine": 1, "policy": 1, "behaviour": 1}, {"ability": 1, "generalise": 1, "unseen": 1, "counterfactual": 1, "transition": 1, "use": 1, "plan": 2, "current": 1, "state": 2, "future": 1, "sometimes": 1, "call": 1, "kaelbling": 1, "lozanoprez": 1, "2010": 1, "even": 1, "exact": 1, "never": 1, "observe": 1}, {"commonly": 1, "successfully": 1, "employ": 1, "modelpredictive": 1, "control": 1, "richalet": 1, "et": 2, "al": 2, "1978": 1, "morari": 1, "lee": 1, "1999": 1, "mayne": 1, "2014": 1, "wagener": 1, "2019": 1}, {"classically": 1, "model": 1, "construct": 1, "hand": 1, "rather": 1, "learn": 1, "directly": 1, "experience": 1, "principle": 1, "plan": 1, "forward": 1, "find": 1, "suitable": 1, "behaviour": 1}, {"possible": 1, "replicate": 1, "standard": 1, "replay": 2, "interest": 1, "rich": 1, "domains": 1, "current": 1, "state": 1, "typically": 1, "exactly": 1, "appear": 1}, {"even": 1, "would": 1, "replay": 1, "allow": 1, "easy": 1, "generation": 1, "possible": 1, "next": 1, "state": 1, "addition": 1, "one": 1, "trajectory": 1, "actually": 1, "happen": 1}, {"use": 1, "model": 2, "select": 1, "action": 1, "rather": 1, "trust": 1, "imagine": 1, "transition": 1, "update": 1, "policy": 1, "predictions": 1, "may": 1, "less": 1, "essential": 1, "highly": 1, "accurate": 1}, {"instance": 1, "model": 2, "may": 1, "predict": 1, "shortcut": 1, "actually": 1, "exist": 1, "use": 1, "steer": 1, "behaviour": 2, "result": 1, "experience": 1, "suitable": 1, "correct": 1, "error": 1, "yield": 1, "kind": 1, "direct": 1, "temporally": 1, "consistent": 1, "typically": 1, "seek": 1, "exploration": 1, "purpose": 1, "lowrey": 1, "et": 1, "al": 1, "2019": 1}, {"illustrate": 1, "experiment": 1, "classic": 1, "four": 1, "room": 1, "gridworld": 1, "sutton": 1, "et": 1, "al": 1, "1998": 1}, {"learn": 1, "tabular": 1, "forward": 1, "model": 1, "generate": 1, "transition": 1, "": 5, "r": 2, "s0": 2, "state": 1, "action": 1, "reward": 1, "0": 1, "1": 1, "discount": 1, "factor": 1}, {"use": 1, "model": 1, "plan": 1, "via": 2, "simple": 1, "breadthfirst": 1, "search": 1, "fix": 1, "depth": 1, "bootstrapping": 1, "value": 1, "function": 1, "qs": 1, "learn": 1, "standard": 1, "qlearning": 1}, {"use": 1, "result": 1, "plan": 1, "value": 1, "action": 1, "current": 1, "state": 1, "behave": 1}, {"process": 1, "interpret": 1, "use": 1, "multistep": 1, "greedy": 2, "policy": 2, "efroni": 1, "et": 1, "al": 1, "2018": 1, "determine": 1, "behaviour": 1, "instead": 1, "standard": 1, "onestep": 1}, {"result": 1, "illustrate": 1, "second": 1, "plot": 1, "figure": 1, "2": 1, "plan": 1, "beneficial": 1}, {"addition": 1, "plan": 2, "forward": 1, "improve": 1, "behaviour": 1, "model": 1, "may": 1, "useful": 1, "credit": 1, "assignment": 1, "backward": 1}, {"consider": 1, "algorithm": 1, "sample": 1, "real": 1, "visit": 1, "state": 2, "replay": 1, "buffer": 1, "instead": 1, "plan": 2, "one": 2, "step": 2, "future": 1, "backward": 1}, {"one": 1, "motivation": 1, "model": 1, "poor": 1, "plan": 1, "step": 1, "forward": 1, "update": 1, "real": 1, "sample": 1, "state": 1, "mislead": 1, "imagine": 1, "transition": 1}, {"potentially": 1, "cause": 1, "harmful": 1, "update": 1, "value": 1, "real": 1, "state": 1}, {"conversely": 1, "plan": 1, "backwards": 1, "update": 1, "imagine": 1, "state": 1}, {"model": 1, "poor": 1, "imagine": 1, "state": 2, "perhaps": 1, "resemble": 1, "real": 1}, {"update": 1, "fictional": 1, "state": 1, "seem": 1, "less": 1, "harmful": 1}, {"model": 1, "become": 1, "accurate": 1, "forward": 1, "backward": 1, "plan": 1, "start": 1, "equally": 1, "useful": 1}, {"purely": 1, "datadriven": 1, "partial": 1, "model": 1, "replay": 1, "buffer": 1, "5": 1, "": 1, "meaningful": 1, "distinction": 1}, {"learn": 1, "model": 1, "time": 1, "inaccurate": 1, "backward": 1, "plan": 2, "may": 1, "less": 1, "errorprone": 1, "forward": 1, "credit": 1, "assignment": 1}, {"illustrate": 1, "potential": 1, "benefit": 1, "backward": 1, "plan": 1, "simple": 1, "experiment": 1, "fourroom": 1, "environment": 1}, {"two": 1, "rightmost": 1, "plot": 1, "figure": 1, "2": 1, "compare": 1, "performance": 1, "apply": 1, "tabular": 1, "qlearning": 1, "transition": 1, "generate": 1, "forward": 1, "model": 2, "red": 1, "backward": 1, "black": 1, "replay": 1, "blue": 1}, {"forward": 1, "model": 1, "learn": 1, "distributions": 1, "state": 1, "reward": 1, "terminations": 1, "prr": 1, "": 1, "s0": 1}, {"backward": 1, "model": 1, "learn": 1, "inverse": 1, "prs": 1, "ar": 1, "": 2, "s0": 1}, {"use": 1, "dirichlet1": 1, "prior": 1}, {"evaluate": 1, "algorithms": 1, "deterministic": 1, "fourroom": 1, "environment": 1, "well": 1, "stochastic": 1, "variant": 1, "step": 1, "20": 1, "probability": 1, "transition": 1, "random": 1, "adjacent": 1, "cell": 1, "irrespective": 1, "action": 1}, {"case": 1, "backward": 1, "plan": 2, "result": 1, "faster": 1, "learn": 1, "forward": 1}, {"deterministic": 1, "case": 1, "forward": 1, "model": 2, "catch": 1, "later": 2, "learn": 2, "reach": 1, "performance": 1, "replay": 2, "2000": 1, "episodes": 1, "instead": 1, "plan": 1, "backward": 1, "competitive": 1, "early": 1, "perform": 1, "slightly": 1, "worse": 1, "train": 1}, {"conjecture": 1, "slower": 1, "convergence": 1, "later": 2, "stag": 1, "train": 2, "may": 1, "due": 1, "fact": 1, "predict": 1, "source": 1, "state": 1, "action": 1, "transition": 2, "nonstationary": 1, "problem": 1, "depend": 1, "agents": 1, "policy": 1, "give": 1, "early": 2, "episodes": 2, "include": 1, "many": 2, "ones": 1, "take": 1, "bayesian": 1, "model": 1, "forget": 1, "policies": 1, "observe": 1}, {"lack": 1, "convergence": 1, "optimal": 1, "policy": 1, "forward": 1, "plan": 1, "algorithm": 1, "stochastic": 1, "set": 1, "may": 2, "due": 1, "independent": 1, "sample": 1, "successor": 1, "state": 1, "reward": 1, "result": 1, "inconsistent": 1, "transition": 1}, {"issue": 1, "may": 1, "address": 1, "suitable": 1, "choice": 1, "model": 1}, {"detail": 1, "investigations": 1, "scope": 1, "paper": 1, "good": 1, "recognise": 1, "model": 1, "choices": 1, "measurable": 1, "effect": 1, "learn": 1}, {"3": 1, "": 2, "failure": 1, "learn": 3, "describe": 1, "plan": 1, "dynastyle": 1, "algorithm": 1, "perhaps": 1, "surprisingly": 1, "easily": 1, "lead": 1, "catastrophic": 1, "update": 1}, {"algorithms": 1, "combine": 1, "function": 1, "approximation": 1, "eg": 1, "neural": 1, "network": 1, "bootstrapping": 1, "temporal": 1, "difference": 1, "methods": 1, "sutton": 5, "1988": 1, "offpolicy": 1, "learn": 1, "barto": 2, "2018": 3, "precup": 1, "et": 3, "al": 3, "2000": 1, "unstable": 1, "williams": 1, "baird": 2, "iii": 1, "1993": 1, "1995": 2, "tsitsiklis": 1, "van": 2, "roy": 1, "1997": 1, "2009": 1, "2016": 1, "": 1, "sometimes": 1, "call": 1, "deadly": 1, "triad": 1, "hasselt": 1}, {"implications": 1, "dynastyle": 1, "learn": 1, "well": 1, "replay": 1, "methods": 1, "cf": 1}, {"van": 1, "hasselt": 1, "et": 1, "al": 1, "2018": 1}, {"use": 1, "replay": 1, "sometimes": 1, "relatively": 1, "straightforward": 1, "determine": 1, "offpolicy": 1, "state": 1, "sample": 2, "distribution": 2, "transition": 3, "always": 1, "real": 1, "assume": 1, "dynamics": 1, "stationary": 1}, {"contrast": 1, "project": 1, "state": 2, "give": 1, "parametric": 1, "model": 2, "may": 1, "differ": 1, "would": 1, "occur": 1, "real": 1, "dynamics": 1, "due": 1, "error": 1}, {"update": 1, "rule": 1, "solve": 1, "predictive": 1, "question": 1, "mdp": 2, "induce": 1, "model": 1, "state": 1, "distribution": 2, "match": 1, "onpolicy": 1}, {"understand": 1, "issue": 1, "better": 1, "consider": 1, "use": 1, "algorithm": 1, "1": 1, "estimate": 1, "expect": 1, "cumulative": 1, "discount": 1, "reward": 1, "v": 1, "": 4, "e": 1, "rt1": 1, "rt2": 1}, {"": 1}, {"": 1}, {"": 24, "st": 4, "policy": 1, "update": 1, "vw": 4, "v": 1, "temporal": 1, "difference": 1, "td": 1, "learn": 1, "sutton": 1, "1988": 1, "w": 3, "rt1": 2, "t1": 2, "st1": 2, "1": 2, "r": 1, "0": 2, "reward": 1, "discount": 1, "transition": 1, "small": 1, "step": 1, "size": 1}, {"consider": 1, "linear": 1, "predictions": 1, "vw": 1, "st": 3, "": 7, "w": 1, "xt": 2, "v": 1, "xst": 1, "feature": 1, "vector": 1, "state": 1}, {"expect": 1, "w": 1, "": 11, "aw": 1, "b": 2, "td": 1, "update": 1, "e": 2, "rt1": 1, "xt": 3, "x": 3, "xt1": 1, "di": 1, "p": 1, "expectation": 1, "transition": 1, "dynamics": 1, "sample": 1, "distribution": 1, "state": 1}, {"transition": 2, "dynamics": 1, "write": 1, "matrix": 1, "p": 1, "contain": 1, "probabilities": 1, "pij": 1, "": 6, "pst1": 1, "st": 1, "j": 2, "state": 2, "policy": 1}, {"diagonal": 2, "matrix": 1, "contain": 1, "probabilities": 1, "dii": 1, "": 5, "di": 1, "p": 1, "st": 1, "sample": 1, "state": 1}, {"matrix": 1, "x": 1, "contain": 1, "feature": 2, "vectors": 1, "xs": 1, "state": 2, "row": 1, "map": 1, "space": 1}, {"note": 1, "p": 1, "linear": 1, "operators": 1, "state": 1, "space": 2, "feature": 1}, {"update": 1, "guarantee": 1, "stable": 1, "ie": 1, "converge": 1, "": 2, "x": 2, "di": 1, "p": 1, "positive": 1, "semidefinite": 1, "sutton": 1, "et": 1, "al": 1, "2016": 1, "spectral": 1, "radius": 1, "smaller": 1, "1": 1}, {"deadly": 1, "triad": 1, "occur": 1, "p": 1, "match": 1, "negative": 1, "definite": 1, "spectral": 1, "radius": 1, "": 2, "6": 1, "larger": 1, "one": 1, "weight": 1, "diverge": 1}, {"happen": 1, "correspond": 1, "steadystate": 1, "distribution": 1, "policy": 1, "condition": 1, "p": 1, "": 1, "update": 1, "offpolicy": 1}, {"proposition": 1, "1": 1}, {"consider": 1, "uniformly": 1, "replay": 1, "transition": 2, "buffer": 1, "contain": 1, "full": 3, "episodes": 2, "eg": 1, "add": 1, "new": 1, "termination": 1, "potentially": 1, "remove": 1, "old": 1, "episode": 1, "use": 1, "td": 1, "algorithm": 1, "define": 1, "update": 1, "1": 1}, {"algorithm": 1, "stable": 1}, {"proof": 1}, {"replay": 2, "buffer": 1, "define": 1, "empirical": 2, "model": 1, "induce": 1, "policy": 1, "distribution": 1, "action": 1, "": 1, "ns": 3, "ans": 1, "number": 1, "time": 1, "pair": 1, "show": 1}, {"behaviour": 1, "policy": 2, "change": 1, "fill": 1, "replay": 1, "result": 1, "empirical": 1, "sample": 1, "mixture": 1, "policies": 1}, {"empirical": 2, "transition": 1, "pij": 1, "": 3, "ni": 1, "jni": 1, "state": 1, "distributions": 1, "dii": 1, "nsn": 1, "n": 1, "total": 1, "size": 1, "replay": 1, "buffer": 1, "correspond": 1, "policy": 1}, {"therefore": 1, "x": 2, "di": 1, "": 3, "p": 1, "0": 1, "td": 1, "stable": 1, "diverge": 1}, {"proposition": 1, "extend": 1, "case": 1, "transition": 1, "add": 1, "replay": 1, "one": 1, "time": 1, "rather": 1, "full": 1, "episodes": 1}, {"however": 1, "sample": 1, "state": 1, "accord": 1, "nonuniform": 1, "distribution": 1, "eg": 1, "use": 1, "prioritise": 1, "replay": 1, "make": 1, "replaybased": 1, "algorithms": 1, "less": 1, "stable": 1, "potentially": 1, "divergent": 1, "cf": 1}, {"van": 1, "hasselt": 1, "et": 1, "al": 1, "2018": 1}, {"show": 1, "similar": 1, "algorithm": 1, "use": 1, "model": 1, "place": 1, "replay": 1, "diverge": 1}, {"proposition": 1, "2": 1}, {"consider": 1, "uniformly": 1, "replay": 2, "state": 1, "buffer": 1, "generate": 1, "transition": 2, "learn": 1, "model": 1, "pm": 1, "": 1, "use": 1, "td": 1, "update": 1, "1": 1}, {"algorithm": 1, "diverge": 1}, {"proof": 1}, {"learn": 1, "dynamics": 3, "pm": 1, "": 1, "p": 1, "necessarily": 2, "match": 1, "empirical": 2, "replay": 2, "mean": 1, "distribution": 2, "use": 1, "update": 1, "correspond": 1, "steadystate": 1}, {"model": 2, "error": 1, "could": 1, "lead": 2, "negative": 1, "definite": 1, "": 5, "x": 2, "di": 1, "p": 1, "result": 1, "spectral": 1, "radius": 1, "1": 1, "divergence": 1, "parameters": 1, "w": 1, "intuitively": 1, "issue": 1, "state": 3, "uncommon": 1, "impossible": 1, "sample": 3, "distribution": 1, "update": 2, "directly": 1, "change": 1, "generalisation": 1}, {"lead": 1, "divergent": 1, "learn": 1, "dynamics": 1}, {"ways": 1, "mitigate": 1, "failure": 1, "describe": 1}, {"first": 1, "could": 1, "repeatedly": 1, "iterate": 1, "model": 3, "sample": 1, "transition": 1, "state": 3, "generate": 1, "well": 1, "induce": 1, "distribution": 1, "consistent": 1}, {"fully": 1, "satisfactory": 1, "state": 1, "typically": 1, "become": 1, "evermore": 1, "unrealistic": 1, "iterate": 1, "learn": 1, "model": 1, "although": 1, "indication": 1, "may": 1, "helpful": 1, "holland": 1, "et": 1, "al": 1, "2018": 1}, {"second": 1, "could": 1, "rely": 1, "less": 1, "bootstrapping": 1, "use": 1, "multistep": 1, "return": 1, "sutton": 3, "1988": 1, "van": 1, "hasselt": 1, "2015": 1, "barto": 1, "2018": 1}, {"mitigate": 1, "instability": 1, "cf": 1}, {"van": 1, "hasselt": 1, "et": 1, "al": 1, "2018": 1}, {"extreme": 1, "full": 1, "montecarlo": 1, "update": 1, "diverge": 1, "though": 1, "would": 1, "high": 1, "variance": 1}, {"third": 1, "could": 1, "employ": 1, "algorithms": 1, "specifically": 1, "stable": 1, "offpolicy": 1, "learn": 1, "although": 1, "often": 1, "specific": 1, "linear": 1, "set": 1, "sutton": 2, "et": 3, "al": 3, "2008": 1, "2009": 1, "van": 1, "hasselt": 1, "2014": 1, "assume": 1, "sample": 1, "do": 1, "trajectory": 1, "2016": 1}, {"note": 1, "several": 1, "algorithms": 1, "exist": 1, "correct": 1, "return": 1, "towards": 1, "desire": 1, "policy": 1, "harutyunyan": 1, "et": 2, "al": 2, "2016": 2, "munos": 1, "separate": 1, "issue": 1, "offpolicy": 1, "sample": 1, "state": 1}, {"although": 1, "offpolicy": 1, "learn": 1, "algorithms": 1, "may": 1, "part": 1, "longterm": 1, "answer": 1, "yet": 1, "definitive": 1, "solution": 1}, {"quote": 1, "sutton": 1, "barto": 1, "2018": 1, "potential": 1, "offpolicy": 1, "learn": 1, "remain": 1, "tantalise": 1, "best": 1, "way": 1, "achieve": 1, "still": 1, "mystery": 1}, {"understand": 2, "failures": 1, "learn": 1, "important": 1, "improve": 1, "algorithms": 1}, {"however": 1, "divergence": 1, "occur": 2, "mean": 1, "cf": 1}, {"van": 1, "hasselt": 1, "et": 1, "al": 1, "2018": 1}, {"indeed": 1, "next": 1, "section": 1, "compare": 1, "replaybased": 1, "algorithm": 2, "modelbased": 1, "stable": 1, "enough": 1, "achieve": 1, "impressive": 1, "sampleefficiency": 1, "atari": 1, "benchmark": 1}, {"4": 1, "": 2, "modelbased": 1, "algorithms": 2, "scale": 1, "discuss": 1, "two": 1, "detail": 1, "first": 1, "simple": 1, "kaiser": 2, "et": 3, "al": 3, "2019": 1, "use": 3, "parametric": 1, "model": 1, "rainbow": 1, "dqn": 1, "hessel": 1, "2018a": 1, "experience": 1, "replay": 1, "baseline": 1}, {"7": 1, "": 1, "simple": 1, "kaiser": 1, "et": 1, "al": 1}, {"2019": 1, "show": 1, "dataefficient": 1, "learn": 4, "possible": 1, "atari": 1, "2600": 1, "videos": 1, "game": 2, "arcade": 1, "environment": 1, "bellemare": 1, "et": 1, "al": 1, "2013": 1, "purely": 1, "modelbased": 1, "approach": 1, "update": 1, "policy": 2, "data": 1, "sample": 1, "parametric": 1, "model": 1, "result": 1, "simulate": 2, "simple": 1, "algorithm": 1, "perform": 1, "relatively": 1, "well": 1, "102400": 1, "interactions": 1, "409600": 1, "frame": 1, "": 1, "two": 1, "hours": 1, "play": 1, "within": 1}, {"algorithm": 1, "1": 1, "correspond": 1, "set": 1, "k": 1, "": 4, "16": 1, "6400": 1, "102400": 1}, {"although": 1, "simple": 1, "use": 3, "limit": 1, "data": 1, "large": 1, "number": 1, "sample": 1, "model": 1, "similar": 1, "p": 1, "": 1, "8000002": 1, "rainbow": 1, "dqn": 1, "one": 1, "main": 1, "result": 1, "kaiser": 1, "et": 1, "al": 1}, {"2019": 1, "compare": 1, "simple": 1, "rainbow": 1, "dqn": 2, "hessel": 1, "et": 7, "al": 7, "2018a": 1, "combine": 1, "algorithm": 1, "mnih": 1, "2013": 1, "2015": 1, "double": 1, "qlearning": 1, "van": 2, "hasselt": 2, "2010": 1, "2016": 3, "duel": 1, "network": 2, "architectures": 1, "wang": 1, "prioritise": 1, "experience": 1, "replay": 1, "schaul": 1, "noisy": 1, "exploration": 1, "fortunato": 1, "2017": 2, "distributional": 1, "reinforcement": 1, "learn": 1, "bellemare": 1}, {"like": 1, "dqn": 2, "rainbow": 1, "use": 2, "minibatches": 1, "transition": 1, "sample": 1, "experience": 1, "replay": 1, "lin": 1, "1992": 1, "qlearning": 1, "watkins": 1, "1989": 1, "learn": 1, "actionvalue": 1, "estimate": 1, "determine": 1, "policy": 1}, {"rainbow": 1, "dqn": 1, "use": 1, "multistep": 1, "return": 1, "cf": 1}, {"sutton": 2, "1988": 1, "barto": 1, "2018": 1, "rather": 1, "onestep": 1, "return": 1, "use": 1, "original": 1, "dqn": 1, "algorithm": 1}, {"41": 1, "": 4, "data": 1, "efficient": 1, "rainbow": 1, "dqn": 1, "notation": 1, "algorithm": 1, "1": 1, "total": 2, "number": 2, "transition": 1, "sample": 1, "replay": 1, "learn": 1, "k": 2, "p": 1, "interactions": 1, "environment": 1}, {"originally": 1, "dqn": 2, "rainbow": 1, "batch": 1, "32": 1, "transition": 1, "sample": 1, "every": 1, "4": 1, "real": 1, "interactions": 1}, {"": 2, "4": 1, "p": 1, "32": 1}, {"total": 1, "number": 1, "interactions": 1, "50m": 1, "200": 1, "million": 1, "frame": 1, "mean": 1, "k": 1, "": 2, "50m4": 1, "125m": 1}, {"experiment": 1, "train": 1, "rainbow": 1, "dqn": 1, "total": 1, "number": 1, "real": 1, "interactions": 1, "comparable": 1, "simple": 1, "set": 1, "k": 1, "": 3, "100000": 1, "1": 1, "p": 1, "32": 1}, {"total": 2, "number": 2, "replay": 1, "sample": 2, "32": 1, "million": 2, "still": 1, "less": 1, "model": 1, "use": 1, "simple": 1, "152": 1}, {"rainbow": 1, "dqn": 1, "also": 1, "efficient": 1, "computationwise": 1, "since": 1, "sample": 1, "replay": 1, "buffer": 1, "faster": 1, "generate": 1, "transition": 1, "learn": 1, "model": 1}, {"change": 1, "make": 2, "rainbow": 1, "dqn": 1, "data": 1, "efficient": 1, "increase": 1, "number": 2, "step": 2, "multistep": 1, "return": 1, "3": 1, "20": 2, "reduce": 1, "start": 1, "sample": 1, "replay": 1, "000": 1, "1600": 1}, {"use": 1, "fairly": 1, "standard": 1, "convolutional": 1, "q": 1, "network": 1, "hessel": 1, "et": 1, "al": 1}, {"2018b": 1}, {"try": 1, "exhaustively": 1, "tune": 2, "algorithm": 2, "doubt": 1, "make": 1, "even": 1, "data": 1, "efficient": 1, "futher": 1, "hyperparameters": 1}, {"42": 1, "": 2, "empirical": 1, "result": 1, "run": 1, "rainbow": 1, "dqn": 1, "26": 1, "atari": 1, "game": 1, "report": 1, "kaiser": 1, "et": 1, "al": 1}, {"2019": 1}, {"figure": 1, "3": 1, "plot": 1, "performance": 1, "version": 1, "rainbow": 1, "dqn": 1, "function": 1, "number": 1, "interactions": 1, "environment": 1}, {"performance": 1, "measure": 1, "term": 1, "episode": 1, "return": 1, "normalise": 1, "use": 1, "human": 1, "random": 1, "score": 1, "van": 1, "hasselt": 1, "et": 1, "al": 1, "2016": 1, "aggregate": 1, "across": 1, "26": 1, "game": 1, "take": 1, "median": 1}, {"error": 1, "bar": 1, "show": 1, "compute": 1, "5": 1, "independent": 1, "replicas": 1, "experiment": 1}, {"final": 1, "performance": 1, "simple": 1, "accord": 1, "metric": 1, "show": 1, "figure": 1, "3": 1, "dash": 1, "horizontal": 1, "line": 1}, {"expect": 1, "hyperparameters": 1, "propose": 1, "hessel": 1, "et": 1, "al": 1}, {"2018a": 1, "largerdata": 1, "regime": 2, "50": 1, "million": 1, "interactions": 1, "well": 1, "suit": 1, "extreme": 1, "dataefficiency": 1, "purple": 1, "line": 1, "figure": 1, "3": 1}, {"performance": 3, "better": 1, "slightlytweaked": 1, "dataefficient": 1, "version": 1, "rainbow": 1, "dqn": 1, "red": 1, "match": 1, "simple": 1, "70000": 1, "interactions": 2, "environment": 1, "reach": 1, "roughly": 1, "25": 1, "higher": 1, "100000": 1}, {"performance": 1, "agent": 1, "superior": 1, "simple": 1, "17": 1, "26": 1, "game": 1}, {"detail": 1, "result": 1, "include": 2, "appendix": 1, "ablations": 1, "pergame": 1, "performance": 1}, {"2": 1, "": 3, "actual": 1, "number": 1, "report": 1, "model": 1, "sample": 1, "19": 1, "800": 1, "000": 1, "152": 1, "million": 1, "p": 1, "vary": 1, "depend": 1, "iteration": 1}, {"8": 1, "": 24, "010": 1, "simple": 1, "w": 1, "bo": 1, "005": 1, "da": 1, "tr": 1, "n": 1, "icie": 1, "ff": 1, "ae": 1, "ainbow": 1, "canonical": 1, "r": 1, "agentenvironment": 1, "interactions": 2, "00": 5, "10": 1, "0": 4, "75": 1, "50": 1, "25": 1, "median": 2, "humanrandom": 1, "normalize": 1, "return": 2, "015": 1, "figure": 1, "3": 1, "humannormalised": 1, "episode": 1, "tune": 1, "rainbow": 1, "function": 1, "environment": 1, "framesaction": 1, "repeat": 1}, {"horizontal": 1, "dash": 1, "line": 1, "correspond": 1, "performance": 1, "simple": 1, "kaiser": 1, "et": 1, "al": 1, "2019": 1}, {"error": 1, "bar": 1, "compute": 1, "5": 1, "seed": 1}, {"5": 1, "": 2, "conclusions": 1, "discuss": 1, "commonalities": 1, "differences": 1, "replay": 1, "modelbased": 1, "methods": 1}, {"particular": 1, "discuss": 1, "model": 2, "errors": 1, "may": 1, "cause": 1, "issue": 1, "use": 1, "parametric": 1, "replaylike": 1, "set": 1, "sample": 1, "observe": 1, "state": 1, "past": 1}, {"note": 1, "modelbased": 1, "learn": 1, "unstable": 1, "theory": 1, "hypothesise": 1, "replay": 1, "likely": 1, "better": 1, "strategy": 1, "state": 1, "sample": 1, "distribution": 1}, {"confirm": 1, "atscale": 1, "experiment": 1, "atari": 1, "2600": 1, "video": 1, "game": 1, "replaybased": 1, "agent": 1, "attain": 1, "stateoftheart": 1, "data": 1, "efficiency": 1, "best": 1, "impressive": 1, "modelbased": 1, "result": 1, "kaiser": 1, "et": 1, "al": 1}, {"2019": 1}, {"hypothesise": 1, "parametric": 1, "model": 1, "perhaps": 1, "useful": 1, "use": 1, "either": 1, "1": 1, "plan": 2, "backward": 1, "credit": 1, "assignment": 1, "2": 1, "forward": 1, "behaviour": 1}, {"plan": 1, "forward": 1, "credit": 1, "assignment": 1, "hypothesise": 1, "show": 1, "less": 1, "effective": 1, "even": 1, "though": 1, "approach": 1, "quite": 1, "common": 1}, {"intuitive": 1, "reason": 1, "model": 2, "inaccurate": 2, "plan": 2, "backwards": 1, "learn": 1, "may": 1, "lead": 1, "update": 2, "fictional": 1, "state": 2, "seem": 1, "less": 1, "harmful": 1, "real": 1, "transition": 1, "would": 1, "happen": 1, "forward": 1, "credit": 1, "assignment": 1}, {"forward": 1, "plan": 2, "behaviour": 1, "rather": 1, "credit": 1, "assignment": 1, "deem": 1, "potentially": 1, "useful": 1, "less": 1, "likely": 1, "harmful": 1, "learn": 1, "result": 1, "trust": 1, "real": 1, "experience": 1, "prediction": 1, "policy": 1, "update": 1}, {"empirical": 1, "result": 1, "support": 1, "conclusions": 1}, {"rich": 1, "literature": 1, "modelbased": 1, "reinforcement": 1, "learn": 2, "paper": 1, "cannot": 1, "cover": 1, "potential": 1, "ways": 1, "plan": 1, "model": 1}, {"one": 1, "notable": 1, "topic": 1, "scope": 1, "paper": 1, "consideration": 1, "abstract": 1, "model": 2, "silver": 1, "et": 1, "al": 1, "2017": 1, "alternative": 1, "ways": 1, "use": 1, "addition": 1, "classic": 1, "plan": 1, "cf": 1}, {"weber": 1, "et": 1, "al": 1, "2017": 1}, {"finally": 1, "note": 1, "discussion": 1, "focus": 1, "mostly": 1, "distinction": 1, "parametric": 1, "model": 2, "replay": 1, "common": 1, "good": 1, "acknowledge": 1, "one": 1, "also": 1, "consider": 1, "nonparametric": 1}, {"instance": 1, "one": 1, "could": 1, "apply": 1, "nearestneighbours": 1, "kernel": 1, "approach": 1, "replay": 2, "buffer": 1, "thereby": 1, "obtain": 1, "nonparametric": 1, "model": 1, "equivalent": 1, "sample": 2, "observe": 1, "state": 3, "interpolate": 1, "generalise": 1, "unseen": 1, "pan": 1, "et": 1, "al": 1, "2018": 1}, {"conceptually": 1, "appeal": 1, "alternative": 1, "although": 1, "come": 1, "practical": 1, "algorithmic": 1, "question": 1, "best": 1, "define": 1, "distance": 1, "metrics": 1, "highdimensional": 1, "state": 1, "space": 1}, {"seem": 1, "another": 1, "interest": 1, "potential": 1, "avenue": 1, "future": 1, "work": 1}, {"9": 1, "": 1, "acknowledgments": 1, "author": 1, "benefit": 1, "greatly": 1, "feedback": 1, "tom": 1, "schaul": 1, "adam": 1, "white": 1, "brian": 1, "tanner": 1, "richard": 1, "sutton": 1, "theophane": 1, "weber": 1, "arthur": 1, "guez": 1, "lars": 1, "buesing": 1}, {"reference": 1, "l": 1, "baird": 1}, {"residual": 1, "algorithms": 1, "reinforcement": 1, "learn": 1, "function": 1, "approximation": 1}, {"proceed": 1, "twelfth": 1, "international": 1, "conference": 1, "machine": 1, "learn": 1, "page": 1, "3037": 1, "1995": 1}, {"g": 1, "bellemare": 1, "naddaf": 1, "j": 1, "veness": 1, "bowl": 1}, {"arcade": 1, "learn": 1, "environment": 1, "evaluation": 1, "platform": 1, "general": 1, "agents": 1}, {"j": 1, "artif": 1}, {"intell": 1}, {"res": 1}, {"jair": 1, "47253279": 1, "2013": 1}, {"g": 1, "bellemare": 1, "w": 1, "dabney": 1, "r": 1, "munos": 1}, {"distributional": 1, "perspective": 1, "reinforcement": 1, "learn": 1}, {"proceed": 1, "34th": 1, "international": 1, "conference": 1, "machine": 1, "learn": 1, "page": 1, "449458": 1, "2017": 1}, {"j": 1}, {"boyan": 1}, {"leastsquares": 1, "temporal": 1, "difference": 1, "learn": 1}, {"proc": 1}, {"16th": 1, "international": 1, "conf": 1}, {"machine": 1, "learn": 1, "page": 1, "4956": 1}, {"morgan": 1, "kaufmann": 1, "1999": 1}, {"j": 1, "bradtke": 1, "g": 1, "barto": 1}, {"linear": 1, "leastsquares": 1, "algorithms": 1, "temporal": 1, "difference": 1, "learn": 1}, {"machine": 1, "learn": 1, "223357": 1, "1996": 1}, {"efroni": 1, "g": 1, "dalal": 1, "b": 1, "scherrer": 1, "mannor": 1}, {"multiplestep": 1, "greedy": 1, "policies": 1, "approximate": 1, "online": 1, "reinforcement": 1, "learn": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "page": 1, "52385247": 1, "2018": 1}, {"fortunato": 1, "g": 1, "azar": 1, "b": 1, "piot": 1, "j": 1, "menick": 1, "osband": 1, "grave": 1, "v": 1, "mnih": 1, "r": 1, "munos": 1, "hassabis": 1, "pietquin": 1, "c": 1, "blundell": 1, "legg": 1}, {"noisy": 1, "network": 1, "exploration": 1}, {"corr": 1, "abs170610295": 1, "2017": 1}, {"harutyunyan": 1, "g": 1, "bellemare": 1, "stepleton": 1, "r": 1, "munos": 1}, {"q": 1, "offpolicy": 1, "corrections": 1}, {"proceed": 1, "27th": 1, "international": 1, "conference": 1, "algorithmic": 1, "learn": 1, "theory": 1, "alt2016": 1, "volume": 1, "9925": 1, "lecture": 1, "note": 1, "artificial": 1, "intelligence": 1, "page": 1, "305320": 1}, {"springer": 1, "2016": 1}, {"hessel": 1, "j": 1, "modayil": 1, "h": 1, "van": 1, "hasselt": 1, "schaul": 1, "g": 1, "ostrovski": 1, "w": 1, "dabney": 1, "horgan": 1, "b": 1, "piot": 1, "azar": 1, "silver": 1}, {"rainbow": 1, "combine": 1, "improvements": 1, "deep": 1, "reinforcement": 1, "learn": 1}, {"aaai": 1, "2018a": 1}, {"hessel": 1, "h": 1, "van": 1, "hasselt": 1, "j": 1, "modayil": 1, "silver": 1}, {"inductive": 1, "bias": 1, "deep": 1, "reinforcement": 1, "learn": 1}, {"openreview": 1, "httpsopenreviewnetforumidrjgvf3rcfq": 1, "2018b": 1}, {"g": 1, "z": 1, "holland": 1, "e": 1, "talvitie": 1, "bowl": 1}, {"effect": 1, "plan": 2, "shape": 1, "dynastyle": 1, "highdimensional": 1, "state": 1, "space": 1}, {"corr": 1, "abs180601825": 1, "2018": 1}, {"l": 1, "p": 1, "kaelbling": 1, "lozanoprez": 1}, {"hierarchical": 1, "plan": 1}, {"workshops": 1, "twentyfourth": 1, "aaai": 1, "conference": 1, "artificial": 1, "intelligence": 1, "2010": 1}, {"l": 1, "kaiser": 1, "babaeizadeh": 1, "p": 2, "milos": 1, "b": 1, "osinski": 1, "r": 2, "h": 2, "campbell": 1, "k": 1, "czechowski": 1, "erhan": 1, "c": 1, "finn": 1, "kozakowski": 1, "levine": 1, "sepassi": 1, "g": 1, "tucker": 1, "michalewski": 1}, {"modelbased": 1, "reinforcement": 1, "learn": 1, "atari": 1}, {"arxiv": 1, "preprint": 1, "arxiv150300185": 1, "2019": 1}, {"l": 1, "lin": 1}, {"selfimproving": 1, "reactive": 1, "agents": 1, "base": 1, "reinforcement": 1, "learn": 1, "plan": 1, "teach": 1}, {"machine": 1, "learn": 1, "83293321": 1, "1992": 1}, {"k": 1, "lowrey": 1, "rajeswaran": 1, "kakade": 1, "e": 1, "todorov": 1, "mordatch": 1}, {"plan": 1, "online": 1, "learn": 2, "offline": 1, "efficient": 1, "exploration": 1, "via": 1, "modelbased": 1, "control": 1}, {"international": 1, "conference": 1, "learn": 1, "representations": 1, "2019": 1}, {"q": 1, "mayne": 1}, {"model": 1, "predictive": 1, "control": 1, "recent": 1, "developments": 1, "future": 1, "promise": 1}, {"automatica": 1, "5012": 1, "29672986": 1, "2014": 1}, {"v": 1, "mnih": 1, "k": 1, "kavukcuoglu": 1, "silver": 1, "grave": 1, "antonoglou": 1, "wierstra": 1, "riedmiller": 1}, {"play": 1, "atari": 1, "deep": 1, "reinforcement": 1, "learn": 1}, {"arxiv": 1, "preprint": 1, "arxiv13125602": 1, "2013": 1}, {"v": 1, "mnih": 1, "k": 1, "kavukcuoglu": 1, "silver": 1}, {"rusu": 1, "j": 1, "veness": 1, "g": 2, "bellemare": 1, "grave": 1, "riedmiller": 1, "k": 1, "fidjeland": 1, "ostrovski": 1, "petersen": 1, "c": 1, "beattie": 1, "sadik": 1, "antonoglou": 1, "h": 1, "king": 1, "kumaran": 1, "wierstra": 1, "legg": 1, "hassabis": 1}, {"humanlevel": 1, "control": 1, "deep": 1, "reinforcement": 1, "learn": 1}, {"nature": 1, "5187540": 1, "529533": 1, "2015": 1}, {"morari": 1, "j": 1, "h": 1, "lee": 1}, {"model": 1, "predictive": 1, "control": 1, "past": 1, "present": 1, "future": 1}, {"computers": 1, "": 1, "chemical": 1, "engineer": 1, "2345667682": 1, "1999": 1}, {"10": 1, "": 1, "r": 1, "munos": 1, "stepleton": 1, "harutyunyan": 1, "bellemare": 1}, {"safe": 1, "efficient": 1, "offpolicy": 1, "reinforcement": 1, "learn": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "page": 1, "10541062": 1, "2016": 1}, {"pan": 1, "zaheer": 1}, {"white": 2, "patterson": 1}, {"organize": 1, "experience": 1, "deeper": 1, "look": 1, "replay": 1, "mechanisms": 1, "samplebased": 1, "plan": 1, "continuous": 1, "state": 1, "domains": 1}, {"proceed": 1, "twentyseventh": 1, "international": 1, "joint": 1, "conference": 1, "artificial": 1, "intelligence": 1, "ijcai18": 1, "page": 1, "47944800": 1}, {"international": 1, "joint": 1, "conferences": 1, "artificial": 1, "intelligence": 1, "organization": 1, "7": 1, "2018": 1}, {"r": 1, "parr": 1, "l": 2, "li": 1, "g": 1, "taylor": 1, "c": 1, "painterwakefield": 1, "littman": 1}, {"analysis": 1, "linear": 2, "model": 1, "valuefunction": 1, "approximation": 1, "feature": 1, "selection": 1, "reinforcement": 1, "learn": 1}, {"proceed": 1, "25th": 1, "international": 1, "conference": 1, "machine": 1, "learn": 1, "page": 1, "752759": 1, "2008": 1}, {"precup": 1, "r": 1, "sutton": 1, "p": 1, "singh": 1}, {"eligibility": 1, "trace": 1, "offpolicy": 1, "policy": 1, "evaluation": 1}, {"proceed": 1, "seventeenth": 1, "international": 1, "conference": 1, "machine": 1, "learn": 1, "icml": 1, "2000": 2, "page": 1, "766773": 1, "stanford": 2, "university": 1, "ca": 1, "usa": 1}, {"morgan": 1, "kaufmann": 1}, {"j": 3, "richalet": 1, "rault": 1, "testud": 1, "papon": 1}, {"model": 1, "predictive": 1, "heuristic": 1, "control": 1}, {"automatica": 1, "journal": 1, "ifac": 1, "145413428": 1, "1978": 1}, {"riedmiller": 1}, {"neural": 2, "fit": 1, "q": 1, "iteration": 1, "": 1, "first": 1, "experience": 1, "data": 1, "efficient": 1, "reinforcement": 1, "learn": 1, "method": 1}, {"j": 1, "gama": 1, "r": 1, "camacho": 1, "p": 1, "brazdil": 1, "jorge": 1, "l": 1, "torgo": 1, "editors": 1, "proceed": 1, "16th": 1, "european": 1, "conference": 1, "machine": 1, "learn": 1, "ecml05": 1, "page": 1, "317328": 1}, {"springer": 1, "2005": 1}, {"schaul": 1, "j": 1, "quan": 1, "antonoglou": 1, "silver": 1}, {"prioritize": 1, "experience": 1, "replay": 1}, {"international": 1, "conference": 1, "learn": 1, "representations": 1, "puerto": 1, "rico": 1, "2016": 1}, {"silver": 1, "h": 1, "van": 1, "hasselt": 1, "hessel": 1, "schaul": 1, "guez": 1, "harley": 1, "g": 1, "dulacarnold": 1, "reichert": 1, "n": 1, "rabinowitz": 1, "barreto": 1, "degris": 1}, {"predictron": 1, "endtoend": 1, "learn": 1, "plan": 1}, {"precup": 1, "w": 1, "teh": 1, "editors": 1, "proceed": 2, "34th": 1, "international": 2, "conference": 1, "machine": 2, "learn": 2, "volume": 1, "70": 1, "research": 1, "page": 1, "31913199": 1, "convention": 1, "centre": 1, "sydney": 1, "australia": 1, "0611": 1, "aug": 1, "2017": 1}, {"pmlr": 1}, {"r": 1, "sutton": 1}, {"learn": 1, "predict": 1, "methods": 1, "temporal": 1, "differences": 1}, {"machine": 1, "learn": 1, "31944": 1, "1988": 1}, {"r": 1, "sutton": 1}, {"integrate": 1, "architectures": 1, "learn": 1, "plan": 1, "react": 1, "base": 1, "approximate": 1, "dynamic": 1, "program": 1}, {"proceed": 1, "seventh": 1, "international": 1, "conference": 1, "machine": 1, "learn": 1, "page": 1, "216224": 1, "1990": 1}, {"r": 1, "sutton": 1}, {"virtues": 1, "linear": 1, "learn": 1, "trajectory": 1, "distributions": 1}, {"proceed": 1, "workshop": 1, "value": 1, "function": 1, "approximation": 1, "machine": 1, "learn": 1, "conference": 1, "page": 1, "85": 1, "1995": 1}, {"r": 1, "sutton": 1, "g": 1, "barto": 1}, {"reinforcement": 1, "learn": 1, "introduction": 1}, {"mit": 1, "press": 1, "cambridge": 1, "2018": 1}, {"r": 1, "sutton": 1, "precup": 1, "p": 1, "singh": 1}, {"intraoption": 1, "learn": 1, "temporally": 1, "abstract": 1, "action": 1}, {"proceed": 1, "fifteenth": 1, "international": 1, "conference": 1, "machine": 1, "learn": 1, "icml": 1, "1998": 1, "page": 1, "556564": 1}, {"morgan": 1, "kaufmann": 1, "publishers": 1, "inc": 1, "1998": 1}, {"r": 2, "sutton": 1, "c": 1, "szepesvri": 1, "h": 1, "maei": 1}, {"convergent": 1, "algorithm": 1, "offpolicy": 1, "temporaldifference": 1, "learn": 1, "linear": 1, "function": 1, "approximation": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "21": 1, "nips08": 1, "2116091616": 1, "2008": 1}, {"r": 2, "sutton": 1, "h": 1, "maei": 1, "precup": 1, "bhatnagar": 1, "silver": 1, "c": 1, "szepesvri": 1, "e": 1, "wiewiora": 1}, {"fast": 1, "gradientdescent": 1, "methods": 1, "temporaldifference": 1, "learn": 1, "linear": 1, "function": 1, "approximation": 1}, {"proceed": 1, "26th": 1, "annual": 1, "international": 1, "conference": 1, "machine": 1, "learn": 1, "icml": 1, "2009": 1, "page": 1, "9931000": 1}, {"acm": 1, "2009": 1}, {"r": 2, "sutton": 1, "mahmood": 1, "white": 1}, {"emphatic": 1, "approach": 1, "problem": 1, "offpolicy": 1, "temporaldifference": 1, "learn": 1}, {"journal": 1, "machine": 1, "learn": 1, "research": 1, "1773129": 1, "2016": 1}, {"j": 1, "n": 1, "tsitsiklis": 1, "b": 1}, {"van": 1, "roy": 1}, {"analysis": 1, "temporaldifference": 1, "learn": 1, "function": 1, "approximation": 1}, {"ieee": 1, "transactions": 1, "automatic": 1, "control": 1, "425674690": 1, "1997": 1}, {"h": 1, "van": 1, "hasselt": 1}, {"double": 1, "qlearning": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "2326132621": 1, "2010": 1}, {"h": 1, "van": 1, "hasselt": 1, "r": 1, "sutton": 1}, {"learn": 1, "predict": 1, "independent": 1, "span": 1}, {"corr": 1, "abs150804582": 1, "2015": 1}, {"h": 1, "van": 1, "hasselt": 1, "r": 2, "mahmood": 1, "sutton": 1}, {"offpolicy": 1, "td": 1, "true": 1, "online": 1, "equivalence": 1}, {"uncertainty": 1, "artificial": 1, "intelligence": 1, "2014": 1}, {"h": 1, "van": 1, "hasselt": 1, "guez": 1, "silver": 1}, {"deep": 1, "reinforcement": 1, "learn": 1, "double": 1, "qlearning": 1}, {"aaai": 1, "2016": 1}, {"11": 1, "": 1, "h": 1, "van": 1, "hasselt": 1, "doron": 1, "f": 1, "strub": 1, "hessel": 1, "n": 1, "sonnerat": 1, "j": 1, "modayil": 1}, {"deep": 1, "reinforcement": 1, "learn": 1, "deadly": 1, "triad": 1}, {"corr": 1, "abs181202648": 1, "2018": 1}, {"h": 1, "van": 1, "seijen": 1, "r": 1, "sutton": 1}, {"deeper": 1, "look": 1, "plan": 1, "learn": 1, "replay": 1}, {"international": 1, "conference": 1, "machine": 1, "learn": 1, "page": 1, "23142322": 1, "2015": 1}, {"n": 1, "wagener": 1, "ca": 1}, {"cheng": 1, "j": 1, "sack": 1, "b": 1}, {"boot": 1}, {"online": 1, "learn": 1, "approach": 1, "model": 1, "predictive": 1, "control": 1}, {"arxiv": 1, "preprint": 1, "axxiv190208967": 1, "2019": 1}, {"wan": 1, "zaheer": 1}, {"white": 2, "r": 1, "sutton": 1}, {"plan": 1, "expectation": 1, "model": 1}, {"corr": 1, "abs190401191": 1, "2019": 1}, {"z": 1, "wang": 1, "n": 1, "de": 1, "freitas": 1, "schaul": 1, "hessel": 1, "h": 1, "van": 1, "hasselt": 1, "lanctot": 1}, {"duel": 1, "network": 1, "architectures": 1, "deep": 1, "reinforcement": 1, "learn": 1}, {"international": 1, "conference": 1, "machine": 1, "learn": 1, "new": 1, "york": 1, "ny": 1, "usa": 1, "2016": 1}, {"c": 2, "j": 1, "h": 1, "watkins": 1}, {"learn": 1, "delay": 1, "reward": 1}, {"phd": 1, "thesis": 1, "university": 1, "cambridge": 1, "england": 1, "1989": 1}, {"c": 2, "j": 1, "h": 1, "watkins": 1, "p": 1, "dayan": 1}, {"qlearning": 1}, {"machine": 1, "learn": 1, "8279292": 1, "1992": 1}, {"weber": 1, "racanire": 1, "p": 3, "reichert": 1, "l": 1, "buesing": 1, "guez": 1, "j": 1, "rezende": 1, "badia": 1, "vinyals": 1, "n": 1, "heess": 1, "li": 1, "r": 1, "pascanu": 1, "battaglia": 1, "silver": 1, "wierstra": 1}, {"imaginationaugmented": 1, "agents": 1, "deep": 1, "reinforcement": 1, "learn": 1}, {"corr": 1, "abs170706203": 1, "2017": 1}, {"r": 1, "j": 1, "williams": 1, "l": 1, "c": 1, "baird": 1, "iii": 1}, {"analysis": 1, "incremental": 1, "variants": 1, "policy": 1, "iteration": 1, "first": 1, "step": 1, "toward": 1, "understand": 1, "actorcritic": 1, "learn": 1, "systems": 1}, {"technical": 1, "report": 1, "tech": 1}, {"rep": 1, "nuccs9311": 1, "northeastern": 1, "university": 1, "college": 1, "computer": 1, "science": 1, "boston": 1, "1993": 1}, {"12": 1}]
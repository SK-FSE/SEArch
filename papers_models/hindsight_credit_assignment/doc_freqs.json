[{"hindsight": 1, "credit": 2, "assignment": 2, "": 2, "anna": 1, "harutyunyan": 2, "dabney": 1, "thomas": 1, "mesnard": 1, "nicolas": 1, "heess": 1, "mohammad": 1, "g": 1, "azar": 1, "bilal": 1, "piot": 1, "hado": 1, "van": 1, "hasselt": 1, "satinder": 1, "singh": 1, "greg": 1, "wayne": 1, "doina": 1, "precup": 1, "rmi": 1, "munos": 1, "deepmind": 1, "wdabney": 1, "munosgooglecom": 1, "abstract": 1, "consider": 1, "problem": 1, "efficient": 1, "reinforcement": 1, "learn": 1}, {"order": 1, "efficiently": 1, "meaningfully": 1, "utilize": 1, "new": 1, "data": 1, "propose": 1, "explicitly": 1, "assign": 1, "credit": 1, "past": 1, "decisions": 1, "base": 1, "likelihood": 1, "lead": 1, "observe": 1, "outcome": 1}, {"approach": 1, "use": 1, "new": 1, "information": 1, "hindsight": 1, "rather": 1, "employ": 1, "foresight": 1}, {"somewhat": 1, "surprisingly": 1, "show": 1, "value": 1, "function": 1, "rewrite": 1, "lens": 1, "yield": 1, "new": 1, "family": 1, "algorithms": 1}, {"study": 1, "properties": 1, "algorithms": 1, "empirically": 1, "show": 1, "successfully": 1, "address": 1, "important": 1, "credit": 1, "assignment": 1, "challenge": 1, "set": 1, "illustrative": 1, "task": 1}, {"1": 1, "": 2, "introduction": 1, "reinforcement": 1, "learn": 1, "rl": 1, "agent": 1, "task": 1, "two": 1, "fundamental": 1, "interdependent": 1, "problems": 1, "exploration": 1, "discover": 1, "useful": 1, "data": 1, "credit": 1, "assignment": 1, "incorporate": 1}, {"work": 1, "take": 1, "careful": 1, "look": 1, "problem": 1, "credit": 1, "assignment": 1}, {"instrumental": 1, "learn": 1, "object": 1, "rl": 1, "": 2, "value": 1, "function": 1, "quantify": 1, "follow": 1, "question": 1, "choose": 1, "action": 1, "state": 1, "x": 1, "affect": 1, "future": 1, "return": 1}, {"challenge": 1, "question": 1, "several": 1, "reason": 1}, {"issue": 1, "1": 1, "variance": 1}, {"simplest": 1, "way": 1, "estimate": 1, "value": 1, "function": 1, "average": 1, "return": 1, "future": 1, "discount": 1, "sum": 1, "reward": 1, "start": 1, "take": 1, "x": 1}, {"monte": 1, "carlo": 1, "style": 1, "estimation": 1, "inefficient": 1, "since": 1, "lot": 1, "randomness": 1, "trajectories": 1}, {"issue": 1, "2": 1, "partial": 1, "observability": 1}, {"amortize": 1, "search": 1, "reduce": 1, "variance": 1, "temporal": 1, "difference": 1, "td": 1, "methods": 1, "like": 1, "sarsa": 1, "qlearning": 1, "use": 1, "learn": 1, "approximation": 1, "value": 1, "function": 1, "bootstrap": 1}, {"introduce": 1, "bias": 1, "due": 1, "approximation": 2, "well": 1, "reliance": 1, "markov": 2, "assumption": 1, "especially": 1, "problematic": 1, "agent": 1, "operate": 1, "outside": 1, "decision": 1, "process": 1, "mdp": 1, "example": 1, "state": 1, "partially": 1, "observe": 1, "function": 1}, {"bootstrapping": 1, "may": 1, "cause": 1, "value": 1, "function": 1, "converge": 1, "remain": 1, "permanently": 1, "bias": 1, "19": 1}, {"issue": 1, "3": 1, "time": 1, "proxy": 1}, {"td": 1, "methods": 1, "control": 1, "biasvariance": 1, "tradeoff": 1, "rely": 1, "time": 1, "sole": 1, "metric": 1, "relevance": 1, "recent": 1, "action": 1, "credit": 1, "blame": 1, "receive": 1, "future": 1, "reward": 1, "20": 1, "21": 1}, {"although": 1, "time": 1, "reasonable": 1, "proxy": 1, "causeandeffect": 1, "especially": 1, "mdps": 1, "general": 1, "heuristic": 1, "hence": 1, "improve": 1, "learn": 1}, {"issue": 1, "4": 1, "counterfactuals": 1}, {"data": 1, "use": 2, "estimate": 1, "action": 3, "value": 1, "trajectories": 1, "contain": 1, "ideally": 1, "would": 1, "like": 1, "able": 1, "trajectory": 1, "update": 1, "relevant": 1, "ones": 1, "happen": 1, "serendipitously": 1, "occur": 1}, {"figure": 1, "1": 1, "illustrate": 1, "issue": 1, "concretely": 1}, {"highlevel": 1, "wish": 1, "achieve": 1, "credit": 1, "assignment": 1, "mechanisms": 1, "sampleefficient": 1, "issue": 2, "1": 1, "4": 1, "expressive": 1, "2": 1, "3": 1}, {"end": 1, "propose": 1, "reverse": 1, "key": 1, "learn": 2, "question": 2, "estimators": 1, "measure": 1, "give": 1, "future": 1, "outcome": 1, "reward": 1, "state": 1, "relevant": 1, "choice": 1, "x": 1, "achieve": 1, "essentially": 1, "credit": 1, "assignment": 1}, {"although": 1, "eligibility": 1, "trace": 1, "consider": 1, "33rd": 1, "conference": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "neurips": 1, "2019": 1, "vancouver": 1, "canada": 1}, {"figure": 1, "1": 1, "leave": 1}, {"consider": 1, "trajectory": 2, "show": 1, "solid": 1, "arrows": 1, "sample": 1, "": 2}, {"rl": 1, "algorithm": 1, "typically": 1, "assign": 1, "credit": 1, "reward": 1, "obtain": 1, "state": 1, "action": 1, "along": 1, "": 2}, {"unsatisfying": 1, "two": 1, "reason": 1, "1": 1, "action": 3, "essential": 1, "reach": 1, "state": 1, "z": 3, "a0": 1, "would": 1, "effective": 1, "hence": 1, "overemphasize": 1, "source": 1, "variance": 1, "2": 1, "c": 1, "sample": 1, "lead": 1, "multistep": 1, "trajectory": 1, "b": 1, "transition": 1, "directly": 1, "get": 1, "credit": 1}, {"note": 1, "c": 1, "could": 2, "exploratory": 1, "action": 1, "also": 1, "likely": 2, "accord": 1, "policy": 1, "z": 1, "give": 1, "reach": 1, "b": 1, "right": 1}, {"choice": 1, "action": 1, "b": 1, "state": 1, "x": 1, "cause": 1, "transition": 1, "either": 1, "ya": 1, "yb": 1, "": 1, "perceptually": 1, "aliased": 1}, {"next": 1, "decision": 1, "action": 1, "c": 1, "transition": 1, "agent": 1, "different": 1, "state": 1, "depend": 1, "true": 1, "underlie": 1}, {"state": 2, "single": 1, "could": 1, "trajectory": 1}, {"scenario": 1, "happen": 1, "eg": 1}, {"feature": 1, "learn": 1}, {"td": 1, "algorithm": 1, "bootstrap": 1, "able": 1, "learn": 1, "correct": 1, "value": 1, "b": 1, "since": 1, "average": 1, "reward": 1, "za": 1, "zb": 1, "": 1}, {"potentially": 1, "long": 1, "trajectory": 1, "noisy": 1, "reward": 1, "monte": 1, "carlo": 1, "algorithm": 1, "incorporate": 1, "noise": 1, "along": 1, "value": 1, "b": 1, "despite": 1, "irrelevant": 1, "choice": 1}, {"would": 1, "like": 1, "able": 1, "directly": 1, "determine": 1, "relevance": 1, "za": 1, "": 1}, {"question": 1, "way": 1, "purposefully": 1, "equivalent": 1, "forward": 1, "view": 1, "20": 1, "rely": 1, "mainly": 1, "vanilla": 1, "feature": 1, "like": 1, "time": 1, "decide": 1, "credit": 1, "assignment": 1}, {"reason": 1, "backward": 1, "view": 1, "explicitly": 1, "open": 1, "new": 1, "family": 1, "algorithms": 1}, {"specifically": 1, "propose": 1, "use": 1, "form": 1, "hindsight": 1, "condition": 1, "determine": 1, "relevance": 1, "past": 1, "action": 1, "particular": 1, "outcome": 1}, {"show": 1, "usual": 1, "value": 1, "function": 1, "rewrite": 1, "hindsight": 1, "yield": 1, "new": 1, "family": 1, "estimators": 2, "derive": 1, "policy": 1, "gradient": 1, "algorithms": 1, "use": 1}, {"demonstrate": 1, "empirically": 1, "ability": 1, "algorithms": 1, "address": 1, "highlight": 1, "issue": 1, "set": 1, "diagnostic": 1, "task": 1, "handle": 1, "well": 1, "mean": 1}, {"2": 1, "": 16, "background": 1, "notation": 1, "markov": 1, "decision": 1, "process": 1, "mdp": 1, "14": 1, "tuple": 1, "x": 6, "p": 2, "r": 3, "state": 2, "space": 2, "action": 2, "0": 2, "1": 2, "statetransition": 1, "distribution": 1, "pyx": 1, "denote": 1, "probability": 1, "transition": 1, "choose": 1, "reward": 1, "function": 1, "scalar": 1, "discount": 1, "factor": 1}, {"stochastic": 1, "policy": 1, "": 1, "map": 1, "state": 2, "distribution": 1, "action": 2, "ax": 1, "denote": 1, "probability": 1, "choose": 1, "x": 1}, {"let": 1, "x": 4, "": 11, "distributions": 1, "trajectories": 1, "xk": 1, "ak": 1, "rk": 1, "kn": 1, "generate": 1, "policy": 1, "def": 1, "p": 1, "give": 1, "x0": 2, "a0": 1, "respectively": 1}, {"let": 1, "z": 1, "": 5, "k0": 1, "k": 1, "rk": 1, "return": 1, "obtain": 1, "along": 1, "trajectory": 1}, {"value": 1, "v": 3, "function": 2, "": 13, "actionvalue": 1, "q": 3, "denote": 1, "expect": 1, "return": 1, "policy": 1, "give": 1, "x0": 2, "x": 5, "a0": 1, "respectively": 1, "h": 2, "def": 2, "e": 2, "z": 2, "xa": 1}, {"1": 1, "benefit": 1, "choose": 1, "give": 1, "action": 1, "usual": 1, "policy": 1, "": 4, "measure": 1, "advantage": 1, "function": 1, "def": 1, "x": 3, "q": 1, "v": 1}, {"policy": 2, "gradient": 2, "algorithms": 1, "improve": 1, "change": 1, "": 1, "direction": 1, "value": 1, "function": 1, "22": 1}, {"gradient": 1, "initial": 1, "state": 1, "x0": 4, "hxx": 1, "x": 3, "v": 1, "": 16, "xx0": 2, "q": 1, "aax": 1, "e": 1, "k": 3, "xk": 2, "aaxk": 1, "k0": 1, "xa": 1, "def": 1, "p": 1, "unnormalized": 1, "discount": 1, "statevisitation": 1, "distribution": 1}, {"practical": 1, "algorithms": 1, "reinforce": 1, "25": 1, "approximate": 2, "q": 1, "nstep": 1, "truncate": 1, "return": 1, "possibly": 1, "combine": 1, "bootstrapped": 1, "value": 1, "function": 1, "v": 3, "": 18, "also": 1, "often": 1, "use": 1, "baseline": 1, "see": 1, "22": 1, "12": 1, "along": 1, "trajectory": 1, "xk": 1, "ak": 1, "rk": 2, "k": 2, "x": 4, "p": 1, "n1": 1, "n": 1, "xn": 1}, {"k0": 1, "": 4, "2": 1, "3": 1, "condition": 1, "future": 2, "classical": 1, "value": 1, "function": 1, "attempt": 1, "answer": 1, "question": 1, "current": 1, "action": 1, "affect": 1, "outcomes": 1}, {"rely": 1, "predictions": 1, "future": 1, "outcomes": 1, "exist": 1, "approach": 1, "often": 1, "exacerbate": 1, "problems": 1, "around": 1, "variance": 1, "issue": 2, "1": 1, "partial": 1, "observability": 1, "2": 1}, {"furthermore": 1, "methods": 1, "tend": 1, "use": 1, "temporal": 1, "distance": 1, "proxy": 1, "relevance": 1, "issue": 2, "3": 1, "unable": 1, "assign": 1, "credit": 1, "counterfactually": 1, "4": 1}, {"propose": 1, "learn": 1, "estimators": 1, "explicitly": 1, "consider": 1, "credit": 1, "assignment": 1, "question": 1, "give": 1, "outcome": 1, "relevant": 1, "past": 1, "decisions": 1}, {"approach": 1, "fact": 1, "link": 1, "classical": 1, "methods": 1, "statistical": 1, "estimation": 1}, {"particular": 1, "monte": 1, "carlo": 1, "simulation": 1, "know": 1, "inaccurate": 1, "rare": 1, "events": 1, "interest": 1, "average": 1, "require": 1, "infeasible": 1, "number": 1, "sample": 1, "obtain": 1, "accurate": 1, "estimate": 1, "16": 1}, {"one": 1, "solution": 1, "change": 1, "measure": 1, "use": 1, "another": 1, "distribution": 1, "events": 1, "less": 1, "rare": 1, "correct": 1, "importance": 1, "sample": 1}, {"girsanov": 1, "theorem": 1, "wellknown": 1, "example": 1, "process": 1, "brownian": 1, "dynamics": 1, "4": 1, "know": 1, "produce": 1, "lower": 1, "variance": 1, "estimate": 1}, {"scenario": 1, "rare": 1, "random": 1, "events": 1, "particularly": 1, "relevant": 1, "efficient": 1, "credit": 1, "assignment": 1, "rl": 1}, {"new": 1, "significant": 1, "outcome": 1, "experience": 1, "agent": 1, "ought": 1, "quickly": 1, "update": 1, "estimate": 1, "policy": 1, "accordingly": 1}, {"let": 1, "": 3, "x": 1, "sample": 1, "trajectory": 1, "f": 1, "function": 1}, {"change": 1, "measure": 1, "policy": 1, "": 4, "sample": 1, "futureconditional": 1, "hindsight": 1, "distribution": 1, "hx": 1, "f": 1, "hope": 1, "improve": 1, "efficiency": 1, "credit": 1, "assignment": 1}, {"importance": 1, "": 4, "sample": 1, "ratio": 1, "haxf": 1, "precisely": 1, "denote": 1, "relevance": 1, "action": 1, "specific": 1, "future": 1, "ax": 1, "f": 1}, {"distribution": 1, "hax": 1, "": 5, "f": 2, "accurate": 1, "allow": 1, "us": 1, "quickly": 1, "assign": 1, "credit": 1, "action": 1, "relevant": 1, "achieve": 1}, {"work": 1, "consider": 1, "f": 1, "future": 2, "state": 1, "return": 1}, {"highlight": 1, "use": 1, "futureconditional": 1, "distribution": 1, "refer": 1, "result": 1, "family": 1, "methods": 1, "hindsight": 1, "credit": 1, "assignment": 1, "hca": 1}, {"remainder": 1, "section": 1, "formalize": 1, "insight": 1, "outline": 1, "derive": 1, "usual": 1, "value": 1, "function": 1, "policy": 1, "gradients": 1, "hindsight": 1, "next": 1, "one": 1, "present": 1, "new": 1, "algorithms": 1, "base": 1, "sample": 1, "expressions": 1}, {"31": 1, "": 3, "condition": 1, "future": 2, "state": 2, "agent": 1, "compose": 1, "estimate": 1, "return": 1, "action": 1, "sum": 1, "reward": 1, "obtain": 1, "xk": 1}, {"one": 1, "option": 1, "hindsight": 1, "condition": 1, "consider": 1, "step": 1, "likelihood": 1, "action": 1, "give": 1, "future": 1, "state": 1, "xk": 1, "reach": 1}, {"definition": 1, "1": 1, "stateconditional": 1, "hindsight": 1, "distributions": 1}, {"action": 2, "state": 2, "define": 1, "hk": 2, "ax": 2, "": 12, "conditional": 1, "probability": 1, "trajectories": 1, "x": 2, "first": 1, "a0": 2, "trajectory": 2, "equal": 1, "give": 1, "occur": 1, "step": 1, "k": 1, "along": 1, "def": 1, "p": 1, "axk": 1}, {"2": 1, "": 3, "intuitively": 1, "hk": 1, "ax": 1, "quantify": 1, "relevance": 1, "action": 1, "future": 1, "state": 1, "xk": 1}, {"relevant": 2, "reach": 1, "xk": 2, "": 2, "probability": 1, "simply": 1, "policy": 1, "ax": 1, "information": 1}, {"instrumental": 1, "reach": 2, "xk": 2, "": 6, "hk": 2, "ax": 4, "vice": 1, "versa": 1, "detract": 1}, {"general": 1, "hk": 1, "lowerentropy": 1, "distribution": 1, "": 1}, {"relationship": 1, "hk": 2, "familiar": 1, "quantities": 1, "understand": 1, "follow": 1, "identity": 1, "obtain": 1, "application": 1, "bay": 1, "rule": 1, "p": 1, "xa": 1, "xk": 1, "": 9, "ax": 1, "pxk": 1, "yx0": 1, "x": 1, "a0": 1}, {"ax": 1, "pxk": 1, "": 5, "yx0": 1, "x": 2, "p": 1, "xk": 1, "use": 1, "identity": 1, "importance": 1, "sample": 1, "rewrite": 1, "usual": 1, "qfunction": 1, "term": 1, "hk": 1}, {"since": 1, "one": 1, "policy": 1, "": 1, "involve": 1, "drop": 1, "explicit": 1, "condition": 1, "imply": 1}, {"theorem": 1, "1": 1}, {"consider": 1, "action": 1, "state": 1, "x": 1, "ax": 1, "": 2, "0": 1}, {"follow": 1, "hold": 1, "h": 2, "x": 4, "ax": 1, "": 4, "k": 3, "q": 1, "rx": 1, "e": 1, "rk": 1}, {"ax": 1, "k1": 1, "": 4, "axxk": 1, "reward": 1, "rk": 1, "along": 1, "way": 1, "weight": 1, "ratio": 1, "hkax": 1, "exactly": 1, "quantify": 1, "relevant": 1, "achieve": 1, "correspond": 1, "state": 1, "xk": 1}, {"follow": 1, "discussion": 1, "": 2, "3": 1, "ratio": 1, "1": 2, "irrelevant": 1, "larger": 1, "smaller": 1, "case": 1}, {"expression": 1, "qfunction": 1, "similar": 1, "eq": 1}, {"1": 1, "new": 1, "expectation": 1, "longer": 1, "condition": 1, "initial": 1, "action": 1, "": 4, "policy": 1, "follow": 1, "start": 1, "a0": 2, "x": 1, "instead": 1}, {"important": 1, "point": 1, "allow": 1, "us": 1, "use": 1, "return": 1, "generate": 1, "action": 2, "a0": 1, "update": 1, "value": 1, "axxk": 1, "": 2, "extent": 1, "relevant": 1, "accord": 1, "hkax": 1}, {"theorem": 1, "1": 2, "imply": 1, "follow": 1, "expression": 1, "advantage": 1, "h": 2, "x": 6, "": 10, "ax": 2, "k": 3, "rx": 1, "r": 2, "e": 1, "rk": 1, "3": 1, "k1": 1, "p": 1, "aa": 1, "axrx": 1}, {"form": 1, "advantage": 1, "particularly": 1, "appeal": 1, "since": 1, "axxk": 1, "": 2, "1": 1, "reward": 2, "directly": 1, "remove": 1, "irrelevant": 1, "consideration": 1}, {"indeed": 1, "whenever": 1, "hkax": 1, "rk": 1, "participate": 1, "advantage": 1, "value": 1, "action": 1}, {"inconsequential": 1, "noise": 1, "outside": 1, "agents": 1, "control": 1, "may": 1, "greatly": 1, "reduce": 1, "variance": 1, "estimate": 1}, {"remove": 1, "time": 1, "dependence": 1}, {"clarity": 1, "exposition": 1, "consider": 1, "hindsight": 1, "distribution": 1, "additionally": 1, "condition": 1, "time": 1}, {"indeed": 1, "hk": 1, "depend": 1, "reach": 1, "state": 1, "also": 1, "number": 1, "timesteps": 1, "k": 1, "take": 1}, {"general": 1, "limit": 1, "introduce": 1, "stronger": 1, "dependence": 1, "particular": 1, "trajectory": 1, "harder": 1, "estimation": 1, "problem": 1, "hindsight": 1, "distribution": 1}, {"turn": 1, "generalize": 1, "result": 1, "present": 1, "timeindependent": 1, "distribution": 1, "h": 1, "ax": 1, "give": 1, "probability": 1, "condition": 1, "reach": 1, "point": 1, "future": 1}, {"scalar": 1, "": 2, "0": 1, "1": 1, "probability": 1, "survival": 1, "step": 1}, {"either": 1, "discount": 1, "": 1, "termination": 1, "probability": 1, "problem": 1, "undiscounted": 1}, {"discount": 1, "reward": 1, "case": 1, "eq": 1}, {"3": 1, "write": 1, "term": 1, "h": 3, "follow": 1, "x": 5, "": 14, "ax": 2, "k": 2, "rx": 1, "r": 1, "e": 1, "1": 1, "rk": 1, "4": 1, "k1": 1, "choice": 1}, {"interest": 1, "reader": 1, "may": 1, "find": 1, "relevant": 1, "proof": 1, "appendix": 1}, {"finally": 1, "possible": 1, "obtain": 1, "hindsight": 1, "vfunction": 1, "analogously": 1, "qfunction": 1, "theorem": 1, "1": 1}, {"next": 1, "section": 1, "returnconditional": 1, "hca": 1}, {"include": 1, "variations": 1, "appendix": 1}, {"32": 1, "": 2, "condition": 1, "future": 1, "return": 1, "previous": 1, "section": 1, "derive": 1, "qfunctions": 1, "explicitly": 1, "reweigh": 1, "reward": 1, "step": 1, "base": 1, "correspond": 1, "state": 1, "connection": 1, "action": 1, "whose": 1, "value": 1, "wish": 1, "estimate": 1}, {"since": 1, "ultimately": 1, "interest": 1, "return": 1, "could": 1, "alternatively": 1, "use": 1, "future": 1, "condition": 1}, {"definition": 1, "2": 1, "returnconditional": 1, "hindsight": 1, "distributions": 1}, {"action": 2, "possible": 1, "return": 1, "z": 5, "define": 1, "hz": 2, "ax": 2, "": 13, "conditional": 1, "probability": 1, "trajectories": 1, "x": 2, "first": 1, "a0": 2, "give": 1, "observe": 1, "along": 1, "def": 1, "p": 1, "az": 1}, {"distribution": 1, "hz": 1, "ax": 1, "": 2, "z": 2, "intuitively": 1, "similar": 1, "hk": 1, "instead": 1, "future": 1, "state": 1, "directly": 1, "quantify": 1, "relevance": 1, "obtain": 1, "entire": 1, "return": 1}, {"appeal": 1, "since": 1, "end": 1, "care": 1, "return": 1}, {"could": 1, "simpler": 1, "learn": 1, "since": 1, "instead": 1, "possibly": 1, "highdimensional": 1, "state": 1, "need": 1, "worry": 1, "scalar": 1, "outcome": 1}, {"hand": 1, "longer": 1, "jumpy": 1, "time": 1, "may": 1, "benefit": 1, "less": 1, "structure": 1, "dynamics": 1}, {"hk": 1, "": 2, "drop": 1, "explicit": 1, "condition": 1, "imply": 1}, {"follow": 1, "result": 1}, {"theorem": 1, "2": 1}, {"consider": 1, "action": 1, "assume": 1, "possible": 1, "random": 1, "return": 1, "z": 3, "": 6, "trajectory": 1, "x": 1, "hz": 1, "ax": 1, "0": 1}, {"h": 1, "ax": 1, "v": 1, "": 4, "x": 1, "e": 1, "xa": 1, "z": 1}, {"5": 1, "hz": 1, "ax": 1, "z": 1, "": 1, "v": 1, "rather": 1, "q": 1, "function": 1, "form": 1, "interest": 1, "properties": 1, "discuss": 1, "next": 1, "section": 1}, {"mathematically": 1, "two": 1, "form": 1, "analogous": 1, "derive": 1, "ratio": 1, "flip": 1}, {"equations": 1, "5": 1, "1": 1, "imply": 1, "follow": 1, "expression": 1, "advantage": 1, "4": 1, "": 6, "x": 1, "e": 1, "xa": 1, "ax": 1, "z": 1}, {"hz": 1, "ax": 1, "z": 3, "": 6, "h": 1, "1": 2, "6": 1, "factor": 1, "cax": 1, "hzax": 1, "axz": 1, "express": 1, "much": 1, "single": 1, "action": 1, "contribute": 1, "obtain": 1, "return": 1}, {"action": 1, "draw": 1, "x": 1, "would": 1, "yield": 1, "return": 1, "cax": 1, "z": 1, "": 1, "0": 2, "advantage": 1}, {"action": 2, "make": 1, "achieve": 2, "z": 4, "likely": 1, "cax": 2, "": 2, "0": 2, "conversly": 1, "would": 1, "contribute": 1}, {"hence": 1, "cax": 1, "z": 1, "express": 1, "impact": 1, "action": 1, "environment": 2, "term": 1, "return": 1, "everything": 1, "else": 1, "future": 1, "decisions": 1, "well": 1, "randomness": 1, "unchanged": 1}, {"h": 1, "hz": 1, "learn": 1, "online": 1, "sample": 1, "trajectories": 1, "see": 1, "sec": 1}, {"4": 1, "algorithms": 1, "discussion": 1, "sec": 1}, {"41": 1}, {"finally": 1, "choose": 1, "focus": 1, "state": 1, "return": 1, "condition": 1, "one": 1, "could": 1, "consider": 1, "options": 1}, {"example": 1, "condition": 1, "reward": 1, "instead": 1, "state": 1, "future": 2, "time": 1, "k": 1, "embed": 1, "part": 1, "trajectory": 1, "could": 1, "interest": 1, "properties": 1}, {"33": 1, "": 2, "policy": 2, "gradients": 1, "give": 1, "gradient": 1, "theorem": 1, "base": 1, "new": 1, "expressions": 1, "value": 1, "function": 1}, {"theorem": 1, "3": 1}, {"let": 1, "": 5, "policy": 1, "parameterized": 1}, {"gradient": 1, "value": 1, "state": 1, "x0": 4, "hx": 2, "x": 2, "": 35, "v": 1, "e": 2, "k": 2, "axk": 3, "qx": 1, "xk": 3, "7": 1, "k0": 2, "log": 1, "ak": 2, "az": 2, "8": 1, "h": 1, "xt": 1, "rt": 1, "tk1": 1, "ax": 1, "def": 1, "1": 1, "zk": 1}, {"hz": 1, "ax": 1, "zk": 1, "": 10, "def": 1, "qx": 1, "xk": 1, "rxk": 1, "x": 1, "tk": 1, "note": 1, "expression": 1, "state": 1, "hca": 1, "eq": 1}, {"7": 1, "write": 1, "action": 1, "rather": 1, "sample": 1, "one": 1}, {"interestingly": 1, "form": 1, "require": 1, "benefit": 1, "baseline": 1}, {"contrary": 1, "usual": 1, "allactions": 1, "algorithm": 1, "must": 1, "use": 2, "critic": 1, "hca": 1, "reweighting": 1, "allow": 1, "us": 1, "return": 1, "sample": 1, "particular": 1, "start": 1, "action": 2, "obtain": 1, "value": 1, "estimate": 1}, {"4": 1, "": 2, "algorithms": 2, "use": 1, "new": 1, "policy": 1, "gradient": 1, "theorem": 1, "3": 1, "give": 1, "novel": 1, "base": 1, "sample": 1, "expectations": 1, "7": 1, "8": 1}, {"discuss": 1, "train": 1, "relevant": 1, "hindsight": 1, "distributions": 1}, {"stateconditional": 1, "hca": 1, "consider": 1, "parametric": 1, "representation": 1, "policy": 1, "x": 1, "futurestateconditional": 1, "distribution": 1, "h": 1, "ax": 1, "well": 1, "baseline": 1, "v": 1, "estimate": 1, "immediate": 1, "reward": 1, "r": 1}, {"generate": 1, "step": 1, "trajectories": 1, "": 5, "xs": 1, "rs": 1, "0st": 1}, {"compose": 1, "estimate": 1, "return": 1, "action": 1, "see": 1, "theorem": 1, "7": 1, "appendix": 1, "qx": 1, "xs": 1, "": 15, "rxs": 1, "1": 1, "x": 1, "ts1": 1, "ts": 1, "h": 2, "ax": 2, "xt": 3, "rt": 1, "v": 1}, {"ax": 3, "": 17, "pt": 1, "1": 1, "ts": 2, "algorithm": 1, "proceed": 1, "train": 1, "v": 2, "xs": 3, "predict": 3, "usual": 1, "return": 1, "zs": 1, "rt": 1, "xt": 1, "rxs": 1, "rs": 1, "square": 1, "loss": 2, "hindsight": 1, "distribution": 1, "h": 1, "x": 1, "p": 1, "cross": 1, "entropy": 1, "finally": 1, "update": 1, "policy": 1, "logits": 1, "qx": 1, "aa": 1}, {"see": 1, "algorithm": 1, "1": 1, "appendix": 1, "detail": 1, "pseudocode": 1}, {"returnconditional": 1, "hca": 1, "consider": 1, "parametric": 1, "representation": 1, "policy": 1, "x": 1, "returnconditioned": 1, "distribution": 1, "hz": 1, "ax": 1, "z": 1}, {"generate": 1, "full": 1, "trajectories": 1, "": 19, "xs": 4, "rs": 1, "sn": 1, "compute": 1, "5": 1, "sample": 1, "advantage": 1, "step": 1, "az": 1, "1": 1, "zs": 3, "hz": 1, "p": 1, "ts": 1, "rt": 1}, {"algorithm": 1, "proceed": 1, "train": 1, "hindsight": 1, "distribution": 1, "ts": 1, "": 7, "hz": 1, "ax": 1, "zs": 1, "predict": 1, "cross": 1, "entropy": 1, "loss": 1, "update": 1, "policy": 1, "gradient": 1, "log": 1, "xs": 2, "az": 1}, {"see": 1, "algorithm": 1, "2": 1, "appendix": 1, "detail": 1, "pseudocode": 1}, {"rl": 1, "without": 1, "value": 1, "function": 1}, {"returnconditional": 1, "version": 1, "lend": 1, "particularly": 1, "simple": 1, "algorithm": 1}, {"particular": 1, "longer": 1, "need": 1, "learn": 1, "value": 1, "function": 1, "v": 1, "": 3, "hz": 1, "ax": 1, "zs": 1, "estimate": 1, "well": 1, "use": 1, "complete": 1, "rollouts": 1, "feasible": 1, "without": 1, "variance": 1, "issue": 1}, {"take": 1, "idea": 1, "reverse": 1, "direction": 1, "learn": 1, "question": 1, "extreme": 1, "entirely": 1, "hindsight": 1}, {"def": 1, "": 6, "result": 1, "actorcritic": 1, "algorithm": 1, "usual": 1, "baseline": 1, "v": 1, "xs": 3, "replace": 1, "bs": 1, "hz": 1, "zs": 2}, {"baseline": 1, "strongly": 1, "correlate": 1, "return": 1, "zs": 1, "proportional": 1, "desirable": 1, "since": 1, "would": 1, "like": 1, "remove": 1, "much": 1, "variance": 1, "due": 1, "dynamics": 1, "world": 1, "agents": 1, "policy": 1, "possible": 1}, {"follow": 1, "proposition": 1, "verify": 1, "despite": 1, "correlate": 1, "baseline": 1, "introduce": 1, "bias": 1, "policy": 1, "gradient": 1}, {"xs": 1, "": 1, "proposition": 1, "1": 1}, {"baseline": 1, "bs": 2, "": 11, "hza": 1, "xs": 2, "zs": 3, "introduce": 1, "bias": 1, "policy": 1, "gradient": 1, "hx": 1, "e": 1, "x0": 2, "log": 1, "v": 1}, {"": 3, "41": 1, "learn": 1, "hindsight": 2, "distributions": 2, "give": 1, "equivalent": 1, "rewrite": 1, "usual": 1, "value": 1, "function": 1, "term": 1, "propose": 1, "motivate": 1, "properties": 1, "accurate": 1}, {"question": 1, "feasible": 1, "learn": 2, "good": 1, "estimate": 1, "distributions": 1, "experience": 1, "whether": 1, "shift": 1, "problem": 1, "way": 1, "beneficial": 1}, {"remainder": 1, "section": 1, "discuss": 1, "question": 1, "next": 1, "one": 1, "provide": 1, "empirical": 1, "evidence": 1, "affirmative": 1}, {"several": 1, "conventional": 1, "object": 1, "could": 1, "learn": 1, "help": 1, "credit": 1, "assignment": 1, "value": 1, "function": 1, "forward": 1, "model": 2, "inverse": 1, "state": 1}, {"accurate": 2, "forward": 1, "model": 2, "allow": 1, "one": 1, "compute": 1, "value": 1, "function": 1, "directly": 1, "variance": 1, "inverse": 1, "": 1, "perform": 1, "precise": 1, "credit": 1, "assignment": 1}, {"however": 1, "learn": 1, "generative": 1, "model": 1, "accurately": 1, "difficult": 1, "longstanding": 1, "challenge": 1, "rl": 1, "especially": 1, "highdimensional": 1, "state": 1, "space": 1}, {"interestingly": 1, "hindsight": 1, "distribution": 2, "discriminative": 1, "rather": 1, "generative": 1, "model": 2, "hence": 1, "require": 1, "full": 1, "state": 1}, {"additionally": 1, "action": 2, "space": 2, "usually": 1, "much": 2, "smaller": 1, "state": 1, "shift": 1, "focus": 1, "potentially": 1, "make": 1, "problem": 1, "easier": 1}, {"certain": 1, "structure": 1, "dynamics": 1, "present": 1, "learn": 1, "hindsight": 1, "distributions": 1, "may": 1, "significantly": 1, "easier": 1, "still": 1, "": 1, "eg": 1}, {"transition": 1, "model": 1, "stochastic": 1, "policy": 1, "change": 1, "particular": 2, "x": 1, "lead": 1, "many": 1, "possible": 1, "future": 2, "state": 2, "explain": 1, "small": 1, "number": 1, "past": 1, "action": 1}, {"general": 1, "learn": 3, "hz": 1, "h": 1, "supervise": 2, "problems": 1, "new": 1, "algorithms": 1, "delegate": 1, "difficulty": 1, "rl": 1, "set": 1, "many": 1, "efficient": 1, "approach": 1, "exist": 1, "eg": 1}, {"7": 1, "23": 1}, {"5": 1, "": 2, "experiment": 1, "empirically": 1, "validate": 1, "proposal": 1, "control": 1, "way": 1, "devise": 1, "set": 1, "diagnostic": 1, "task": 1, "highlight": 1, "issue": 1, "14": 1, "also": 1, "representative": 1, "occur": 1, "practice": 1, "fig": 1}, {"2": 1}, {"systematically": 1, "verify": 1, "intuitions": 1, "develop": 1, "throughout": 1, "paper": 1}, {"case": 1, "learn": 1, "hindsight": 1, "distributions": 1, "tandem": 1, "control": 1, "policy": 1}, {"problem": 1, "compare": 1, "hca": 1, "state": 1, "return": 1, "condition": 1, "standard": 1, "baseline": 1, "policy": 1, "gradient": 1, "nstep": 1, "advantage": 1, "actor": 1, "critic": 1, "n": 1, "": 2, "monte": 1, "carlo": 1}, {"result": 1, "average": 1, "100": 1, "independent": 1, "run": 1, "plot": 1, "depict": 1, "mean": 1, "standard": 1, "deviations": 1}, {"simplicity": 1, "take": 1, "": 2, "1": 1, "task": 1}, {"shortcut": 1}, {"begin": 1, "example": 1, "capture": 1, "intuition": 1, "fig": 1}, {"1": 1, "leave": 1}, {"fig": 1}, {"2": 1, "leave": 1, "depict": 1, "chain": 1, "length": 1, "n": 1, "reward": 1, "final": 1, "state": 1}, {"step": 1, "one": 1, "action": 1, "take": 1, "shortcut": 2, "directly": 1, "6": 1, "": 1, "figure": 1, "2": 1, "leave": 1}, {"state": 2, "two": 1, "action": 1, "one": 1, "transition": 1, "directly": 1, "goal": 1, "next": 1, "chain": 1}, {"center": 1, "delay": 1, "effect": 1}, {"start": 1, "state": 2, "present": 1, "choice": 2, "two": 1, "action": 1, "follow": 1, "aliased": 1, "chain": 1, "consequence": 1, "initial": 1, "apparent": 1, "final": 1}, {"right": 1, "ambiguous": 1, "bandit": 1}, {"action": 2, "transition": 1, "particular": 1, "state": 2, "high": 1, "probability": 2, "low": 1}, {"two": 1, "state": 1, "noisy": 1, "reward": 1, "credit": 1, "assignment": 1, "action": 1, "become": 1, "challenge": 1}, {"figure": 1, "3": 1, "shortcut": 1}, {"leave": 1, "learn": 1, "curve": 1, "n": 1, "": 1, "5": 1, "policy": 1, "long": 1, "short": 1, "paths": 1, "initialize": 1, "uniformly": 1}, {"explicitly": 1, "consider": 1, "likelihood": 1, "reach": 1, "final": 1, "state": 1, "allow": 1, "stateconditioned": 1, "hca": 1, "quickly": 1, "adjust": 1, "policy": 1}, {"right": 1, "advantage": 1, "shortcut": 1, "action": 1, "estimate": 1, "perform": 1, "1000": 1, "rollouts": 1, "fix": 1, "policy": 1}, {"xaxis": 1, "depict": 1, "policy": 1, "probabilities": 1, "action": 1, "long": 1, "path": 1}, {"oracle": 1, "compute": 1, "analytically": 1, "without": 1, "sample": 1}, {"shortcut": 1, "action": 1, "unlikely": 1, "rarely": 1, "encounter": 1, "difficult": 1, "obtain": 1, "accurate": 1, "estimate": 1, "advantage": 1}, {"hca": 1, "consistently": 1, "able": 1, "maintain": 1, "larger": 1, "accurate": 1, "advantage": 1}, {"figure": 1, "4": 1, "delay": 1, "effect": 1}, {"leave": 1, "bootstrapping": 1}, {"learn": 1, "curve": 1, "n": 1, "": 3, "5": 1, "0": 1, "3step": 1, "return": 1, "cause": 1, "agent": 1, "bootstrap": 1, "partially": 1, "observe": 1, "region": 1}, {"expect": 1, "naive": 1, "bootstrapping": 1, "unable": 1, "learn": 1, "good": 1, "estimate": 1}, {"middle": 1, "use": 1, "full": 1, "monte": 1, "carlo": 1, "return": 1, "n": 1, "": 1, "3": 1, "overcome": 1, "partial": 1, "observability": 1, "prone": 1, "noise": 1}, {"plot": 1, "depict": 1, "learn": 1, "curve": 1, "set": 1, "add": 1, "white": 1, "noise": 1, "": 2, "2": 1}, {"right": 1}, {"average": 1, "performance": 1, "wrt": 1}, {"different": 1, "noise": 1, "level": 1, "": 1, "predictably": 1, "state": 1, "hca": 1, "robust": 1}, {"20": 5, "": 38, "21": 1, "19": 3, "18": 3, "value": 4, "17": 3, "16": 3, "15": 4, "14": 3, "average": 1, "0": 2, "40": 2, "60": 2, "episodes": 2, "80": 2, "100": 2, "hca": 2, "state": 1, "return": 1, "policy": 1, "gradient": 1, "optimal": 1, "00": 1, "01": 1, "02": 1, "epsilon": 1, "03": 1, "04": 1, "figure": 1, "5": 1, "ambiguous": 1, "bandit": 1, "gaussian": 1, "reward": 1, "mean": 1, "1": 1, "2": 1, "standard": 1, "deviation": 1}, {"leave": 1, "state": 1, "identity": 1, "observe": 1}, {"hca": 1, "methods": 1, "improve": 1, "pg": 1}, {"middle": 1, "state": 2, "identity": 1, "hide": 1, "handicap": 1, "hca": 2, "return": 1, "continue": 1, "improve": 1, "pg": 1}, {"right": 1, "average": 1, "performance": 1, "wrt": 1}, {"different": 1, "gaussian": 1, "reward": 1, "mean": 1, "1": 1, "2": 1, "standard": 1, "deviation": 1, "05": 1}, {"note": 1, "optimal": 1, "value": 1, "decay": 1, "case": 1}, {"7": 1, "": 1, "transition": 1, "final": 1, "state": 1, "continue": 1, "longer": 1, "path": 1, "may": 1, "likely": 1, "accord": 1, "policy": 1}, {"perstep": 1, "penalty": 1, "1": 2, "final": 1, "reward": 1}, {"also": 1, "chance": 1, "01": 1, "agent": 1, "transition": 1, "absorb": 1, "state": 1, "directly": 1}, {"problem": 1, "highlight": 1, "two": 1, "issue": 2, "1": 1, "importance": 1, "counterfactual": 2, "credit": 1, "assignment": 1, "4": 1, "long": 1, "path": 2, "take": 1, "frequently": 1, "shortcut": 1, "update": 1, "become": 1, "increasingly": 1, "effective": 1, "see": 1, "fig": 1}, {"3": 2, "right": 1, "2": 1, "use": 1, "time": 1, "proxy": 1, "relevance": 1, "issue": 1, "show": 1, "heuristic": 1, "even": 1, "fullyobservable": 1, "mdp": 1}, {"relevance": 1, "state": 2, "along": 1, "chain": 1, "accurately": 1, "reflect": 1, "long": 1, "temporal": 1, "distance": 1, "goal": 1}, {"fig": 1}, {"3": 1, "show": 1, "hca": 1, "effective": 1, "quickly": 1, "adjust": 1, "policy": 1, "towards": 1, "shortcut": 1, "action": 1}, {"delay": 1, "effect": 1}, {"next": 1, "task": 1, "instantiate": 1, "example": 1, "fig": 1}, {"1": 1, "right": 1}, {"fig": 1}, {"2": 1, "middle": 1, "depict": 1, "pomdp": 1, "first": 1, "decision": 1, "aliasing": 1, "final": 1, "state": 1}, {"common": 1, "case": 1, "partial": 1, "observability": 1, "especially": 1, "pertinent": 1, "feature": 1, "learn": 1}, {"show": 1, "1": 2, "bootstrapping": 1, "naively": 1, "inadequate": 1, "case": 1, "issue": 2, "2": 2, "hca": 1, "able": 2, "carry": 1, "appropriate": 1, "information1": 1, "monte": 1, "carlo": 1, "overcome": 1, "partial": 1, "observability": 1, "performance": 1, "deteriorate": 1, "intermediate": 1, "reward": 1, "noise": 1, "present": 1}, {"hca": 1, "hand": 1, "able": 1, "reduce": 1, "variance": 1, "due": 1, "irrelevant": 1, "noise": 1, "reward": 1}, {"additionally": 1, "example": 1, "first": 1, "decision": 1, "relevant": 1, "choice": 1, "despite": 1, "temporally": 1, "remote": 1, "highlight": 1, "use": 1, "temporal": 1, "proximity": 1, "credit": 1, "assignment": 1, "heuristic": 1, "issue": 1, "3": 1}, {"one": 1, "final": 1, "state": 2, "reward": 1, "r": 2, "": 3, "1": 2, "penalize": 1, "middle": 1, "contain": 1, "white": 1, "noise": 1, "standard": 1, "deviation": 1}, {"fig": 1}, {"4": 1, "depict": 1, "result": 1}, {"task": 1, "returnconditional": 1, "hca": 1, "difficult": 2, "learn": 2, "problem": 1, "need": 1, "correctly": 1, "model": 1, "noise": 1, "distribution": 1, "condition": 1, "value": 1, "naively": 1, "hence": 1, "perform": 1, "similarly": 1, "baseline": 1}, {"ambiguous": 1, "bandit": 1}, {"finally": 1, "emphasize": 1, "credit": 1, "assignment": 1, "challenge": 1, "even": 1, "longterm": 1, "consider": 1, "problem": 1, "without": 1, "temporal": 1, "component": 1}, {"fig": 1}, {"2": 1, "right": 1, "depict": 1, "bandit": 1, "two": 2, "action": 1, "lead": 1, "different": 1, "state": 1, "whose": 1, "reward": 1, "function": 1, "similar": 1, "draw": 1, "overlap": 1, "gaussian": 1, "distributions": 1, "probability": 1, "": 1, "crossover": 1}, {"challenge": 1, "due": 1, "variance": 1, "issue": 2, "1": 1, "lack": 1, "counterfactual": 1, "update": 1, "4": 1}, {"difficult": 1, "tell": 1, "whether": 1, "action": 1, "genuinely": 1, "better": 1, "happen": 1, "tail": 1, "end": 1, "distribution": 1}, {"common": 1, "scenario": 1, "bootstrapping": 1, "similar": 1, "value": 1}, {"due": 1, "explicit": 1, "aim": 1, "model": 1, "distributions": 1, "hindsight": 1, "algorithms": 1, "efficient": 1, "fig": 1}, {"5": 1, "leave": 1}, {"highlight": 1, "differences": 1, "two": 1, "type": 1, "hindsight": 1, "condition": 1, "introduce": 1, "partial": 1, "observability": 1, "issue": 1, "2": 1, "see": 1, "fig": 1}, {"5": 1, "right": 1}, {"returnconditional": 1, "policy": 2, "still": 1, "able": 1, "improve": 1, "gradient": 1, "stateconditioning": 1, "fail": 1, "provide": 1, "informative": 1, "condition": 1, "construction": 1}, {"6": 1, "": 2, "relate": 1, "work": 1, "hindsight": 1, "experience": 1, "replay": 1, "1": 1, "introduce": 1, "idea": 1, "offpolicy": 1, "learn": 1, "many": 1, "goals": 1, "trajectory": 1}, {"intuition": 1, "regardless": 1, "goal": 1, "trajectory": 1, "pursue": 1, "originally": 1, "hindsight": 1, "eg": 1, "successfully": 1, "find": 1, "one": 1, "correspond": 1, "final": 1, "state": 1, "something": 1, "learn": 1}, {"rauber": 1, "et": 1, "al": 1}, {"15": 1, "extend": 1, "intuition": 1, "policy": 1, "gradient": 1, "algorithms": 1, "goalconditioned": 1, "policies": 1}, {"goyal": 1, "et": 1, "al": 1}, {"5": 1, "also": 1, "use": 1, "goal": 2, "condition": 1, "learn": 1, "backtrack": 1, "model": 1, "predict": 1, "stateaction": 1, "pair": 1, "occur": 1, "trajectories": 1, "end": 1, "state": 1}, {"work": 1, "share": 1, "intuition": 1, "hindsight": 1, "use": 1, "data": 1, "learn": 1, "many": 1, "things": 1, "context": 1, "goalconditioned": 1, "policies": 2, "essentially": 1, "contrast": 1, "conditional": 1, "unconditional": 1, "condition": 1, "extra": 1, "outcome": 1, "state": 1, "return": 1}, {"note": 1, "never": 1, "act": 1, "wrt": 1}, {"conditional": 1, "policy": 1, "use": 1, "solely": 1, "credit": 1, "assignment": 1}, {"temporal": 1, "value": 1, "transport": 1, "algorithm": 1, "11": 1, "also": 1, "aim": 1, "propagate": 1, "credit": 1, "efficiently": 1, "backward": 1, "time": 1}, {"use": 1, "attention": 1, "mechanism": 1, "memory": 1, "jump": 1, "part": 1, "trajectory": 1, "irrelevant": 1, "reward": 1, "obtain": 1}, {"demonstrate": 1, "challenge": 1, "problems": 1, "method": 1, "bias": 1, "promise": 1, "direction": 1, "future": 1, "research": 1, "apply": 1, "unbiased": 1, "hindsight": 1, "mechanism": 2, "past": 1, "state": 1, "choose": 1, "attention": 1}, {"another": 1, "line": 1, "work": 1, "relate": 1, "intuition": 1, "rudder": 1, "2": 1}, {"use": 1, "lstm": 1, "predict": 1, "future": 1, "return": 2, "sensitivity": 1, "analysis": 1, "distribute": 1, "1": 1, "": 3, "see": 1, "discussion": 1, "appendix": 1, "f": 1, "8": 1, "immediate": 1, "reward": 1, "order": 1, "reduce": 1, "learn": 1, "horizon": 1, "make": 1, "longterm": 1, "credit": 1, "assignment": 1, "easier": 1}, {"instead": 1, "aim": 1, "redistribute": 1, "return": 1, "state": 1, "hca": 1, "dowmnweights": 1, "individual": 1, "reward": 1, "accord": 1, "relevance": 1, "past": 1, "action": 1}, {"large": 1, "number": 1, "variance": 1, "reduction": 1, "techniques": 1, "apply": 1, "rl": 1, "eg": 1}, {"use": 1, "learn": 1, "value": 1, "function": 1, "critics": 1, "control": 1, "variates": 1, "eg": 1}, {"24": 1}, {"model": 1, "environment": 1, "available": 1, "use": 1, "reduce": 1, "variance": 1}, {"rollouts": 1, "state": 1, "fill": 1, "role": 1, "policy": 1, "gradients": 1, "18": 1}, {"differentiable": 1, "system": 1, "dynamics": 1, "allow": 1, "lowvariance": 1, "estimate": 1, "qvalue": 1, "gradient": 2, "use": 1, "pathwise": 1, "derivative": 1, "estimator": 1, "effectively": 1, "backpropagating": 1, "objective": 1, "along": 1, "trajectories": 1, "eg": 1}, {"17": 1, "9": 1, "10": 1}, {"stochastic": 1, "systems": 1, "require": 1, "knowledge": 1, "environment": 1, "noise": 1}, {"bypass": 1, "heess": 1, "et": 1, "al": 1}, {"9": 1, "infer": 1, "noise": 1, "give": 1, "observe": 1, "trajectory": 1}, {"buesing": 1, "et": 1, "al": 1}, {"3": 1, "apply": 1, "idea": 1, "pomdps": 1, "view": 1, "reason": 1, "events": 1, "hindsight": 1}, {"use": 1, "structural": 1, "causal": 1, "model": 1, "dynamics": 1, "infer": 1, "posterior": 1, "latent": 1, "cause": 1, "empirical": 1, "trajectories": 1}, {"use": 1, "empirical": 1, "rather": 1, "learn": 1, "distribution": 1, "latent": 1, "cause": 1, "reduce": 1, "bias": 1, "together": 1, "deterministic": 1, "model": 1, "system": 1, "dynamics": 1, "allow": 1, "explore": 1, "effect": 1, "alternative": 1, "action": 1, "choices": 1, "observe": 1, "trajectory": 1}, {"inverse": 1, "model": 1, "similar": 1, "ones": 1, "use": 1, "appear": 1, "instance": 1, "variational": 1, "intrinsic": 1, "control": 1, "6": 1, "see": 1, "also": 1, "eg": 1}, {"8": 1}, {"however": 1, "work": 2, "inverse": 2, "model": 2, "serve": 1, "way": 1, "determine": 1, "influence": 2, "action": 2, "future": 2, "outcome": 1, "whereas": 1, "6": 1, "8": 1, "aim": 1, "use": 1, "derive": 1, "intrinsic": 1, "reward": 1, "train": 1, "policies": 1, "observations": 1}, {"finally": 1, "prioritize": 1, "sweep": 1, "view": 1, "change": 1, "sample": 1, "distribution": 1, "hindsight": 1, "knowledge": 1, "td": 1, "errors": 1, "13": 1}, {"7": 1, "": 2, "close": 1, "propose": 1, "new": 1, "family": 1, "algorithms": 1, "explicitly": 1, "consider": 1, "question": 1, "credit": 1, "assignment": 1, "part": 1, "instead": 1, "estimate": 1, "traditional": 1, "value": 1, "function": 1}, {"propose": 1, "estimators": 1, "come": 1, "new": 1, "properties": 1, "validate": 1, "empirically": 1, "able": 1, "address": 1, "key": 1, "issue": 1, "credit": 1, "assignment": 1}, {"investigate": 1, "scalability": 1, "algorithms": 1, "deep": 1, "reinforcement": 1, "learn": 1, "set": 1, "excite": 1, "problem": 1, "future": 1, "research": 1}, {"acknowledgements": 1, "author": 1, "thank": 1, "joseph": 1, "modayil": 1, "review": 1, "earlier": 1, "manuscripts": 1, "theo": 1, "weber": 1, "several": 1, "insightful": 1, "suggestions": 1, "anonymous": 1, "reviewers": 1, "useful": 1, "feedback": 1}, {"reference": 1, "1": 1, "marcin": 1, "andrychowicz": 1, "filip": 1, "wolski": 1, "alex": 1, "ray": 1, "jonas": 1, "schneider": 1, "rachel": 1, "fong": 1, "peter": 1, "welinder": 1, "bob": 1, "mcgrew": 1, "josh": 1, "tobin": 1, "pieter": 1, "abbeel": 1, "wojciech": 1, "zaremba": 1}, {"hindsight": 1, "experience": 1, "replay": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "page": 1, "50485058": 1, "2017": 1}, {"2": 1, "jose": 1, "arjonamedina": 1, "michael": 2, "gillhofer": 1, "widrich": 1, "thomas": 1, "unterthiner": 1, "johannes": 1, "brandstetter": 1, "sepp": 1, "hochreiter": 1}, {"rudder": 1, "return": 1, "decomposition": 1, "delay": 1, "reward": 1}, {"h": 2, "wallach": 1, "larochelle": 1, "beygelzimer": 1, "f": 1, "alchbuc": 1, "e": 1, "fox": 1, "r": 1, "garnett": 1, "editors": 1, "advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "32": 1, "page": 1, "1354413555": 1, "2019": 1}, {"3": 1, "lars": 1, "buesing": 1, "theophane": 1, "weber": 1, "yori": 1, "zwols": 1, "sbastien": 1, "racanire": 1, "arthur": 1, "guez": 1, "jeanbaptiste": 1, "lespiau": 1, "nicolas": 1, "heess": 1}, {"woulda": 1, "coulda": 1, "shoulda": 1, "counterfactuallyguided": 1, "policy": 1, "search": 1}, {"corr": 1, "abs181106272": 1, "2018": 1}, {"4": 1, "igor": 1, "vladimirovich": 1, "girsanov": 1}, {"transform": 1, "certain": 1, "class": 1, "stochastic": 1, "process": 1, "absolutely": 1, "continuous": 1, "substitution": 1, "measure": 1}, {"theory": 1, "probability": 1, "": 1, "applications": 1, "53285301": 1, "1960": 1}, {"5": 1, "anirudh": 1, "goyal": 1, "philemon": 1, "brakel": 1, "william": 1, "fedus": 1, "soumye": 1, "singhal": 1, "timothy": 1, "lillicrap": 1, "sergey": 1, "levine": 1, "hugo": 1, "larochelle": 1, "yoshua": 1, "bengio": 1}, {"recall": 1, "trace": 1, "backtrack": 1, "model": 1, "efficient": 1, "reinforcement": 1, "learn": 1}, {"international": 1, "conference": 1, "learn": 1, "representationsiclr": 1, "2019": 1}, {"9": 1, "": 1, "6": 1, "karol": 1, "gregor": 1, "danilo": 1, "jimenez": 1, "rezende": 1, "daan": 1, "wierstra": 1}, {"variational": 1, "intrinsic": 1, "control": 1}, {"arxiv": 1, "preprint": 1, "arxiv161107507": 1, "2016": 1}, {"7": 1, "michael": 1, "gutmann": 1, "aapo": 1, "hyvrinen": 1}, {"noisecontrastive": 1, "estimation": 2, "new": 1, "principle": 1, "unnormalized": 1, "statistical": 1, "model": 1}, {"proceed": 1, "thirteenth": 1, "international": 1, "conference": 1, "artificial": 1, "intelligence": 1, "statistics": 1, "page": 1, "297304": 1, "2010": 1}, {"8": 1, "karol": 1, "hausman": 1, "jost": 1, "tobias": 1, "springenberg": 1, "ziyu": 1, "wang": 1, "nicolas": 1, "heess": 1, "martin": 1, "riedmiller": 1}, {"learn": 1, "embed": 1, "space": 1, "transferable": 1, "robot": 1, "skills": 1}, {"international": 1, "conference": 1, "learn": 1, "representations": 1, "iclr": 1, "2018": 1}, {"9": 1, "nicolas": 1, "heess": 1, "gregory": 1, "wayne": 1, "david": 1, "silver": 1, "timothy": 1, "lillicrap": 1, "tom": 1, "erez": 1, "yuval": 1, "tassa": 1}, {"learn": 1, "continuous": 1, "control": 1, "policies": 1, "stochastic": 1, "value": 1, "gradients": 1}, {"c": 1, "cortes": 1, "n": 1, "lawrence": 1, "lee": 1, "sugiyama": 1, "r": 1, "garnett": 1, "editors": 1, "advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "28": 1, "page": 1, "29442952": 1}, {"curran": 1, "associate": 1, "inc": 1, "2015": 1}, {"10": 1, "mikael": 1, "henaff": 1, "william": 1, "f": 1, "whitney": 1, "yann": 1, "lecun": 1}, {"modelbased": 1, "plan": 1, "discrete": 1, "continuous": 1, "action": 1}, {"arxiv": 1, "preprint": 1, "arxiv170507177": 1, "2017": 1}, {"11": 1, "chiachun": 1, "hang": 1, "timothy": 1, "lillicrap": 1, "josh": 1, "abramson": 1, "yan": 1, "wu": 1, "mehdi": 1, "mirza": 1, "federico": 1, "carnevale": 1, "arun": 1, "ahuja": 1, "greg": 1, "wayne": 1}, {"optimize": 1, "agent": 1, "behavior": 1, "long": 1, "time": 1, "scale": 1, "transport": 1, "value": 1}, {"arxiv": 1, "preprint": 1, "arxiv181006721": 1, "2018": 1}, {"12": 1, "volodymyr": 1, "mnih": 1, "adria": 1, "puigdomenech": 1, "badia": 1, "mehdi": 1, "mirza": 1, "alex": 1, "grave": 1, "timothy": 1, "lillicrap": 1, "tim": 1, "harley": 1, "david": 1, "silver": 1, "koray": 1, "kavukcuoglu": 1}, {"asynchronous": 1, "methods": 1, "deep": 1, "reinforcement": 1, "learn": 1}, {"maria": 1, "florina": 1, "balcan": 1, "kilian": 1, "q": 1, "weinberger": 1, "editors": 1, "proceed": 2, "33rd": 1, "international": 1, "conference": 1, "machine": 2, "learn": 2, "volume": 1, "48": 1, "research": 1, "page": 1, "19281937": 1, "new": 2, "york": 2, "usa": 1, "2022": 1, "jun": 1, "2016": 1}, {"pmlr": 1}, {"13": 1, "andrew": 1, "w": 1, "moore": 1, "christopher": 1, "g": 1, "atkeson": 1}, {"prioritize": 1, "sweep": 1, "reinforcement": 1, "learn": 1, "less": 2, "data": 1, "time": 1}, {"machine": 1, "learn": 1, "131103130": 1, "1993": 1}, {"14": 1, "martin": 1, "puterman": 1}, {"markov": 1, "decision": 1, "process": 1, "discrete": 1, "stochastic": 1, "dynamic": 1, "program": 1}, {"john": 1, "wiley": 1, "": 1, "sons": 1, "1994": 1}, {"15": 1, "paulo": 1, "rauber": 1, "avinash": 1, "ummadisingu": 1, "filipe": 1, "mutz": 1, "jrgen": 1, "schmidhuber": 1}, {"hindsight": 1, "policy": 1, "gradients": 1}, {"international": 1, "conference": 1, "learn": 1, "representations": 1, "iclr": 1, "2019": 1}, {"16": 1, "gerardo": 1, "rubino": 1, "bruno": 1, "tuffin": 1, "et": 1, "al": 1}, {"rare": 1, "event": 1, "simulation": 1, "use": 1, "monte": 1, "carlo": 1, "methods": 1, "volume": 1, "73": 1}, {"wiley": 1, "online": 1, "library": 1, "2009": 1}, {"17": 1, "john": 1, "schulman": 1, "nicolas": 1, "heess": 1, "theophane": 1, "weber": 1, "pieter": 1, "abbeel": 1}, {"gradient": 1, "estimation": 1, "use": 1, "stochastic": 1, "computation": 1, "graph": 1}, {"corr": 1, "abs150605254": 1, "2015": 1}, {"18": 1, "john": 1, "schulman": 1, "sergey": 1, "levine": 1, "pieter": 1, "abbeel": 1, "michael": 1, "jordan": 1, "philipp": 1, "moritz": 1}, {"trust": 1, "region": 1, "policy": 1, "optimization": 1}, {"international": 1, "conference": 1, "machine": 1, "learn": 1, "page": 1, "18891897": 1, "2015": 1}, {"19": 1, "satinder": 1, "singh": 1, "tommi": 1, "jaakkola": 1, "michael": 1, "jordan": 1}, {"learn": 1, "without": 1, "state": 1, "estimation": 1, "partially": 1, "observable": 1, "environments": 1}, {"international": 1, "conference": 1, "machine": 1, "learn": 1, "icml": 1, "1994": 1}, {"20": 1, "richard": 1, "sutton": 1}, {"learn": 1, "predict": 1, "methods": 1, "temporal": 1, "differences": 1}, {"machine": 1, "learn": 1, "31944": 1, "1988": 1}, {"21": 1, "richard": 1, "sutton": 1, "andrew": 1, "g": 1, "barto": 1}, {"reinforcement": 1, "learn": 1, "introduction": 1}, {"mit": 1, "press": 1, "cambridge": 1, "usa": 1, "2nd": 1, "edition": 1, "2018": 1}, {"22": 1, "richard": 1, "sutton": 1, "david": 1, "mcallester": 1, "satinder": 1, "p": 1, "singh": 1, "yishay": 1, "mansour": 1}, {"policy": 1, "gradient": 1, "methods": 1, "reinforcement": 1, "learn": 1, "function": 1, "approximation": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "page": 1, "10571063": 1, "2000": 1}, {"23": 1, "aaron": 1, "van": 1, "den": 1, "oord": 1, "yazhe": 1, "li": 1, "oriol": 1, "vinyals": 1}, {"representation": 1, "learn": 1, "contrastive": 1, "predictive": 1, "cod": 1}, {"arxiv": 1, "preprint": 1, "arxiv180703748": 1, "2018": 1}, {"10": 1}]
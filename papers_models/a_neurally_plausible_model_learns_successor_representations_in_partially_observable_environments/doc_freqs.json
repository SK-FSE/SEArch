[{"neurally": 1, "plausible": 1, "model": 1, "learn": 1, "successor": 1, "representations": 1, "partially": 1, "observable": 1, "environments": 1, "": 1, "eszter": 1, "vrtes": 1, "maneesh": 1, "sahani": 1, "gatsby": 1, "computational": 1, "neuroscience": 1, "unit": 1, "university": 1, "college": 1, "london": 2, "w1t": 1, "4jg": 1, "uk": 1}, {"esztermaneeshgatsbyuclacuk": 1, "": 1, "abstract": 1, "animals": 1, "need": 1, "devise": 1, "strategies": 1, "maximize": 1, "return": 1, "interact": 1, "environment": 1, "base": 1, "incoming": 1, "noisy": 1, "sensory": 1, "observations": 1}, {"taskrelevant": 1, "state": 1, "agents": 1, "location": 1, "within": 1, "environment": 1, "presence": 1, "predator": 1, "often": 1, "directly": 1, "observable": 1, "must": 1, "infer": 1, "use": 1, "available": 1, "sensory": 1, "information": 1}, {"successor": 1, "representations": 1, "sr": 1, "propose": 1, "middleground": 1, "modelbased": 1, "modelfree": 1, "reinforcement": 1, "learn": 1, "strategies": 1, "allow": 1, "fast": 1, "value": 1, "computation": 1, "rapid": 1, "adaptation": 1, "change": 1, "reward": 1, "function": 1, "goal": 1, "locations": 1}, {"indeed": 1, "recent": 1, "study": 1, "suggest": 1, "feature": 1, "neural": 1, "responses": 1, "consistent": 1, "sr": 1, "framework": 1}, {"however": 1, "clear": 1, "representations": 1, "might": 1, "learn": 1, "compute": 1, "partially": 1, "observe": 1, "noisy": 1, "environments": 1}, {"introduce": 1, "neurally": 1, "plausible": 1, "model": 1, "use": 1, "distributional": 2, "successor": 2, "feature": 1, "build": 1, "distribute": 1, "code": 1, "representation": 2, "computation": 2, "uncertainty": 1, "allow": 1, "efficient": 1, "value": 1, "function": 1, "partially": 1, "observe": 1, "environments": 1, "via": 1}, {"show": 1, "distributional": 1, "successor": 1, "feature": 1, "support": 1, "reinforcement": 1, "learn": 2, "noisy": 1, "environments": 1, "direct": 1, "successful": 1, "policies": 1, "infeasible": 1}, {"1": 1, "": 2, "introduction": 1, "humans": 1, "animals": 1, "able": 1, "evaluate": 1, "longterm": 1, "consequences": 1, "action": 1, "adapt": 1, "behaviour": 1, "maximize": 1, "reward": 1, "across": 1, "different": 1, "environments": 1}, {"behavioural": 1, "flexibility": 1, "often": 1, "think": 1, "result": 1, "interaction": 1, "two": 1, "adaptive": 1, "systems": 1, "implement": 1, "modelbased": 1, "modelfree": 1, "reinforcement": 1, "learn": 1, "rl": 1}, {"modelbased": 1, "learn": 1, "allow": 1, "flexible": 1, "goaldirected": 1, "behaviour": 1, "acquire": 1, "internal": 1, "model": 1, "environment": 1, "use": 1, "evaluate": 1, "consequences": 1, "action": 1}, {"result": 1, "agent": 1, "rapidly": 1, "adjust": 1, "policy": 1, "localize": 1, "change": 1, "environment": 1, "reward": 1, "function": 1}, {"flexibility": 1, "come": 1, "high": 1, "computational": 1, "cost": 1, "optimal": 1, "action": 1, "value": 1, "function": 1, "depend": 1, "expensive": 1, "simulations": 1, "model": 1}, {"modelfree": 1, "methods": 1, "hand": 1, "learn": 1, "cache": 1, "value": 1, "state": 1, "action": 2, "enable": 1, "rapid": 1, "selection": 1}, {"however": 1, "approach": 1, "particularly": 1, "slow": 1, "adapt": 1, "change": 2, "task": 1, "adjust": 1, "behaviour": 1, "even": 1, "localize": 1, "eg": 1}, {"placement": 1, "reward": 1, "require": 1, "update": 1, "cache": 1, "value": 1, "state": 1, "environment": 1}, {"suggest": 2, "brain": 1, "make": 1, "use": 1, "complementary": 1, "approach": 1, "may": 1, "compete": 1, "behavioural": 2, "control": 1, "daw": 2, "et": 3, "al": 3, "2005": 1, "indeed": 1, "several": 1, "study": 1, "subject": 1, "implement": 1, "hybrid": 1, "modelfree": 1, "modelbased": 1, "strategies": 1, "2011": 1, "glscher": 1, "2010": 1}, {"successor": 1, "representations": 1, "sr": 1, "dayan": 1, "1993": 1, "augment": 1, "internal": 1, "state": 2, "use": 1, "modelfree": 1, "systems": 1, "expect": 1, "future": 1, "occupancy": 1, "world": 1}, {"srs": 1, "view": 1, "precompiled": 1, "representation": 1, "model": 1, "give": 1, "policy": 1}, {"thus": 1, "learn": 1, "base": 1, "srs": 1, "fall": 1, "modelfree": 1, "model33rd": 1, "conference": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "neurips": 1, "2019": 1, "vancouver": 1, "canada": 1}, {"base": 1, "approach": 1, "correspondingly": 1, "reproduce": 1, "range": 1, "behaviours": 1, "russek": 1, "et": 1, "al": 1, "2017": 1}, {"recent": 1, "study": 1, "argue": 1, "evidence": 1, "consistent": 1, "srs": 1, "rodent": 1, "hippocampal": 1, "human": 1, "behavioural": 1, "data": 1, "stachenfeld": 1, "et": 2, "al": 2, "2017": 2, "momennejad": 1}, {"motivate": 1, "theoretical": 1, "experimental": 1, "work": 2, "argue": 1, "neural": 1, "rl": 1, "systems": 1, "operate": 1, "latent": 1, "state": 2, "need": 1, "handle": 1, "uncertainty": 1, "dayan": 1, "daw": 1, "2008": 1, "gershman": 1, "2018": 1, "starkweather": 1, "et": 1, "al": 1, "2017": 1, "take": 1, "successor": 1, "framework": 1, "consider": 1, "partially": 1, "observable": 1, "environments": 1}, {"adopt": 1, "framework": 1, "distribute": 1, "distributional": 1, "cod": 1, "vrtes": 1, "sahani": 1, "2018": 1, "show": 1, "learn": 1, "latent": 2, "dynamical": 1, "model": 1, "environment": 1, "naturally": 1, "integrate": 1, "srs": 1, "define": 1, "space": 1}, {"begin": 1, "short": 1, "overviews": 1, "reinforcement": 1, "learn": 1, "partially": 1, "observe": 1, "set": 1, "section": 3, "2": 1, "sr": 1, "3": 1, "distribute": 1, "distributional": 1, "cod": 1, "ddcs": 1, "4": 1}, {"section": 1, "5": 1, "describe": 1, "use": 1, "ddcs": 1, "generative": 1, "recognition": 1, "model": 2, "lead": 1, "particularly": 1, "simple": 1, "algorithm": 1, "learn": 2, "latent": 1, "state": 1, "dynamics": 1, "associate": 1, "sr": 1, "": 3, "2": 1, "partially": 1, "observable": 1, "markov": 2, "decision": 2, "process": 2, "mdp": 1, "provide": 1, "framework": 1, "wide": 1, "range": 1, "sequential": 1, "decisionmaking": 1, "task": 1, "relevant": 1, "reinforcement": 1}, {"mdp": 1, "define": 1, "set": 1, "state": 2, "action": 2, "reward": 1, "function": 1, "r": 2, "": 3, "probability": 1, "distribution": 1, "s0": 1, "describe": 1, "markovian": 1, "dynamics": 1, "condition": 1, "agent": 1}, {"notational": 1, "convenience": 1, "take": 1, "reward": 1, "function": 1, "independent": 1, "action": 1, "depend": 1, "state": 1, "approach": 1, "describe": 1, "easily": 1, "extend": 1, "general": 1, "case": 1}, {"partially": 1, "observable": 2, "markov": 1, "decision": 1, "process": 1, "pomdp": 1, "generalization": 1, "mdp": 1, "markovian": 1, "state": 1, "": 1, "directly": 1, "agent": 1}, {"instead": 1, "agent": 1, "receive": 1, "observations": 1, "": 1, "depend": 1, "current": 1, "latent": 1, "state": 1, "via": 1, "observation": 1, "process": 1, "zos": 1}, {"formally": 1, "pomdp": 1, "tuple": 1, "": 3, "r": 1, "z": 1, "comprise": 1, "object": 1, "define": 2, "discount": 1, "factor": 1, "pomdps": 1, "either": 1, "discrete": 1, "continuous": 1, "state": 1, "space": 1}, {"focus": 1, "general": 1, "continuous": 1, "case": 1, "although": 1, "model": 1, "present": 1, "applicable": 1, "discrete": 1, "state": 1, "space": 1, "well": 1}, {"3": 1, "": 2, "successor": 1, "representation": 1, "agent": 1, "explore": 1, "environment": 1, "state": 1, "visit": 1, "order": 1, "agents": 1, "policy": 1, "transition": 1, "structure": 1, "world": 1}, {"state": 1, "representations": 1, "respect": 1, "dynamic": 1, "order": 1, "likely": 1, "efficient": 1, "value": 1, "estimation": 1, "may": 1, "promote": 1, "effective": 1, "generalization": 1}, {"may": 1, "true": 1, "observe": 1, "state": 1, "coordinate": 1}, {"instance": 1, "barrier": 1, "spatial": 1, "environment": 1, "might": 1, "mean": 1, "two": 1, "state": 1, "adjacent": 1, "physical": 1, "coordinate": 1, "associate": 1, "different": 1, "value": 1}, {"dayan": 1, "1993": 1, "argue": 1, "natural": 1, "state": 2, "space": 1, "modelfree": 1, "value": 1, "estimation": 1, "one": 1, "distance": 1, "reflect": 1, "similarity": 1, "future": 1, "paths": 1, "give": 1, "agents": 1, "policy": 1}, {"successor": 1, "representation": 1, "dayan": 1, "1993": 1, "sr": 1, "state": 3, "si": 4, "define": 1, "expect": 1, "discount": 1, "sum": 1, "future": 1, "occupancies": 1, "sj": 3, "": 15, "give": 1, "current": 1, "e": 1, "hx": 1, "k": 1, "istk": 1, "st": 1}, {"1": 1, "": 3, "k0": 1, "discrete": 1, "state": 2, "space": 1, "sr": 1, "n": 3, "matrix": 1, "number": 1, "environment": 1}, {"sr": 1, "depend": 1, "current": 1, "policy": 1, "": 1, "expectation": 1, "right": 1, "hand": 1, "side": 1, "eq": 1}, {"1": 1, "take": 1, "respect": 1, "possibly": 1, "stochastic": 1, "policy": 1, "p": 1, "st": 2, "": 3, "environmental": 1, "transition": 1, "st1": 1}, {"sr": 1, "make": 1, "possible": 1, "express": 1, "value": 1, "function": 1, "particularly": 1, "simple": 1, "form": 1}, {"follow": 1, "eq": 1}, {"1": 1, "usual": 1, "definition": 1, "value": 1, "function": 1, "x": 1, "v": 1, "": 10, "si": 2, "sj": 2, "rsj": 2, "2": 1, "j": 1, "immediate": 1, "reward": 1, "state": 1}, {"successor": 1, "matrix": 1, "": 1, "learn": 2, "temporal": 1, "difference": 1, "td": 2, "sutton": 1, "1988": 1, "much": 1, "way": 1, "use": 1, "update": 1, "value": 1, "function": 1}, {"particular": 1, "sr": 1, "update": 1, "accord": 1, "td": 1, "error": 1, "sj": 4, "": 14, "ist": 1, "st1": 1, "st": 1, "3": 1, "2": 1, "reflect": 1, "errors": 1, "state": 1, "predictions": 1, "rather": 1, "reward": 1, "learn": 1, "signal": 1, "typically": 1, "associate": 1, "model": 1, "base": 1, "rl": 1}, {"show": 1, "eq": 1}, {"2": 1, "value": 1, "function": 1, "factorize": 1, "srie": 1, "information": 1, "expect": 1, "future": 1, "state": 1, "policyand": 1, "instantaneous": 1, "reward": 1, "state1": 1, "": 1}, {"modularity": 1, "enable": 1, "rapid": 1, "policy": 2, "evaluation": 1, "change": 1, "reward": 2, "condition": 1, "fix": 1, "function": 1, "need": 1, "relearn": 1, "evaluate": 1, "v": 1, "": 1}, {"contrast": 1, "modelfree": 1, "modelbased": 1, "algorithms": 1, "require": 1, "extensive": 1, "experience": 1, "rely": 1, "computationally": 1, "expensive": 1, "evaluation": 1, "respectively": 1, "recompute": 1, "value": 1, "function": 1}, {"31": 1, "": 20, "successor": 4, "representation": 4, "use": 3, "feature": 7, "generalize": 1, "continuous": 1, "state": 2, "set": 3, "function": 5, "define": 1, "also": 1, "refer": 1, "sf": 1, "encode": 1, "expect": 1, "value": 3, "instead": 1, "occupancies": 1, "individual": 1, "hx": 1, "st": 5, "e": 1, "k": 1, "stk": 1, "4": 1, "k0": 1, "assume": 1, "reward": 1, "write": 1, "approximate": 1, "linear": 2, "rs": 1, "wtrew": 2, "collect": 1, "vector": 1, "v": 2, "simple": 1, "form": 1, "analagous": 1, "discrete": 1, "case": 1, "5": 1, "consistency": 1, "approximation": 1, "eq": 1}, {"4": 1, "parametrize": 1, "successor": 1, "feature": 1, "": 2, "st": 1}, {"x": 1, "": 17, "st": 5, "uij": 3, "j": 3, "6": 1, "form": 1, "sfs": 1, "embody": 1, "weight": 1, "find": 1, "temporal": 1, "difference": 1, "learn": 1, "st1": 1, "7": 1, "see": 1, "discrete": 1, "case": 1, "td": 1, "error": 1, "signal": 1, "prediction": 1, "errors": 1, "feature": 1, "state": 1, "rather": 1, "reward": 1}, {"4": 1, "": 2, "distribute": 2, "distributional": 2, "cod": 2, "ddc": 1, "candidate": 1, "neural": 1, "representation": 1, "uncertainty": 1, "zemel": 1, "et": 1, "al": 1, "1998": 1, "sahani": 2, "dayan": 1, "2003": 1, "recently": 1, "show": 1, "support": 1, "accurate": 1, "inference": 1, "learn": 1, "hierarchical": 1, "latent": 1, "variable": 1, "model": 1, "vrtes": 1, "2018": 1}, {"ddc": 1, "population": 1, "neurons": 1, "represent": 2, "distributions": 1, "fire": 2, "rat": 2, "implicitly": 1, "set": 1, "expectations": 1, "": 5, "eps": 1, "8": 1, "vector": 2, "ps": 1, "distribution": 1, "encode": 1, "function": 1, "specific": 1, "neuron": 1}, {"ddcs": 1, "think": 1, "represent": 1, "exponential": 1, "family": 1, "distributions": 1, "sufficient": 1, "statistics": 1, "use": 1, "mean": 1, "parameters": 1, "eps": 1, "wainwright": 1, "jordan": 1, "2008": 1}, {"5": 1, "": 2, "distributional": 1, "successor": 2, "representation": 3, "discuss": 1, "support": 1, "efficient": 1, "value": 1, "computation": 1, "incorporate": 1, "information": 1, "policy": 1, "environment": 1, "state": 1}, {"however": 1, "realistic": 1, "settings": 1, "state": 1, "directly": 1, "observable": 1, "agent": 1, "limit": 1, "statedependent": 1, "noisy": 1, "sensory": 1, "information": 1}, {"1": 1, "": 1, "alternatively": 1, "general": 1, "case": 1, "actiondependent": 1, "reward": 2, "expect": 1, "instantaneous": 1, "policydependent": 1, "action": 1, "state": 1}, {"3": 1, "": 23, "algorithm": 2, "1": 1, "wakesleep": 1, "ddc": 3, "statespace": 1, "model": 2, "initialise": 1, "w": 5, "converge": 1, "sleep": 2, "phase": 2, "sample": 1, "ssleep": 2, "osleep": 2, "t0n": 1, "psn": 1, "p": 1, "update": 3, "fw": 2, "t1": 3, "ot1": 3, "f": 1, "wake": 1, "collect": 1, "observations": 1, "infer": 1, "posterior": 1, "ot": 4, "observation": 1, "parameters": 1, "end": 1, "section": 1, "lay": 1, "representation": 1, "uncertainty": 1, "allow": 1, "learn": 1, "compute": 1, "successor": 1, "representations": 1, "define": 1, "latent": 1, "variables": 1}, {"first": 1, "describe": 1, "algorithm": 1, "learn": 1, "inference": 1, "dynamical": 1, "latent": 1, "variable": 1, "model": 1, "use": 1, "ddcs": 1}, {"establish": 1, "link": 1, "ddc": 1, "successor": 1, "feature": 1, "eq": 1}, {"4": 1, "show": 1, "combine": 1, "learn": 1, "call": 1, "distributional": 1, "successor": 1, "feature": 1}, {"discuss": 1, "different": 1, "algorithmic": 1, "implementationrelated": 1, "choices": 1, "propose": 1, "scheme": 1, "implications": 1}, {"51": 1, "": 2, "learn": 1, "inference": 1, "state": 2, "space": 1, "model": 2, "use": 1, "ddcs": 1, "consider": 1, "pomdps": 1, "statespace": 1, "transition": 1, "define": 1, "conditional": 1, "ddc": 1, "mean": 1, "depend": 1, "linearly": 1, "precede": 1, "feature": 1}, {"conditional": 1, "distribution": 1, "describe": 1, "latent": 1, "dynamics": 1, "imply": 1, "follow": 2, "policy": 1, "": 11, "write": 1, "form": 1, "p": 2, "st1": 4, "st": 5, "est1": 1, "9": 1, "matrix": 1, "parametrizing": 1, "functional": 1, "relationship": 1, "expectation": 1, "respect": 1}, {"agent": 1, "access": 1, "sensory": 1, "observations": 1, "ot": 1, "time": 1, "step": 1, "order": 1, "able": 1, "make": 1, "use": 1, "underlie": 1, "latent": 1, "structure": 1, "learn": 2, "parameters": 1, "generative": 1, "model": 2, "pst1": 1, "st": 2, "": 2, "pot": 1, "well": 1, "perform": 1, "inference": 1}, {"consider": 1, "online": 1, "inference": 1, "filter": 1, "ie": 1}, {"time": 2, "step": 1, "recognition": 1, "model": 1, "produce": 1, "estimate": 1, "qst": 1, "ot": 3, "": 6, "posterior": 1, "distribution": 1, "pst": 1, "give": 1, "observations": 1, "o1": 1, "o2": 1}, {"": 1}, {"": 1}, {"ot": 1, "": 1}, {"ddc": 2, "helmholtz": 1, "machine": 1, "vrtes": 1, "sahani": 1, "2018": 1, "distributions": 1, "represent": 1, "set": 1, "expectationsie": 1, "ot": 4, "": 9, "eqst": 1, "st": 1, "10": 1, "filter": 1, "posterior": 2, "compute": 1, "iteratively": 1, "use": 1, "previous": 1, "time": 1, "step": 1, "t1": 1, "ot1": 1, "new": 1, "observation": 1}, {"markovian": 1, "structure": 1, "state": 1, "space": 1, "model": 1, "see": 1, "fig": 1}, {"1": 1, "ensure": 1, "recognition": 1, "model": 1, "write": 1, "recursive": 1, "function": 1, "ot": 2, "": 7, "fw": 1, "t1": 1, "ot1": 1, "11": 1, "set": 1, "parameters": 1, "w": 1}, {"recognition": 1, "generative": 1, "model": 1, "update": 1, "use": 1, "adapt": 1, "version": 1, "wakesleep": 1, "algorithm": 1, "hinton": 1, "et": 1, "al": 1, "1995": 1, "vrtes": 1, "sahani": 1, "2018": 1}, {"follow": 1, "describe": 1, "two": 1, "phase": 1, "algorithm": 2, "detail": 1, "see": 1, "1": 1}, {"sleep": 2, "phase": 2, "aim": 1, "adjust": 1, "parameters": 1, "recognition": 1, "model": 2, "give": 1, "current": 1, "generative": 1}, {"specifically": 1, "recognition": 1, "model": 1, "approximate": 1, "expectation": 1, "ddc": 1, "encode": 1, "function": 1, "st": 1, "": 2, "filter": 1, "posterior": 1, "pst": 1, "ot": 1}, {"achieve": 1, "moment": 1, "match": 1, "ie": 1, "simulate": 1, "sequence": 1, "latent": 1, "observe": 1, "state": 1, "current": 1, "model": 2, "4": 1, "": 2, "minimize": 1, "euclidean": 1, "distance": 1, "output": 1, "recognition": 1, "sufficient": 1, "statistic": 1, "vector": 1}, {"evaluate": 1, "latent": 1, "state": 1, "next": 1, "time": 1, "step": 1}, {"x": 1, "sleep": 1, "w": 2, "": 13, "argmin": 1, "kssleep": 1, "fw": 1, "t1": 1, "ot1": 1, "osleep": 2, "k2": 1, "12": 1, "nq": 1, "1": 1, "ssleep": 1, "t0n": 1, "ps0": 1, "po0": 1, "s0": 1, "pst1": 1, "st": 1, "pot1": 1, "st1": 1}, {"t0": 1, "": 7, "update": 1, "rule": 1, "implement": 1, "online": 1, "sample": 1, "simulate": 2, "sufficiently": 1, "long": 1, "sequence": 2, "multiple": 1, "ssleep": 1, "osleep": 2, "recognition": 1, "model": 1, "learn": 1, "sleep": 1, "approximate": 1, "expectations": 1, "form": 1, "fw": 1, "t1": 1, "ot1": 1, "epst": 1, "ot": 1, "st": 1, "yield": 1, "ddc": 1, "representation": 1, "posterior": 1}, {"wake": 2, "phase": 2, "parameters": 1, "generative": 1, "model": 1, "adapt": 1, "capture": 1, "sensory": 1, "observations": 1, "better": 1}, {"focus": 1, "learn": 2, "policydependent": 1, "latent": 1, "dynamics": 1, "p": 1, "st1": 1, "st": 1, "": 1, "observation": 1, "model": 1, "approach": 1, "vrtes": 1, "sahani": 1, "2018": 1}, {"give": 1, "sequence": 1, "infer": 1, "posterior": 1, "representations": 1, "ot": 2, "": 6, "compute": 1, "use": 1, "wake": 1, "phase": 1, "observations": 1, "parameters": 1, "latent": 1, "dynamics": 1, "update": 1, "minimize": 1, "simple": 1, "predictive": 1, "cost": 1, "function": 1, "x": 1, "argmin": 1, "kt1": 1, "ot1": 1, "k2": 1, "13": 1, "intuition": 1, "behind": 1, "eq": 1}, {"13": 1, "optimal": 1, "generative": 1, "model": 1, "latent": 1, "dynamics": 1, "satisfy": 1, "follow": 1, "equality": 1, "": 5, "ot": 2, "epot1": 1, "t1": 1, "ot1": 1}, {"predictions": 1, "make": 1, "combine": 1, "posterior": 2, "time": 2, "prior": 1, "agree": 1, "average": 1, "next": 1, "stepmaking": 1, "": 1, "stationary": 1, "point": 1, "optimization": 1, "eq": 1}, {"13": 1}, {"detail": 1, "nature": 1, "approximation": 1, "imply": 1, "wake": 1, "phase": 1, "update": 1, "relationship": 1, "variational": 1, "learn": 1, "see": 1, "supplementary": 1, "material": 1}, {"practice": 1, "update": 1, "do": 1, "online": 1, "use": 1, "gradient": 1, "step": 1, "analogous": 1, "prediction": 1, "errors": 1, "": 12, "t1": 1, "ot1": 1, "ot": 2, "14": 1, "recognition": 1, "generative": 1, "latent": 1, "dynamics": 1, "s1": 1, "s2": 1, "r1": 1}, {"": 1}, {"": 1}, {"st1": 1, "r2": 1, "": 14, "trajectories": 1, "st": 1, "rt": 1, "rt1": 1, "o1": 1, "o2": 1, "ot1": 1, "ot": 1, "1": 1, "2": 1}, {"": 1}, {"": 1}, {"t1": 1, "": 6, "b": 1, "noisy": 1, "2d": 1, "environment": 1, "ddc": 2, "statespace": 2, "model": 2, "figure": 1, "1": 1, "learn": 1, "inference": 1, "parametrized": 1}, {"structure": 1, "generative": 1, "recognition": 1, "model": 1}, {"b": 1, "visualization": 1, "dynamics": 1, "learn": 1, "wakesleep": 1, "algorithm": 1, "1": 1}, {"arrows": 1, "show": 1, "conditional": 1, "mean": 1, "est1": 1, "st": 1, "st1": 1, "": 1, "location": 1}, {"c": 1, "posterior": 1, "mean": 1, "trajectories": 2, "infer": 1, "use": 1, "recognition": 1, "model": 1, "plot": 1, "top": 1, "true": 1, "latent": 1, "observe": 1}, {"figure": 1, "1": 2, "show": 1, "statespace": 1, "model": 1, "correspond": 1, "random": 1, "walk": 1, "policy": 1, "latent": 1, "space": 1, "noisy": 1, "observations": 1, "learn": 1, "use": 1, "ddcs": 1, "algorithm": 1}, {"detail": 1, "experiment": 1, "see": 1, "supplementary": 1, "material": 1}, {"52": 1, "": 2, "learn": 1, "distributional": 1, "successor": 1, "feature": 1, "next": 1, "show": 1, "use": 1, "ddc": 1, "parametrize": 1, "generative": 1, "model": 1, "eq": 1}, {"9": 1, "make": 1, "possible": 1, "compute": 1, "successor": 1, "feature": 1, "define": 1, "latent": 1, "space": 1, "tractable": 1, "form": 1, "computation": 1, "combine": 1, "inference": 1, "base": 1, "sensory": 1, "observations": 1}, {"5": 1, "": 1, "follow": 1, "definition": 1, "sfs": 1, "eq": 1}, {"4": 1, "st": 3, "": 15, "e": 2, "hx": 1, "x": 1, "k": 2, "stk": 2, "k0": 2, "15": 1, "compute": 1, "conditional": 1, "expectations": 1, "feature": 1, "vector": 1, "eq": 1}, {"15": 1, "apply": 1, "dynamics": 1, "k": 2, "time": 1, "feature": 1, "st": 3, "": 4, "estk": 1, "stk": 1}, {"thus": 1, "st": 3, "": 15, "x": 1, "k": 2, "16": 1, "k0": 1, "1": 1, "17": 1, "eq": 1}, {"17": 1, "reminiscent": 1, "result": 1, "discrete": 1, "observe": 1, "state": 2, "space": 1, "si": 1, "": 4, "sj": 1, "p": 2, "1": 1, "ij": 1, "dayan": 1, "1993": 1, "matrix": 1, "contain": 1, "markovian": 1, "transition": 1, "probabilities": 1}, {"continuous": 1, "state": 1, "space": 1, "however": 1, "find": 1, "close": 1, "form": 1, "solution": 1, "like": 1, "eq": 1}, {"17": 1, "nontrivial": 1, "require": 1, "evaluate": 1, "set": 1, "typically": 1, "intractable": 1, "integrals": 1}, {"solution": 1, "present": 1, "directly": 1, "exploit": 1, "ddc": 2, "parametrization": 1, "generative": 1, "model": 1, "correspondence": 1, "feature": 1, "use": 1, "sfs": 1}, {"framework": 1, "compute": 2, "successor": 3, "feature": 3, "close": 1, "form": 1, "latent": 1, "space": 1, "also": 1, "evaluate": 1, "distributional": 2, "posterior": 1, "expectation": 1, "sfs": 1, "give": 1, "sequence": 1, "sensory": 1, "observations": 1, "est": 3, "ot": 4, "st": 3, "": 13, "1": 2, "18": 1, "19": 1, "result": 1, "section": 1, "suggest": 1, "number": 1, "different": 1, "ways": 1, "learn": 1}, {"521": 1, "": 4, "learn": 1, "distributional": 2, "sfs": 2, "sleep": 1, "phase": 1, "matrix": 1, "u": 1, "1": 1, "need": 1, "compute": 1, "eq": 1}, {"19": 1, "learn": 1, "temporal": 1, "differences": 1, "feature": 1, "predictions": 1, "base": 1, "sleep": 1, "phase": 1, "simulate": 1, "latent": 1, "state": 1, "sequence": 1, "see": 1, "eq": 1}, {"67": 1}, {"follow": 1, "potential": 1, "change": 1, "dynamics": 1, "environment": 1, "sleep": 1, "phase": 1, "learn": 1, "allow": 1, "update": 1, "sfs": 1, "therefore": 1, "cache": 1, "value": 1, "offline": 1, "without": 1, "need": 1, "experience": 1}, {"522": 1, "": 2, "compute": 1, "distributional": 1, "sfs": 1, "dynamics": 1, "alternatively": 1, "eq": 1}, {"19": 1, "implement": 1, "fix": 1, "point": 1, "linear": 1, "dynamical": 1, "system": 1, "recurrent": 1, "connections": 1, "reflect": 1, "model": 1, "latent": 1, "dynamics": 2, "": 15, "x": 4, "ot": 2, "1": 2, "20": 1, "21": 1, "case": 1, "need": 1, "learn": 1, "explicitly": 1, "implicitly": 1, "compute": 1}, {"work": 1, "underlie": 1, "assumption": 1, "dynamical": 1, "system": 1, "eq": 1}, {"20": 1, "reach": 1, "equilibrium": 1, "timescale": 1, "": 2, "faster": 1, "observations": 1, "ot": 1, "evolve": 1}, {"approach": 1, "avoid": 1, "compute": 1, "matrix": 2, "inverse": 1, "directly": 1, "allow": 1, "evaluation": 1, "policies": 1, "give": 1, "correspond": 1, "dynamics": 1, "": 1, "offline": 1}, {"523": 1, "": 4, "learn": 2, "distributional": 3, "sfs": 3, "wake": 2, "phase": 2, "instead": 1, "fully": 1, "rely": 1, "latent": 1, "dynamics": 1, "compute": 2, "use": 2, "posteriors": 2, "recognition": 1, "model": 1, "obfot": 1, "serve": 1, "define": 1, "directly": 1, "ddc": 1, "p": 1, "data": 1}, {"k": 2, "e": 1, "": 7, "tk": 1, "otk": 1, "ot": 3, "treat": 1, "posterior": 1, "representation": 1, "feature": 1, "space": 1, "fot": 1, "acquire": 1, "sequence": 1, "observations": 1, "o1": 1}, {"": 1}, {"": 1}, {"ot": 1, "": 1}, {"analogously": 1, "section": 1, "31": 1, "6": 1, "": 1, "figure": 1, "2": 1, "value": 1, "function": 1, "random": 1, "walk": 1, "policy": 1, "two": 1, "different": 1, "reward": 1, "locations": 1}, {"value": 1, "compute": 1, "use": 1, "sfs": 1, "base": 1, "latent": 1, "infer": 1, "ddc": 1, "posterior": 1, "observe": 1, "state": 1, "variables": 1}, {"fot": 1, "": 3, "u": 1, "ot": 1}, {"matrix": 1, "u": 2, "td": 1, "learn": 2, "assume": 1, "linear": 1, "function": 1, "approximation": 1, "update": 1, "online": 1, "execute": 1, "give": 1, "policy": 1, "continuously": 1, "infer": 2, "latent": 1, "state": 1, "representations": 1, "use": 1, "recognition": 2, "model": 4, "": 12, "ot": 4, "ot1": 1, "22": 1, "23": 1, "fot": 1, "define": 1, "equivalent": 1, "es": 1, "st": 1, "generative": 1, "show": 1, "optimalassuming": 1, "mismatchand": 1, "correctly": 1, "correspond": 1, "posteriors": 1, "see": 1, "supplementary": 1, "material": 1}, {"general": 1, "however": 1, "exchange": 1, "order": 1, "td": 1, "learn": 1, "inference": 1, "lead": 1, "different": 1, "sfs": 1}, {"advantage": 1, "learn": 1, "distributional": 1, "successor": 1, "feature": 1, "wake": 1, "phase": 1, "even": 1, "model": 1, "perfectly": 1, "capture": 1, "data": 1, "eg": 1}, {"due": 1, "lack": 1, "flexibility": 1, "early": 1, "learn": 2, "sfs": 1, "reflect": 1, "structure": 1, "observations": 1, "posteriors": 1, "ot": 1, "": 1}, {"53": 1, "": 2, "value": 2, "computation": 1, "noisy": 2, "2d": 1, "environment": 2, "illustrate": 1, "importance": 1, "able": 1, "consistently": 1, "handle": 1, "uncertainty": 1, "sfs": 1, "learn": 1, "function": 1}, {"use": 1, "simple": 1, "2dimensional": 1, "box": 1, "environment": 1, "continuous": 1, "state": 1, "space": 1, "include": 1, "internal": 1, "wall": 1}, {"agent": 1, "direct": 1, "access": 1, "spatial": 1, "coordinate": 1, "receive": 1, "observations": 1, "corrupt": 1, "gaussian": 1, "noise": 1}, {"figure": 1, "2": 1, "show": 1, "value": 1, "function": 1, "compute": 1, "use": 2, "successor": 1, "feature": 1, "learn": 1, "three": 1, "different": 1, "settings": 1, "assume": 1, "direct": 1, "access": 1, "latent": 2, "state": 3, "treat": 1, "observations": 2, "though": 1, "noisefree": 1, "measurements": 1, "estimate": 1, "infer": 1}, {"value": 2, "function": 2, "compute": 2, "latent": 1, "space": 1, "ddc": 1, "posterior": 1, "representations": 1, "reflect": 1, "structure": 1, "environment": 1, "rely": 1, "sfs": 1, "observe": 1, "state": 1, "fail": 1, "learn": 1, "barrier": 1}, {"demonstrate": 1, "simply": 1, "due": 1, "use": 1, "suboptimal": 1, "random": 1, "walk": 1, "policy": 2, "persist": 1, "learn": 2, "successor": 1, "feature": 1, "adjust": 1, "give": 1, "reward": 1, "function": 1, "see": 1, "figure": 1, "3": 1}, {"policy": 3, "learn": 1, "generalize": 1, "iteration": 1, "sutton": 1, "barto": 1, "1998": 1, "alternate": 1, "take": 1, "action": 1, "follow": 1, "greedy": 1, "update": 1, "successor": 1, "feature": 1, "estimate": 1, "correspond": 1, "value": 1, "function": 1}, {"value": 4, "state": 1, "action": 2, "compute": 1, "function": 4, "v": 3, "onestep": 1, "lookahead": 1, "combine": 1, "immediate": 1, "reward": 1, "expect": 1, "take": 1, "give": 1, "qst": 1, "": 7, "rst": 1, "est1": 1, "st": 1, "st1": 1, "24": 1, "case": 1, "latent": 1, "space": 1, "express": 1, "linear": 1, "feature": 1, "wtrew": 1, "u": 1, "eq": 1}, {"56": 1, "expectation": 1, "24": 1, "express": 1, "est1": 1, "st": 3, "v": 1, "st1": 2, "": 22, "wtrew": 2, "u": 2, "es0": 1, "sa": 1, "7": 1, "p": 3, "25": 1, "26": 1, "linear": 1, "map": 1, "contain": 1, "information": 1, "distribution": 1, "pst1": 1}, {"specifically": 1, "p": 1, "train": 1, "predict": 1, "est1": 1, "st": 2, "st1": 1, "": 3, "bilinear": 1, "function": 1, "state": 1, "action": 1, "feature": 1}, {"give": 1, "stateaction": 1, "value": 1, "implement": 1, "greedy": 1, "policy": 1, "choose": 1, "action": 1, "maximize": 1, "qs": 1, "": 17, "argmax": 3, "qst": 1, "27": 1, "aa": 2, "rst": 1, "wtrew": 1, "u": 1, "p": 1, "st": 1, "28": 1, "operation": 1, "eq": 1}, {"28": 1, "possibly": 1, "continuous": 1, "space": 1, "action": 1, "could": 1, "biologically": 1, "implement": 1, "ring": 2, "attractor": 1, "neurons": 1, "receive": 1, "statedependent": 1, "input": 1, "feedforward": 1, "weight": 1, "reflect": 1, "tune": 1, "neuron": 1}, {"figure": 1, "2": 1, "compute": 1, "value": 1, "function": 1, "fully": 1, "observe": 1, "case": 1, "use": 2, "infer": 1, "state": 1, "noisy": 1, "observations": 1}, {"latter": 1, "two": 1, "replace": 1, "st": 1, "": 1, "eq": 1}, {"28": 1, "infer": 1, "state": 1, "representation": 1, "ot": 2, "": 2, "observe": 1, "feature": 1, "respectively": 1}, {"agent": 1, "follow": 1, "greedy": 1, "policy": 1, "receive": 1, "new": 1, "observations": 1, "correspond": 1, "sfs": 1, "adapt": 1, "accordingly": 1}, {"figure": 1, "3": 1, "show": 1, "learn": 1, "value": 1, "function": 1, "v": 3, "": 6, "give": 1, "reward": 1, "location": 1, "correspond": 1, "dynamics": 1}, {"agent": 1, "access": 1, "true": 1, "latent": 1, "state": 1, "well": 1, "one": 1, "use": 1, "distributional": 1, "sfs": 1, "successfully": 1, "learn": 1, "policies": 1, "lead": 1, "reward": 1, "location": 1}, {"agent": 1, "learn": 1, "sfs": 1, "purely": 1, "base": 1, "observations": 1, "remain": 1, "highly": 1, "suboptimal": 1}, {"histogram": 1, "collect": 1, "reward": 1, "": 1, "figure": 1, "3": 1, "value": 1, "function": 1, "compute": 1, "sfs": 1, "learn": 1, "policy": 1}, {"top": 1, "row": 1, "show": 1, "reward": 1, "value": 1, "function": 1, "learn": 1, "three": 1, "different": 1, "condition": 1}, {"bottom": 1, "row": 1, "show": 1, "histogram": 1, "collect": 1, "reward": 1, "100": 1, "episodes": 1, "random": 1, "initial": 1, "state": 1, "learn": 1, "dynamics": 1, "": 1, "visualize": 1, "fig": 1}, {"1": 1}, {"6": 1, "": 2, "discussion": 1, "relate": 1, "work": 1, "show": 1, "ddc": 1, "represention": 1, "uncertainty": 2, "latent": 1, "variables": 1, "naturally": 1, "integrate": 1, "representations": 1, "future": 1, "state": 1, "thus": 1, "offer": 1, "natural": 1, "generalisation": 1, "srs": 1, "realistic": 1, "environments": 1, "partial": 1, "observability": 1}, {"propose": 1, "algorithm": 1, "jointly": 1, "tackle": 1, "problem": 1, "learn": 2, "latent": 1, "variable": 1, "model": 1, "perform": 1, "online": 1, "inference": 1, "filter": 1}, {"distributional": 1, "sfs": 1, "applicable": 1, "pomdps": 1, "continuous": 1, "discrete": 1, "variables": 1, "leverage": 1, "flexible": 1, "posterior": 1, "approximation": 1, "restrict": 1, "simple": 1, "parametric": 1, "form": 1, "represent": 1, "population": 1, "neurons": 1, "distribute": 1, "fashion": 1}, {"parametrising": 1, "latent": 2, "dynamics": 2, "ddcs": 1, "attractive": 1, "make": 1, "compute": 2, "sfs": 2, "space": 1, "analytically": 1, "tractable": 1, "allow": 1, "distributional": 1, "recurrent": 1, "sec": 1}, {"522": 1, "far": 1, "unclear": 1, "sample": 1, "model": 1, "might": 1, "implement": 1, "neural": 1, "circuit": 1}, {"alternatively": 1, "one": 1, "consider": 1, "standard": 1, "exponential": 1, "family": 1, "parametrisation": 1, "remain": 1, "compatible": 1, "sleep": 1, "wake": 1, "phase": 1, "td": 1, "learn": 1, "distributional": 1, "sfs": 1}, {"earlier": 1, "work": 1, "biological": 1, "reinforcement": 1, "learn": 1, "pomdps": 1, "restrict": 1, "case": 1, "binary": 1, "categorical": 1, "latent": 1, "variables": 1, "posterior": 1, "beliefs": 1, "compute": 1, "analytically": 1, "rao": 1, "2010": 1}, {"8": 1, "": 1, "furthermore": 1, "transition": 1, "model": 1, "pomdp": 1, "assume": 1, "know": 1, "rather": 1, "learn": 1, "present": 1, "work": 1}, {"define": 1, "distributional": 1, "sfs": 1, "state": 1, "use": 1, "single": 1, "step": 1, "lookahead": 1, "compute": 1, "stateaction": 1, "value": 1, "eq": 1}, {"24": 1}, {"alternatively": 1, "sfs": 1, "could": 1, "define": 1, "directly": 1, "state": 1, "action": 1, "kulkarni": 1, "et": 2, "al": 2, "2016": 1, "barreto": 1, "2017": 1, "whilst": 1, "retain": 1, "distributional": 1, "development": 1, "present": 1}, {"barreto": 1, "et": 1, "al": 1}, {"2017": 1, "2019": 1, "show": 1, "successor": 1, "representations": 1, "correspond": 1, "previously": 1, "learn": 1, "task": 2, "use": 1, "basis": 1, "construct": 1, "policies": 1, "novel": 1, "enable": 1, "generalization": 1}, {"framework": 1, "extend": 1, "similar": 1, "way": 1, "eliminate": 1, "need": 1, "adapt": 1, "sfs": 1, "policy": 1, "agent": 1, "change": 1}, {"neurotransmitter": 1, "dopamine": 1, "long": 1, "hypothesise": 1, "signal": 1, "reward": 1, "prediction": 1, "errors": 1, "rpe": 1, "thus": 1, "play": 1, "key": 1, "role": 1, "temporal": 1, "difference": 1, "learn": 1, "schultz": 1, "et": 1, "al": 1, "1997": 1}, {"recently": 1, "argue": 1, "dopamine": 1, "activity": 1, "consistent": 1, "rpes": 1, "compute": 1, "base": 1, "belief": 1, "state": 1, "rather": 1, "sensory": 1, "observations": 1, "directly": 1, "babayan": 1, "et": 3, "al": 3, "2018": 1, "lak": 1, "2017": 2, "sarno": 1}, {"thus": 1, "dopamine": 1, "well": 1, "suit": 1, "carry": 1, "information": 1, "necessary": 1, "learn": 1, "value": 1, "function": 1, "state": 1, "uncertainty": 1}, {"another": 1, "line": 1, "experimental": 1, "work": 1, "dopamine": 2, "find": 1, "signal": 1, "sensory": 1, "prediction": 1, "errors": 1, "pe": 1, "even": 1, "absence": 1, "associate": 1, "change": 1, "value": 1, "takahashi": 1, "et": 2, "al": 2, "2017": 1, "suggest": 1, "general": 1, "role": 1, "learn": 1, "gershman": 1, "2018": 2, "gardner": 1}, {"gardner": 1, "et": 1, "al": 1}, {"propose": 1, "dopaminesignalling": 1, "prediction": 1, "error": 2, "feature": 1, "state": 1, "may": 1, "provide": 1, "neural": 1, "substrate": 1, "signal": 1, "necessary": 1, "learn": 1, "successor": 1, "representations": 1}, {"distributional": 1, "sfs": 1, "unify": 1, "two": 1, "set": 1, "observations": 1, "theoretical": 1, "implications": 1, "single": 1, "framework": 1}, {"posit": 1, "pes": 2, "compute": 1, "posterior": 1, "belief": 1, "latent": 1, "state": 2, "represent": 1, "ddcs": 1, "define": 1, "set": 1, "nonlinear": 1, "feature": 1, "hide": 1, "rather": 1, "reward": 1}, {"propose": 1, "learn": 1, "scheme": 1, "distributional": 1, "sfs": 1, "allow": 1, "flexible": 1, "interpolation": 1, "modelbased": 1, "modelfree": 1, "approach": 1}, {"wake": 1, "phase": 2, "learn": 2, "sfs": 1, "ground": 1, "observations": 1, "rely": 1, "model": 2, "belief": 1, "state": 2, "update": 2, "sleep": 1, "use": 1, "simulate": 1, "latent": 1, "sfsakin": 1, "dyna": 1, "algorithm": 1, "sutton": 1, "1990": 1}, {"framework": 1, "learn": 1, "distributional": 1, "successor": 1, "feature": 1, "present": 1, "also": 1, "provide": 1, "link": 1, "various": 1, "intrigue": 1, "seemingly": 1, "disparate": 1, "experimental": 1, "observations": 1, "hippocampus": 1}, {"relationship": 1, "hippocampal": 1, "place": 1, "cell": 1, "activity": 1, "nondistributional": 1, "srs": 1, "explore": 1, "previously": 1, "eg": 1, "stachenfeld": 1, "et": 1, "al": 1, "2014": 1, "2017": 1, "provide": 1, "interpretation": 1, "phenomena": 1, "splitter": 1, "cells": 1, "show": 1, "spatial": 1, "tune": 1, "depend": 1, "whole": 1, "trajectory": 1, "ie": 1}, {"policy": 1, "traverse": 1, "animal": 1, "current": 1, "position": 1, "grieve": 1, "et": 1, "al": 1, "2016": 1}, {"however": 1, "discuss": 1, "earlier": 1, "relevant": 1, "state": 2, "give": 1, "reinforcement": 1, "learn": 2, "problem": 1, "case": 1, "sr": 1, "cannot": 1, "assume": 1, "directly": 1, "available": 1, "agent": 1, "must": 1, "infer": 1, "observations": 1}, {"hypothesis": 1, "hippocampal": 1, "place": 1, "cell": 1, "activity": 1, "encode": 1, "infer": 1, "location": 1, "concomitant": 1, "uncertainty": 1, "also": 1, "link": 1, "experimental": 1, "data": 1, "madl": 1, "et": 1, "al": 1, "2014": 1}, {"thus": 1, "approach": 1, "connect": 1, "two": 1, "separate": 1, "thread": 1, "literature": 1, "thereby": 1, "encompass": 1, "group": 1, "experimental": 1, "result": 1}, {"lastly": 1, "framework": 1, "help": 1, "link": 1, "simulation": 1, "internal": 1, "model": 1, "learn": 1}, {"acquisition": 1, "inference": 1, "model": 3, "framework": 1, "require": 1, "simulate": 1, "experience": 1, "sleep": 1, "sample": 1, "agents": 1, "current": 1, "environment": 1, "provide": 1, "basis": 1, "update": 1, "recognition": 1}, {"sleep": 1, "sample": 1, "reflect": 1, "agents": 1, "knowledge": 1, "environmental": 1, "dynamics": 1, "need": 1, "correspond": 1, "exactly": 1, "previously": 1, "experience": 1, "trajectory": 1}, {"reminiscent": 1, "hippocampal": 1, "replay": 1, "recapitulate": 1, "previous": 1, "experience": 2, "often": 1, "represent": 1, "novel": 1, "trajectories": 1, "previously": 1, "animal": 1, "gupta": 1, "et": 3, "al": 3, "2010": 1, "lafsdttir": 1, "2015": 1, "stella": 1, "2019": 1}, {"relatedly": 1, "liu": 1, "et": 1, "al": 1}, {"2019": 1, "recently": 1, "observe": 1, "replay": 1, "events": 1, "humans": 1, "reflect": 1, "abstract": 1, "structural": 1, "knowledge": 1, "learn": 1, "task": 1}, {"model": 1, "suggest": 1, "novel": 1, "functional": 1, "interpretation": 1, "replay": 1, "trajectories": 1, "namely": 1, "may": 1, "play": 1, "important": 1, "role": 1, "learn": 1, "infer": 1, "relevant": 1, "latent": 1, "state": 1, "observations": 1}, {"accord": 1, "observation": 1, "experimental": 1, "interference": 1, "replay": 1, "events": 1, "impede": 1, "learn": 1, "contexts": 1, "optimal": 1, "action": 1, "depend": 1, "historybased": 1, "inference": 1, "jadhav": 1, "et": 1, "al": 1, "2012": 1}, {"distributional": 1, "sfs": 1, "provide": 1, "interpretation": 1, "variety": 1, "experimental": 1, "observations": 1, "step": 1, "towards": 1, "algorithmic": 1, "solutions": 1, "flexible": 1, "decision": 1, "make": 1, "realistic": 1, "challenge": 1, "problem": 1, "settings": 1, "animals": 1, "face": 1, "ie": 1}, {"state": 1, "uncertainty": 1}, {"9": 1, "": 1, "reference": 1, "b": 1, "babayan": 1, "n": 1, "uchida": 1, "j": 1, "gershman": 1}, {"belief": 1, "state": 1, "representation": 1, "dopamine": 1, "system": 1}, {"nat": 1, "commun": 1, "911891": 1, "2018": 1}, {"barreto": 1, "w": 1, "dabney": 1, "r": 1, "munos": 1, "j": 2}, {"hunt": 1, "schaul": 1, "h": 1, "p": 1, "van": 1, "hasselt": 1, "silver": 1}, {"successor": 1, "feature": 1, "transfer": 1, "reinforcement": 1, "learn": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "page": 1, "40554065": 1, "2017": 1}, {"barreto": 1, "borsa": 1, "j": 1, "quan": 1, "schaul": 1, "silver": 1, "hessel": 1, "mankowitz": 1, "dek": 1, "r": 1, "munos": 1}, {"transfer": 1, "deep": 1, "reinforcement": 1, "learn": 1, "use": 1, "successor": 1, "feature": 1, "generalise": 1, "policy": 1, "improvement": 1}, {"arxiv190110964": 1, "cs": 1, "2019": 1}, {"n": 1, "daw": 1, "niv": 1, "p": 1, "dayan": 1}, {"uncertaintybased": 1, "competition": 1, "prefrontal": 1, "dorsolateral": 1, "striatal": 1, "systems": 1, "behavioral": 1, "control": 1}, {"nat": 1, "neurosci": 1, "8121704": 1, "2005": 1}, {"n": 1, "daw": 1, "j": 2, "gershman": 1, "b": 1, "seymour": 1, "p": 1, "dayan": 1, "r": 1, "dolan": 1}, {"modelbased": 1, "influence": 1, "humans": 1, "choices": 1, "striatal": 1, "prediction": 1, "errors": 1}, {"neuron": 1, "69612041215": 1, "2011": 1}, {"p": 1, "dayan": 1}, {"improve": 1, "generalization": 1, "temporal": 1, "difference": 1, "learn": 1, "successor": 1, "representation": 1}, {"neural": 1, "comput": 1, "54613624": 1, "1993": 1}, {"p": 1, "dayan": 1, "n": 1, "daw": 1}, {"decision": 1, "theory": 1, "reinforcement": 1, "learn": 1, "brain": 1}, {"cogn": 1, "affect": 1, "behav": 1, "neurosci": 1, "84429453": 1, "2008": 1}, {"p": 1, "h": 1, "gardner": 1, "g": 1, "schoenbaum": 1, "j": 1, "gershman": 1}, {"rethink": 1, "dopamine": 1, "generalize": 1, "prediction": 1, "error": 1}, {"proc": 1}, {"biol": 1}, {"sci": 1, "2851891": 1, "2018": 1}, {"j": 1, "gershman": 1}, {"successor": 1, "representation": 1, "computational": 1, "logic": 1, "neural": 1, "substrates": 1}, {"j": 1}, {"neurosci": 1, "383371937200": 1, "2018": 1}, {"j": 2, "glscher": 1, "n": 1, "daw": 1, "p": 2, "dayan": 1, "odoherty": 1}, {"state": 1, "versus": 1, "reward": 1, "dissociable": 1, "neural": 1, "prediction": 1, "error": 1, "signal": 1, "underlie": 1, "modelbased": 1, "modelfree": 1, "reinforcement": 1, "learn": 1}, {"neuron": 1, "664585595": 1, "2010": 1}, {"r": 2, "grieve": 1, "e": 1, "wood": 1, "p": 1, "dudchenko": 1}, {"place": 1, "cells": 1, "maze": 1, "encode": 1, "rout": 1, "rather": 1, "destinations": 1}, {"elife": 1, "5e15986": 1, "2016": 1}, {"gupta": 1}, {"van": 1, "der": 1, "meer": 1, "touretzky": 1, "redish": 1}, {"hippocampal": 1, "replay": 1, "simple": 1, "function": 1, "experience": 1}, {"neuron": 1, "655695705": 1, "2010": 1}, {"g": 1, "e": 1, "hinton": 1, "p": 1, "dayan": 1, "b": 1, "j": 1, "frey": 1, "r": 1, "neal": 1}, {"wakesleep": 1, "algorithm": 1, "unsupervised": 1, "neural": 1, "network": 1}, {"science": 1, "268521411581161": 1, "1995": 1}, {"p": 2, "jadhav": 1, "c": 1, "kemere": 1, "w": 1, "german": 1, "l": 1, "frank": 1}, {"awake": 1, "hippocampal": 1, "sharpwave": 1, "ripple": 1, "support": 1, "spatial": 1, "memory": 1}, {"science": 1, "336608714541458": 1, "2012": 1}, {"kulkarni": 1, "saeedi": 1, "gautam": 1, "j": 1, "gershman": 1}, {"deep": 1, "successor": 1, "reinforcement": 1, "learn": 1}, {"arxiv160602396": 1, "cs": 1, "stat": 1, "2016": 1}, {"lak": 1, "k": 1, "nomoto": 1, "keramati": 1, "sakagami": 1, "kepecs": 1}, {"midbrain": 1, "dopamine": 1, "neurons": 1, "signal": 1, "belief": 1, "choice": 1, "accuracy": 1, "perceptual": 1, "decision": 1}, {"curr": 1}, {"biol": 1, "276821832": 1, "2017": 1}, {"liu": 1, "r": 1, "j": 2, "dolan": 1, "z": 1, "kurthnelson": 1, "e": 1, "behrens": 1}, {"human": 1, "replay": 1, "spontaneously": 1, "reorganize": 1, "experience": 1}, {"cell": 1, "1783640652e14": 1, "2019": 1}, {"madl": 1, "franklin": 1, "k": 1, "chen": 1, "montaldi": 1, "r": 1, "trappl": 1}, {"bayesian": 1, "integration": 1, "information": 1, "hippocampal": 1, "place": 1, "cells": 1}, {"plos": 1, "one": 1, "93e89762": 1, "2014": 1}, {"momennejad": 1, "e": 1, "russek": 1, "j": 2, "h": 1, "cheong": 1, "botvinick": 1, "n": 1, "daw": 1, "gershman": 1}, {"successor": 1, "representation": 1, "human": 1, "reinforcement": 1, "learn": 1}, {"nat": 1, "hum": 1, "behav": 1, "19680": 1, "2017": 1}, {"h": 1, "f": 1, "lafsdttir": 1, "c": 1, "barry": 1}, {"b": 1, "saleem": 1, "hassabis": 1, "h": 1, "j": 1, "spiers": 1}, {"hippocampal": 1, "place": 1, "cells": 1, "construct": 1, "reward": 1, "relate": 1, "sequence": 1, "unexplored": 1, "space": 1}, {"elife": 1, "4e06063": 1, "2015": 1}, {"10": 1, "": 1, "r": 1, "p": 1, "n": 1, "rao": 1}, {"decision": 2, "make": 1, "uncertainty": 1, "neural": 1, "model": 1, "base": 1, "partially": 1, "observable": 1, "markov": 1, "process": 1}, {"front": 1}, {"comput": 1}, {"neurosci": 1, "4": 1, "2010": 1}, {"e": 1, "russek": 1, "momennejad": 1, "botvinick": 1, "j": 1, "gershman": 1, "n": 1, "daw": 1}, {"predictive": 1, "representations": 1, "link": 1, "modelbased": 1, "reinforcement": 1, "learn": 1, "modelfree": 1, "mechanisms": 1}, {"plos": 1, "comput": 1, "biol": 1, "139e1005768": 1, "2017": 1}, {"sahani": 1, "p": 1, "dayan": 1}, {"doubly": 1, "distributional": 1, "population": 1, "cod": 1, "simultaneous": 1, "representation": 1, "uncertainty": 1, "multiplicity": 1}, {"neural": 1, "comput": 1, "151022552279": 1, "2003": 1}, {"sarno": 1, "v": 1, "de": 1, "lafuente": 1, "r": 1, "romo": 1, "n": 1, "parga": 1}, {"dopamine": 1, "reward": 1, "prediction": 1, "error": 1, "signal": 1, "cod": 1, "temporal": 1, "evaluation": 1, "perceptual": 1, "decision": 1, "report": 1}, {"proc": 1}, {"natl": 1}, {"acad": 1}, {"sci": 1}, {"usa": 1, "11448": 1, "e10494e10503": 1, "2017": 1}, {"w": 1, "schultz": 1, "p": 2, "dayan": 1, "r": 1, "montague": 1}, {"neural": 1, "substrate": 1, "prediction": 1, "reward": 1}, {"science": 1, "275": 1, "530615931599": 1, "1997": 1}, {"k": 1, "l": 1, "stachenfeld": 1, "botvinick": 1, "j": 1, "gershman": 1}, {"design": 1, "principles": 1, "hippocampal": 1, "cognitive": 1, "map": 1}, {"z": 1, "ghahramani": 1, "well": 1, "c": 1, "cortes": 1, "n": 1, "lawrence": 1, "k": 1, "q": 1, "weinberger": 1, "editors": 1, "advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "27": 1, "page": 1, "25282536": 1}, {"curran": 1, "associate": 1, "inc": 1, "2014": 1}, {"k": 1, "l": 1, "stachenfeld": 1, "botvinick": 1, "j": 1, "gershman": 1}, {"hippocampus": 1, "predictive": 1, "map": 1}, {"nat": 1, "neurosci": 1, "201116431653": 1, "2017": 1}, {"c": 1, "k": 1, "starkweather": 1, "b": 1, "babayan": 1, "n": 1, "uchida": 1, "j": 1, "gershman": 1}, {"dopamine": 1, "reward": 1, "prediction": 1, "errors": 1, "reflect": 1, "hiddenstate": 1, "inference": 1, "across": 1, "time": 1}, {"nat": 1, "neurosci": 1, "204581589": 1, "2017": 1}, {"f": 1, "stella": 1, "p": 1, "baracskay": 1, "j": 2, "oneill": 1, "csicsvari": 1}, {"hippocampal": 1, "reactivation": 1, "random": 1, "trajectories": 1, "resemble": 1, "brownian": 1, "diffusion": 1}, {"neuron": 1, "2019": 1}, {"r": 1, "sutton": 1}, {"learn": 1, "predict": 1, "methods": 1, "temporal": 1, "differences": 1}, {"mach": 1, "learn": 1, "31944": 1, "1988": 1}, {"r": 1, "sutton": 1}, {"integrate": 1, "architectures": 1, "learn": 1, "plan": 1, "react": 1, "base": 1, "approximate": 1, "dynamic": 1, "program": 1}, {"b": 1, "porter": 1, "r": 1, "mooney": 1, "editors": 1, "machine": 1, "learn": 1, "proceed": 1, "1990": 1, "page": 1, "216224": 1}, {"morgan": 1, "kaufmann": 1, "san": 1, "francisco": 1, "ca": 1, "1990": 1}, {"r": 1, "sutton": 1, "g": 1, "barto": 1}, {"introduction": 1, "reinforcement": 1, "learn": 1, "volume": 1, "135": 1}, {"mit": 1, "press": 1, "cambridge": 1, "1998": 1}, {"k": 1, "takahashi": 1, "h": 1, "batchelor": 1, "b": 1, "liu": 1, "khanna": 1, "morales": 1, "g": 1, "schoenbaum": 1}, {"dopamine": 1, "neurons": 1, "respond": 1, "errors": 1, "prediction": 1, "sensory": 1, "feature": 1, "expect": 1, "reward": 1}, {"neuron": 1, "95613951405e3": 1, "2017": 1}, {"e": 1, "vrtes": 1, "sahani": 1}, {"flexible": 1, "accurate": 1, "inference": 1, "learn": 1, "deep": 1, "generative": 1, "model": 1}, {"bengio": 1, "h": 2, "wallach": 1, "larochelle": 1, "k": 1, "grauman": 1, "n": 1, "cesabianchi": 1, "r": 1, "garnett": 1, "editors": 1, "advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "31": 1, "page": 1, "41664175": 1}, {"curran": 1, "associate": 1, "inc": 1, "2018": 1}, {"j": 1, "wainwright": 1, "jordan": 1}, {"graphical": 1, "model": 1, "exponential": 1, "families": 1, "variational": 1, "inference": 1}, {"find": 1}, {"trend": 1, "mach": 1}, {"learn": 1, "1121305": 1, "2008": 1}, {"r": 1, "zemel": 1, "p": 1, "dayan": 1, "pouget": 1}, {"probabilistic": 1, "interpretation": 1, "population": 1, "cod": 1}, {"neural": 1, "comput": 1, "102403430": 1, "1998": 1}, {"11": 1}]
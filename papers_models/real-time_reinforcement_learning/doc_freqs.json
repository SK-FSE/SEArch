[{"realtime": 1, "reinforcement": 2, "learn": 2, "": 4, "arxiv191104448v4": 1, "cslg": 1, "12": 1, "dec": 1, "2019": 1, "simon": 1, "ramstedt": 1, "mila": 2, "element": 2, "ai": 2, "universit": 1, "de": 1, "montral": 2, "simonramstedtgmailcom": 1, "christopher": 1, "pal": 1, "polytechnique": 1, "christopherpalpolymtlca": 1, "abstract": 1, "markov": 1, "decision": 1, "process": 1, "mdps": 1, "mathematical": 1, "framework": 1, "underlie": 1, "algorithms": 1, "rl": 1, "often": 1, "use": 1, "way": 1, "wrongfully": 1, "assume": 1, "state": 1, "agents": 1, "environment": 1, "change": 1, "action": 1, "selection": 1}, {"rl": 1, "systems": 1, "base": 1, "mdps": 2, "begin": 1, "find": 1, "application": 1, "realworld": 1, "safetycritical": 1, "situations": 1, "mismatch": 1, "assumptions": 1, "underlie": 1, "classical": 1, "reality": 1, "realtime": 1, "computation": 1, "may": 1, "lead": 1, "undesirable": 1, "outcomes": 1}, {"paper": 1, "introduce": 1, "new": 1, "framework": 1, "state": 1, "action": 1, "evolve": 1, "simultaneously": 1, "show": 1, "relate": 1, "classical": 1, "mdp": 1, "formulation": 1}, {"analyze": 1, "exist": 1, "algorithms": 1, "new": 1, "realtime": 1, "formulation": 1, "show": 1, "suboptimal": 1, "use": 1, "real": 1, "time": 1}, {"use": 1, "insights": 1, "create": 1, "new": 1, "algorithm": 2, "realtime": 2, "actorcritic": 2, "rtac": 1, "outperform": 1, "exist": 1, "stateoftheart": 1, "continuous": 1, "control": 1, "soft": 1, "nonrealtime": 1, "settings": 1}, {"code": 1, "videos": 1, "find": 1, "githubcomrmstrtrl": 1}, {"reinforcement": 1, "learn": 1, "lead": 1, "great": 1, "successes": 1, "game": 1, "tesauro": 1, "1994": 1, "mnih": 1, "et": 4, "al": 4, "2015": 2, "silver": 1, "2017": 1, "start": 1, "apply": 1, "successfully": 1, "realworld": 1, "robotic": 1, "control": 1, "schulman": 1, "hwangbo": 1, "2019": 1}, {"theoretical": 1, "underpin": 1, "methods": 1, "reinforcement": 1, "learn": 1, "markov": 1, "decision": 1, "process": 1, "mdp": 1, "framework": 1, "bellman": 1, "1957": 1}, {"well": 1, "suit": 2, "describe": 1, "turnbased": 1, "decision": 1, "problems": 1, "board": 1, "game": 1, "framework": 1, "ill": 1, "realtime": 1, "applications": 1, "environments": 1, "state": 1, "continue": 1, "evolve": 1, "agent": 1, "select": 1, "action": 1, "travnik": 1, "et": 1, "al": 1, "2018": 1}, {"nevertheless": 1, "framework": 1, "use": 2, "realtime": 1, "problems": 1, "essentially": 1, "trick": 1, "eg": 1}, {"pause": 1, "simulate": 1, "environment": 1, "action": 2, "selection": 2, "ensure": 1, "time": 1, "require": 1, "negligible": 1, "hwangbo": 1, "et": 1, "al": 1, "2017": 1}, {"figure": 1, "1": 1, "turnbased": 1, "interaction": 1, "": 3, "instead": 1, "rely": 1, "trick": 1, "propose": 1, "augment": 1, "decisionmaking": 1, "framework": 1, "realtime": 1, "reinforcement": 1, "learn": 1, "rtrl": 1, "agent": 1, "allow": 1, "exactly": 1, "one": 1, "timestep": 1, "select": 1, "action": 1}, {"rtrl": 1, "conceptually": 1, "simple": 1, "open": 1, "new": 1, "algorithmic": 1, "possibilities": 1, "special": 1, "structure": 1}, {"leverage": 1, "rtrl": 1, "create": 1, "realtime": 2, "actorcritic": 3, "rtac": 1, "new": 1, "algorithm": 1, "better": 1, "suit": 1, "interaction": 1, "base": 1, "soft": 1, "haarnoja": 1, "et": 1, "al": 1, "2018a": 1}, {"show": 1, "experimentally": 1, "rtac": 1, "outperform": 1, "sac": 1, "realtime": 1, "nonrealtime": 1, "settings": 1}, {"figure": 1, "2": 1, "realtime": 1, "interaction": 1, "": 1, "33rd": 1, "conference": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "neurips": 1, "2019": 1, "vancouver": 1, "canada": 1}, {"1": 1, "": 2, "background": 1, "reinforcement": 1, "learn": 1, "world": 1, "split": 1, "agent": 1, "environment": 1}, {"agent": 1, "represent": 2, "policy": 1, "": 1, "stateconditioned": 1, "action": 1, "distribution": 1, "environment": 1, "markov": 1, "decision": 1, "process": 1, "def": 1}, {"1": 1}, {"traditionally": 1, "agentenvironment": 1, "interaction": 1, "govern": 1, "mdp": 1, "framework": 1}, {"however": 1, "strictly": 1, "use": 1, "mdps": 1, "represent": 1, "environment": 1}, {"agentenvironment": 1, "interaction": 1, "instead": 1, "describe": 1, "different": 1, "type": 1, "markov": 1, "reward": 1, "process": 1, "mrp": 1, "tbmrp": 1, "def": 1}, {"2": 1, "behave": 1, "like": 1, "traditional": 1, "interaction": 1, "scheme": 1}, {"definition": 1, "1": 1}, {"markov": 3, "decision": 1, "process": 3, "mdp": 1, "characterize": 1, "tuple": 1, "1": 1, "state": 2, "space": 2, "2": 1, "action": 1, "3": 1, "initial": 1, "distribution": 2, "": 14, "r": 6, "4": 1, "transition": 1, "p": 1, "5": 1, "reward": 2, "function": 2, "agentenvironment": 1, "system": 1, "condense": 1, "consist": 1, "statereward": 1}, {"markov": 1, "process": 1, "induce": 1, "sequence": 2, "state": 1, "st": 1, "tn": 3, "together": 1, "r": 1, "reward": 1, "rt": 1, "": 2, "rst": 1}, {"usual": 1, "objective": 1, "find": 1, "policy": 1, "maximize": 1, "expect": 1, "sum": 1, "reward": 1}, {"practice": 1, "reward": 1, "discount": 1, "augment": 1, "guarantee": 1, "convergence": 1, "reduce": 1, "variance": 1, "encourage": 1, "exploration": 1}, {"however": 1, "evaluate": 1, "performance": 1, "agent": 1, "always": 1, "use": 1, "undiscounted": 1, "sum": 1, "reward": 1}, {"11": 1, "": 2, "turnbased": 2, "reinforcement": 2, "learn": 2, "usually": 1, "consider": 1, "part": 1, "standard": 1, "framework": 1, "scheme": 1, "agent": 1, "environment": 1, "interact": 1}, {"call": 1, "interaction": 1, "scheme": 1, "turnbased": 1, "markov": 1, "reward": 1, "process": 1}, {"definition": 1, "2": 1}, {"turnbased": 1, "markov": 2, "reward": 1, "process": 2, "": 11, "r": 2, "tbmrp": 1, "e": 2, "combine": 1, "decision": 1, "p": 1, "policy": 1, "z": 2, "st1": 1, "st": 2, "pst1": 1, "aast": 2, "da": 2, "rst": 2}, {"1": 1, "": 2, "say": 1, "interaction": 1, "turnbased": 1, "environment": 2, "pause": 2, "agent": 2, "select": 1, "action": 1, "receive": 1, "new": 1, "observation": 1}, {"illustrate": 1, "figure": 1, "1": 1}, {"action": 1, "select": 1, "certain": 1, "state": 2, "pair": 1, "induce": 1, "next": 1}, {"state": 1, "change": 1, "action": 1, "selection": 1, "process": 1}, {"2": 1, "": 3, "figure": 1, "3": 1, "tbmrp": 1, "realtime": 2, "reinforcement": 1, "learn": 1, "contrast": 1, "conventional": 1, "turnbased": 1, "interaction": 2, "scheme": 1, "propose": 1, "alternative": 1, "framework": 1, "state": 1, "action": 1, "evolve": 1, "simultaneously": 1}, {"agent": 1, "environment": 1, "step": 1, "unison": 1, "produce": 1, "new": 1, "stateaction": 2, "pair": 2, "x": 2, "t1": 1, "": 6, "st1": 1, "at1": 1, "old": 1, "st": 1, "illustrate": 1, "figure": 1, "2": 1, "4": 1}, {"x": 1, "": 9, "rr": 1, "rtmrp": 1, "e": 1, "definition": 1, "3": 1}, {"realtime": 1, "markov": 2, "reward": 1, "process": 2, "x": 1, "combine": 1, "decision": 1, "e": 1, "": 8, "p": 1, "r": 1, "policy": 1, "st1": 1, "at1": 2, "st": 4, "pst1": 1, "rr": 1, "rst": 1}, {"2": 1, "system": 1, "state": 1, "space": 1, "x": 1, "": 2}, {"initial": 1, "action": 1, "a0": 1, "set": 1, "fix": 1, "value": 1, "ie": 1}, {"": 9, "s0": 2, "a0": 2, "c1": 1, "figure": 1, "4": 1, "rtmrp": 1, "note": 1, "introduce": 1, "new": 1, "policy": 1, "take": 1, "stateaction": 1, "pair": 1, "instead": 1, "state": 1}, {"system": 1, "state": 1, "x": 1, "": 1, "stateaction": 1, "pair": 1, "alone": 1, "sufficient": 1, "statistic": 1, "future": 1, "stochastic": 1, "process": 1, "anymore": 1}, {"1": 1, "": 2, "dirac": 1, "delta": 1, "distribution": 1}, {"": 4, "x": 2, "probability": 1, "one": 1}, {"2": 1, "": 1, "21": 1, "realtime": 2, "framework": 2, "make": 1, "backtoback": 1, "action": 2, "selection": 1, "agent": 1, "exactly": 1, "one": 1, "timestep": 1, "select": 1}, {"agent": 1, "take": 2, "longer": 1, "policy": 1, "would": 1, "break": 1, "stag": 1, "less": 1, "one": 1, "timestep": 1, "evaluate": 1}, {"hand": 1, "agent": 1, "take": 1, "less": 1, "one": 1, "timestep": 1, "select": 1, "action": 2, "realtime": 1, "framework": 1, "delay": 1, "apply": 1, "next": 1, "observation": 1, "make": 1}, {"optimal": 1, "case": 1, "agent": 1, "immediately": 1, "upon": 1, "finish": 1, "select": 1, "action": 2, "observe": 1, "next": 2, "state": 1, "start": 1, "compute": 1}, {"continuous": 1, "backtoback": 1, "action": 2, "selection": 1, "ideal": 1, "allow": 1, "agent": 1, "update": 1, "quickest": 1, "delay": 1, "introduce": 1, "realtime": 1, "framework": 1}, {"achieve": 1, "backtoback": 1, "action": 1, "selection": 1, "might": 1, "necessary": 1, "match": 1, "timestep": 1, "size": 1, "policy": 1, "evaluation": 1, "time": 1}, {"current": 1, "algorithms": 1, "reduce": 1, "timestep": 1, "size": 1, "might": 1, "lead": 1, "worse": 1, "performance": 1}, {"recently": 1, "however": 1, "progress": 1, "make": 1, "towards": 1, "timestep": 1, "agnostic": 1, "methods": 1, "tallec": 1, "et": 1, "al": 1, "2019": 1}, {"believe": 1, "backtoback": 1, "action": 1, "selection": 1, "achievable": 1, "goal": 1, "demonstrate": 1, "realtime": 1, "framework": 1, "effective": 1, "even": 1, "able": 1, "tune": 1, "timestep": 1, "size": 1, "section": 1, "5": 1}, {"22": 1, "": 2, "realtime": 3, "interaction": 2, "express": 2, "within": 2, "turnbased": 2, "framework": 3, "possible": 1, "standard": 1, "allow": 1, "us": 1, "reconnect": 1, "vast": 1, "body": 1, "work": 1, "rl": 1}, {"specifically": 1, "try": 1, "find": 1, "augment": 1, "environment": 1, "rtmdp": 1, "e": 2, "behave": 1, "turnbased": 1, "interaction": 2, "would": 1, "realtime": 1}, {"realtime": 1, "framework": 1, "agent": 1, "communicate": 1, "action": 1, "environment": 1, "via": 1, "state": 1}, {"however": 1, "traditional": 1, "turnbased": 1, "framework": 1, "environment": 1, "directly": 1, "influence": 1, "state": 1}, {"therefore": 1, "need": 1, "deterministically": 1, "pass": 1, "action": 1, "next": 1, "state": 1, "augment": 1, "transition": 1, "function": 1}, {"rtmdp": 1, "two": 1, "type": 1, "action": 3, "1": 1, "emit": 1, "policy": 1, "2": 1, "component": 1, "state": 1, "x": 1, "": 4, "st": 1, "t1": 1, "probability": 1, "one": 1}, {"x": 1, "": 6, "p": 1, "r": 1, "rtmdp": 1, "e": 1, "augment": 1, "definition": 1, "4": 1}, {"realtime": 1, "markov": 2, "decision": 2, "process": 2, "x": 5, "another": 1, "e": 1, "": 34, "p": 3, "r": 2, "1": 1, "state": 2, "space": 2, "2": 1, "action": 1, "x0": 1, "s0": 2, "a0": 2, "c": 1, "3": 1, "initial": 1, "distribution": 2, "xt1": 1, "xt": 2, "st1": 1, "at1": 2, "st": 3, "pst1": 1, "4": 1, "transition": 1, "rst": 1}, {"5": 1, "reward": 1, "function": 1, "r": 2, "x": 2, "tap": 1, "see": 1, "code": 1, "": 1, "interact": 1, "rtmdp": 1, "e": 1, "conventional": 1, "turnbased": 1, "theorem": 1, "1": 1}, {"2": 1, "policy": 1, "": 3, "x": 1, "manner": 1, "give": 1, "rise": 1, "markov": 1, "reward": 1, "process": 1, "interact": 1, "e": 1, "realtime": 1, "ie": 1}, {"rtmrp": 1, "e": 2, "": 5, "tbmrp": 1, "rtmdp": 1}, {"3": 1, "interestingly": 1, "rtmdp": 1, "equivalent": 1, "1step": 1, "constant": 1, "delay": 1, "mdp": 1, "walsh": 1, "et": 1, "al": 1}, {"2008": 1}, {"however": 1, "believe": 1, "different": 2, "intuitions": 1, "behind": 1, "warrant": 1, "name": 1, "constant": 1, "delay": 2, "mdp": 1, "try": 1, "model": 2, "external": 1, "action": 2, "observation": 1, "whereas": 1, "rtmdp": 1, "time": 1, "take": 1, "select": 1}, {"connection": 1, "make": 1, "sense": 1, "though": 1, "framework": 1, "action": 2, "selection": 2, "assume": 1, "instantaneous": 2, "apply": 1, "delay": 1, "account": 1, "fact": 1}, {"23": 1, "": 2, "turnbased": 2, "interaction": 1, "express": 2, "within": 1, "realtime": 1, "framework": 1, "also": 1, "possible": 1, "define": 1, "augmentation": 1, "tbmdp": 1, "e": 1, "allow": 1, "us": 1, "environments": 1, "eg": 1}, {"chess": 1, "go": 1, "within": 1, "realtime": 1, "framework": 1, "def": 1}, {"7": 1, "appendix": 1}, {"assign": 1, "separate": 1, "timesteps": 1, "agent": 2, "environment": 2, "allow": 1, "act": 1, "pause": 1}, {"specifically": 1, "add": 1, "binary": 1, "variable": 1, "b": 1, "state": 1, "keep": 1, "track": 1, "whether": 1, "environments": 1, "agents": 1, "turn": 1}, {"b": 1, "invert": 1, "every": 2, "timestep": 2, "underlie": 1, "environment": 1, "advance": 1}, {"b": 1, "": 1, "interact": 1, "tbmdp": 1, "e": 1, "real": 1, "time": 1, "give": 1, "rise": 1, "theorem": 1, "2": 1}, {"policy": 1, "": 1, "markov": 1, "reward": 1, "process": 1, "contain": 1, "def": 1}, {"10": 1, "mrp": 1, "result": 1, "": 1, "interact": 1, "e": 1, "conventional": 1, "turnbased": 1, "manner": 1, "ie": 1}, {"tbmrp": 1, "e": 2, "": 4, "rtmrp": 1, "tbmdp": 1, "4": 1, "result": 1, "use": 2, "conventional": 1, "algorithms": 2, "realtime": 2, "framework": 2, "build": 1, "turnbased": 1, "problems": 1}, {"2": 1, "": 5, "proof": 1, "appendix": 1, "c": 1, "3": 2, "reinforcement": 1, "learn": 1, "realtime": 1, "markov": 1, "decision": 1, "process": 1, "establish": 1, "rtmdp": 2, "compatibility": 1, "layer": 1, "conventional": 1, "rl": 1, "rtrl": 1, "look": 1, "exist": 1, "theory": 1, "change": 1, "move": 1, "environment": 1, "e": 2}, {"since": 1, "rl": 1, "methods": 1, "assume": 1, "environments": 1, "dynamics": 2, "completely": 1, "unknown": 1, "able": 1, "make": 1, "use": 1, "fact": 1, "precisely": 1, "know": 1, "part": 1, "rtmdp": 1}, {"specifically": 1, "learn": 2, "data": 1, "effect": 1, "feedthrough": 1, "mechanism": 1, "could": 2, "lead": 1, "much": 1, "slower": 1, "worse": 1, "performance": 2, "apply": 1, "environment": 1, "rtmdp": 1, "e": 2, "instead": 1, "especially": 1, "hurt": 1, "offpolicy": 1, "algorithms": 1, "among": 1, "successful": 1, "rl": 1, "methods": 1, "date": 1, "mnih": 1, "et": 2, "al": 2, "2015": 1, "haarnoja": 1, "2018a": 1}, {"offpolicy": 1, "methods": 1, "make": 1, "use": 1, "actionvalue": 1, "function": 1}, {"": 4, "environment": 1, "e": 1, "p": 1, "r": 1, "policy": 1, "definition": 1, "5": 1}, {"action": 2, "value": 1, "function": 1, "qe": 3, "recursively": 1, "define": 1, "": 15, "st": 2, "rst": 1, "est1": 1, "pst": 1, "eat1": 1, "st1": 3, "at1": 2, "5": 1, "identity": 1, "use": 1, "train": 1, "actionvalue": 1, "estimator": 1, "transition": 1, "sample": 2, "replay": 1, "memory": 1, "contain": 1, "offpolicy": 1, "experience": 1, "next": 1, "policy": 1, "lemma": 1, "1": 1}, {"realtime": 1, "markov": 1, "decision": 1, "process": 1, "actionvalue": 1, "function": 1, "": 6, "rst": 1, "est1": 1, "pst": 1, "ea": 1, "t1": 1, "st1": 2, "at1": 1, "6": 1, "qrtmdpe": 2, "st": 1, "note": 1, "action": 1, "affect": 1, "reward": 1, "next": 1, "state": 1}, {"thing": 1, "affect": 2, "at1": 2, "turn": 1, "next": 1, "timestep": 1, "rst1": 1, "": 3, "st2": 1}, {"learn": 1, "effect": 1, "action": 1, "e": 1, "specifically": 1, "future": 1, "reward": 1, "perform": 2, "two": 1, "update": 1, "previously": 1, "one": 1}, {"investigate": 1, "experimentally": 1, "effect": 1, "offpolicy": 1, "soft": 1, "actorcritic": 1, "algorithm": 1, "haarnoja": 1, "et": 1, "al": 1, "2018a": 1, "section": 1, "51": 1}, {"31": 1, "": 2, "learn": 2, "statevalue": 2, "function": 3, "offpolicy": 2, "usually": 1, "use": 1, "way": 1, "actionvalue": 1}, {"": 1, "definition": 1, "6": 1}, {"statevalue": 1, "function": 1, "environment": 1, "e": 1, "": 13, "p": 1, "r": 1, "policy": 1, "st": 2, "eat": 1, "rst": 1, "est1": 1, "pst": 1, "st1": 1, "7": 1, "definition": 1, "show": 1, "expectation": 2, "action": 1, "take": 1, "next": 1, "state": 1}, {"use": 1, "identity": 1, "train": 1, "statevalue": 1, "estimator": 1, "cannot": 1, "simply": 1, "change": 1, "action": 1, "distribution": 1, "allow": 1, "offpolicy": 1, "learn": 1, "since": 1, "way": 1, "resampling": 1, "next": 1, "state": 1}, {"lemma": 1, "2": 1}, {"realtime": 1, "markov": 1, "decision": 1, "process": 1, "statevalue": 1, "function": 1, "vrtmdpe": 2, "st": 2, "": 13, "rst": 1, "est1": 1, "pst": 1, "ea": 1, "st1": 1}, {"8": 1, "st": 1, "": 2, "st1": 1, "always": 1, "valid": 1, "transition": 1, "matter": 1, "action": 1, "select": 1}, {"therefore": 1, "use": 2, "realtime": 1, "framework": 1, "value": 1, "function": 1, "offpolicy": 1, "learn": 1}, {"since": 2, "equation": 3, "8": 2, "5": 1, "except": 1, "policy": 1, "input": 1, "use": 2, "statevalue": 1, "function": 2, "previously": 1, "actionvalue": 1, "without": 1, "learn": 1, "dynamics": 1, "rtmdp": 1, "data": 1, "already": 1, "apply": 1}, {"32": 1, "": 2, "partial": 1, "simulation": 1, "offpolicy": 1, "learn": 1, "procedure": 1, "describe": 1, "previous": 1, "section": 1, "apply": 1, "generally": 1}, {"whenever": 1, "part": 1, "agentenvironment": 1, "system": 2, "know": 1, "temporarily": 1, "independent": 1, "remain": 1, "use": 1, "generate": 1, "synthetic": 1, "experience": 1}, {"precisely": 1, "transition": 4, "start": 1, "state": 1, "": 5, "w": 2, "z": 3, "generate": 1, "accord": 1, "true": 1, "kernel": 1, "s0": 1, "simulate": 1, "know": 1, "part": 2, "w0": 1, "use": 1, "store": 1, "sample": 1, "unknown": 1, "0": 1}, {"possible": 1, "transition": 1, "kernel": 1, "factorize": 1, "w0": 2, "": 2, "z": 2, "0": 2, "know": 1, "unknown": 1}, {"hindsight": 1, "experience": 1, "replay": 1, "andrychowicz": 1, "et": 1, "al": 1, "2017": 1, "see": 1, "another": 1, "example": 1, "partial": 1, "simulation": 1}, {"goal": 2, "part": 1, "state": 1, "evolve": 1, "independently": 1, "rest": 1, "allow": 1, "change": 1, "hindsight": 1}, {"next": 1, "section": 1, "use": 1, "partial": 1, "simulation": 1, "principle": 1, "compute": 1, "gradient": 1, "policy": 1, "loss": 1}, {"4": 2, "": 4, "realtime": 1, "actorcritic": 2, "rtac": 1, "algorithms": 1, "konda": 1, "tsitsiklis": 1, "2000": 1, "formulate": 1, "rl": 1, "problem": 1, "bilevel": 1, "optimization": 1, "critic": 2, "evaluate": 1, "actor": 2, "accurately": 1, "possible": 1, "try": 1, "improve": 1, "evaluation": 1}, {"silver": 1, "et": 1, "al": 1}, {"2014": 1, "show": 1, "possible": 1, "reparameterize": 1, "actor": 3, "evaluation": 1, "directly": 1, "compute": 1, "pathwise": 1, "derivative": 1, "critic": 1, "respect": 1, "parameters": 1, "thus": 1, "tell": 1, "improve": 1}, {"heess": 1, "et": 1, "al": 1}, {"2015": 1, "extend": 1, "stochastic": 1, "policies": 1, "haarnoja": 1, "et": 1, "al": 1}, {"2018a": 1, "extend": 1, "maximum": 1, "entropy": 1, "objective": 1, "create": 1, "soft": 1, "actorcritic": 1, "sac": 1, "rtac": 1, "go": 1, "base": 1, "compare": 1}, {"sac": 1, "parameterized": 1, "policy": 2, "": 8, "actor": 1, "optimize": 1, "minimize": 1, "kldivergence": 1, "exponential": 1, "approximation": 1, "actionvalue": 1, "function": 1, "q": 1, "critic": 1, "normalize": 1, "z": 2, "unknown": 1, "irrelevant": 1, "gradient": 1, "give": 1, "rise": 1, "loss": 1, "1": 1, "3": 1, "lsac": 1, "e": 1, "est": 1, "dkl": 1, "st": 1, "exp": 1, "qst": 1, "zst": 1, "9": 1, "uniform": 1, "distribution": 1, "replay": 1, "memory": 1, "contain": 1, "past": 1, "state": 1, "action": 1, "reward": 1}, {"actionvalue": 1, "function": 1, "optimize": 1, "fit": 1, "equation": 1, "5": 1, "present": 1, "previous": 1, "section": 1, "augment": 1, "entropy": 1, "term": 1}, {"thus": 1, "expect": 1, "sac": 1, "perform": 1, "worse": 1, "rtmdps": 1}, {"order": 1, "create": 1, "algorithm": 1, "better": 1, "suit": 1, "realtime": 1, "set": 1, "propose": 1, "use": 1, "statevalue": 1, "function": 1, "approximator": 1, "v": 1, "critic": 1, "instead": 1, "give": 1, "rise": 1, "policy": 1, "gradient": 1}, {"proposition": 1, "1": 1}, {"follow": 1, "policy": 2, "loss": 1, "base": 1, "statevalue": 1, "function": 1, "": 11, "st": 1, "exp": 1, "1": 1, "vv": 1, "st1": 1, "zst1": 1, "10": 1, "lrtac": 1, "est": 1, "est1": 1, "pst": 1, "dkl": 1, "rtmdp": 1, "e": 1, "gradient": 1, "lsac": 1, "ie": 1}, {"rtmdp": 2, "e": 3, "sac": 1, "": 10, "lrtac": 1, "lrtmdp": 1, "11": 1, "value": 1, "function": 1, "train": 1, "offpolicy": 1, "accord": 1, "procedure": 1, "describe": 1, "section": 1, "31": 1, "fit": 1, "augment": 1, "version": 1, "equation": 1, "8": 1, "specifically": 1, "st": 1}, {"v": 5, "target": 2, "": 21, "rst": 1, "est1": 1, "pst": 1, "ea": 1, "st": 1, "vv": 1, "st1": 2, "log": 1, "12": 1, "therefore": 2, "value": 1, "loss": 1, "x": 1, "xt": 2, "2": 1, "lrtac": 1, "r": 2, "ex": 1, "rtmdp": 1, "ev": 1, "41": 1, "13": 1, "merge": 1, "actor": 2, "critic": 4, "use": 2, "statevalue": 1, "function": 1, "another": 1, "advantage": 1, "evaluate": 1, "timestep": 1, "depend": 1, "actors": 1, "output": 1, "anymore": 1, "able": 1, "single": 1, "neural": 1, "network": 1, "represent": 1}, {"merge": 1, "actor": 1, "critic": 1, "make": 1, "necessary": 1, "trade": 1, "value": 1, "function": 1, "policy": 1, "loss": 1}, {"therefore": 1, "introduce": 1, "additional": 1, "hyperparameter": 1, "": 8, "rtac": 1, "l": 1, "lrtac": 1, "1": 1, "lrtmdp": 1, "ev": 1, "v": 1, "rtmdp": 1, "e": 1, "14": 1, "merge": 1, "actor": 1, "critic": 1, "could": 2, "speed": 1, "learn": 1, "even": 1, "improve": 1, "generalization": 1, "also": 1, "lead": 1, "greater": 1, "instability": 1}, {"compare": 1, "rtac": 1, "merge": 1, "separate": 1, "actor": 1, "critic": 1, "network": 1, "section": 1, "5": 1}, {"42": 1, "": 2, "stabilize": 1, "learn": 1, "actorcritic": 1, "algorithms": 1, "know": 1, "unstable": 1, "train": 1}, {"use": 1, "number": 1, "techniques": 1, "help": 1, "make": 1, "train": 1, "stable": 1}, {"notably": 1, "use": 1, "popart": 1, "output": 1, "normalization": 1, "van": 1, "hasselt": 1, "et": 1, "al": 1, "2016": 1, "normalize": 1, "value": 1, "target": 1}, {"necessary": 1, "v": 1, "": 1, "represent": 1, "use": 1, "overlap": 1, "set": 1, "parameters": 1}, {"since": 1, "scale": 1, "error": 1, "gradients": 1, "value": 1, "loss": 1, "highly": 1, "nonstationary": 1, "3": 1, "": 2, "temperature": 1, "hyperparameter": 1}, {"": 2, "0": 1, "maximum": 1, "entropy": 1, "objective": 2, "reduce": 1, "traditional": 1, "scale": 1}, {"compare": 1, "hyperparameters": 1, "table": 1, "": 3, "entropy": 1}, {"reward": 1, "scale": 1, "": 3, "5": 1, "hard": 1, "find": 1, "good": 1, "tradeoff": 1, "policy": 1, "value": 1, "loss": 1}, {"v": 1, "": 1, "separate": 1, "popart": 1, "matter": 1, "less": 1, "still": 1, "improve": 1, "performance": 1, "sac": 1, "well": 1, "rtac": 1}, {"another": 1, "difficulty": 1, "recursive": 1, "value": 1, "function": 1, "target": 1}, {"since": 1, "try": 1, "maximize": 1, "value": 3, "function": 2, "overestimation": 1, "errors": 1, "approximator": 1, "amplify": 1, "recursively": 1, "use": 1, "target": 1, "follow": 1, "optimization": 1, "step": 1}, {"introduce": 1, "fujimoto": 1, "et": 1, "al": 1}, {"2018": 1, "like": 1, "sac": 1, "use": 1, "two": 1, "value": 3, "function": 1, "approximators": 1, "take": 1, "minimum": 1, "compute": 1, "target": 1, "reduce": 1, "overestimation": 1, "ie": 1}, {"vv": 1, "": 4, "mini12": 1, "v": 1}, {"lastly": 1, "stabilize": 1, "recursive": 1, "value": 1, "function": 1, "estimation": 1, "use": 1, "target": 1, "network": 2, "slowly": 1, "track": 1, "weight": 1, "mnih": 1, "et": 2, "al": 2, "2015": 2, "lillicrap": 1, "ie": 1}, {"": 8, "1": 1}, {"track": 1, "weight": 1, "": 1, "use": 1, "compute": 1, "v": 1, "target": 1, "equation": 1, "12": 1}, {"5": 1, "": 2, "experiment": 1, "compare": 1, "realtime": 1, "actorcritic": 2, "soft": 1, "haarnoja": 1, "et": 4, "al": 4, "2018a": 1, "several": 1, "openaigymmujoco": 1, "benchmark": 1, "environments": 2, "brockman": 1, "2016": 1, "todorov": 1, "2012": 1, "well": 1, "two": 1, "avenue": 1, "autonomous": 1, "drive": 1, "visual": 1, "observations": 1, "ibrahim": 1, "2019": 1}, {"sac": 1, "agents": 1, "use": 2, "result": 1, "include": 1, "actionvalue": 1, "statevalue": 1, "function": 1, "approximator": 1, "fix": 1, "entropy": 1, "scale": 1, "": 1, "haarnoja": 1, "et": 1, "al": 1}, {"2018a": 1}, {"code": 1, "accompany": 1, "paper": 1, "drop": 1, "statevalue": 1, "function": 1, "approximator": 1, "since": 1, "impact": 1, "result": 1, "do": 1, "observe": 1, "haarnoja": 1, "et": 1, "al": 1}, {"2018b": 1}, {"comparison": 1, "algorithms": 1, "ddpg": 1, "ppo": 1, "td3": 1, "also": 1, "see": 1, "haarnoja": 1, "et": 1, "al": 1}, {"2018ab": 1}, {"make": 1, "comparison": 2, "two": 1, "algorithms": 1, "fair": 1, "possible": 1, "also": 1, "use": 1, "output": 1, "normalization": 1, "sac": 2, "improve": 1, "performance": 1, "task": 1, "see": 1, "figure": 1, "9": 1, "appendix": 1, "normalize": 1, "unnormalized": 1}, {"sac": 1, "rtac": 1, "perform": 1, "single": 1, "optimization": 1, "step": 1, "every": 1, "timestep": 1, "environment": 1, "start": 1, "first": 1, "10000": 1, "timesteps": 1, "collect": 1, "experience": 1, "base": 1, "initial": 1, "random": 1, "policy": 1}, {"hyperparameters": 1, "use": 1, "find": 1, "table": 1, "1": 1}, {"51": 1, "": 2, "sac": 4, "realtime": 2, "markov": 1, "decision": 1, "process": 1, "compare": 1, "return": 1, "trend": 1, "turnbased": 1, "environments": 2, "e": 2, "rtmdp": 1, "performance": 1, "deteriorate": 1}, {"seem": 1, "confirm": 1, "hypothesis": 1, "learn": 1, "dynamics": 1, "augment": 1, "environment": 1, "data": 1, "impede": 1, "actionvalue": 1, "function": 1, "approximation": 1, "hypothesize": 1, "section": 1, "3": 1}, {"figure": 1, "5": 1, "return": 1, "trend": 1, "sac": 1, "turnbased": 1, "environments": 2, "e": 2, "realtime": 1, "rtmdp": 1}, {"mean": 1, "95": 1, "confidence": 1, "interval": 1, "compute": 1, "eight": 1, "train": 1, "run": 1, "per": 1, "environment": 1}, {"6": 2, "": 3, "52": 1, "rtac": 2, "sac": 2, "mujoco": 1, "real": 1, "time": 1, "figure": 1, "show": 1, "comparison": 1, "realtime": 1, "versions": 1, "benchmark": 1, "environments": 1}, {"see": 1, "rtac": 1, "learn": 1, "much": 1, "faster": 1, "achieve": 1, "higher": 1, "return": 1, "sac": 1, "rtmdp": 1, "e": 1}, {"make": 1, "sense": 1, "learn": 1, "data": 1, "passthrough": 1, "behavior": 1, "rtmdp": 1}, {"show": 2, "rtac": 1, "separate": 1, "neural": 1, "network": 1, "policy": 1, "value": 2, "components": 1, "big": 1, "part": 1, "rtacs": 1, "advantage": 1, "sac": 1, "function": 1, "update": 1}, {"however": 1, "fact": 1, "policy": 1, "value": 1, "function": 1, "network": 1, "merge": 1, "improve": 1, "rtacs": 1, "performance": 1, "plot": 1, "suggest": 1}, {"note": 1, "rtac": 1, "always": 1, "rtmdp": 1, "e": 1, "therefore": 1, "explicitly": 1, "state": 1}, {"rtac": 1, "even": 1, "outperform": 1, "sac": 2, "e": 1, "allow": 1, "act": 1, "without": 1, "realtime": 1, "constraints": 1, "four": 1, "six": 1, "environments": 1, "include": 1, "two": 1, "hardest": 1, "ones": 1, "": 2, "ant": 1, "humanoid": 1, "largest": 1, "state": 1, "action": 1, "space": 1, "figure": 1, "11": 1}, {"theorize": 1, "possible": 1, "due": 1, "merge": 1, "actor": 1, "critic": 1, "network": 1, "use": 1, "rtac": 1}, {"important": 1, "note": 1, "however": 1, "rtac": 1, "merge": 1, "actor": 1, "critic": 1, "network": 1, "output": 1, "normalization": 1, "critical": 1, "figure": 1, "12": 1}, {"figure": 1, "6": 1, "comparison": 1, "rtac": 1, "sac": 1, "rtmdp": 1, "versions": 1, "benchmark": 1, "environments": 1}, {"mean": 1, "95": 1, "confidence": 1, "interval": 1, "compute": 1, "eight": 1, "train": 1, "run": 1, "per": 1, "environment": 1}, {"53": 1, "": 2, "autonomous": 2, "drive": 2, "task": 2, "addition": 1, "mujoco": 1, "environments": 1, "also": 1, "test": 1, "rtac": 1, "sac": 1, "use": 1, "avenue": 1, "simulator": 1, "ibrahim": 1, "et": 1, "al": 1, "2019": 1}, {"avenue": 1, "gameenginebased": 1, "simulator": 1, "agent": 1, "control": 1, "car": 1}, {"task": 1, "show": 1, "agent": 1, "stay": 1, "road": 1, "possibly": 1, "steer": 1, "around": 1, "pedestrians": 1}, {"observations": 1, "single": 1, "image": 1, "256x64": 1, "grayscale": 1, "pixels": 1, "cars": 1, "velocity": 1}, {"action": 1, "continuous": 1, "two": 1, "dimensional": 1, "represent": 1, "steer": 1, "angle": 1, "gasbrake": 1}, {"agent": 1, "reward": 2, "proportionally": 1, "cars": 1, "velocity": 1, "direction": 1, "road": 1, "negatively": 1, "make": 1, "contact": 1, "pedestrian": 1, "another": 1, "car": 1}, {"addition": 1, "episodes": 1, "terminate": 1, "leave": 1, "road": 1, "collide": 1, "object": 1, "pedestrians": 1}, {"figure": 1, "7": 1, "leave": 1, "agents": 1, "view": 1, "racesolo": 1}, {"right": 1, "passenger": 1, "view": 1, "citypedestrians": 1}, {"7": 1, "": 1, "figure": 1, "8": 1, "comparison": 1, "rtac": 1, "sac": 1, "rtmdp": 1, "versions": 1, "autonomous": 1, "drive": 1, "task": 1}, {"see": 1, "rtac": 1, "realtime": 2, "constraints": 2, "outperform": 1, "sac": 1, "even": 1, "without": 1}, {"mean": 1, "95": 1, "confidence": 1, "interval": 1, "compute": 1, "four": 1, "train": 1, "run": 1, "per": 1, "environment": 1}, {"hyperparameters": 1, "use": 2, "autonomous": 1, "drive": 1, "task": 2, "largely": 1, "mujoco": 1, "however": 1, "lower": 2, "entropy": 1, "reward": 1, "scale": 1, "005": 1, "learn": 1, "rate": 1, "00002": 1}, {"use": 1, "convolutional": 1, "neural": 1, "network": 1, "four": 1, "layer": 1, "convolutions": 1, "filter": 1, "size": 1, "8": 1, "4": 3, "stride": 1, "2": 3, "1": 1, "64": 2, "128": 2, "channel": 1}, {"convolutional": 1, "layer": 2, "follow": 1, "two": 1, "fully": 1, "connect": 1, "512": 1, "units": 1}, {"6": 1, "": 2, "relate": 1, "work": 1, "travnik": 1, "et": 1, "al": 1}, {"2018": 1, "notice": 1, "traditional": 1, "mdp": 1, "framework": 1, "ill": 1, "suit": 1, "realtime": 1, "problems": 1}, {"paper": 1, "however": 1, "rigorous": 1, "framework": 1, "propose": 1, "alternative": 1, "theoretical": 1, "analysis": 1, "provide": 1}, {"firoiu": 1, "et": 1, "al": 1}, {"2018": 1, "apply": 1, "multistep": 1, "action": 2, "delay": 2, "level": 1, "play": 1, "field": 1, "humans": 1, "artificial": 1, "agents": 1, "ale": 1, "atari": 1, "benchmark": 1, "however": 1, "address": 1, "problems": 1, "arise": 1, "turnbased": 1, "mdp": 1, "framework": 1, "recognize": 1, "significance": 1, "consequences": 1, "onestep": 1}, {"similar": 1, "rtac": 1, "naf": 1, "gu": 1, "et": 1, "al": 1, "2016": 1, "able": 1, "continuous": 1, "control": 1, "single": 1, "neural": 1, "network": 1}, {"however": 1, "require": 1, "actionvalue": 1, "function": 1, "quadratic": 1, "action": 1, "thus": 1, "possible": 1, "optimize": 1, "close": 1, "form": 1}, {"assumption": 1, "quite": 1, "restrictive": 1, "could": 1, "outperform": 1, "general": 1, "methods": 1, "ddpg": 1}, {"svg1": 1, "heess": 1, "et": 1, "al": 1, "2015": 1, "differentiable": 1, "transition": 1, "model": 1, "use": 1, "compute": 1, "pathwise": 1, "derivative": 1, "value": 1, "function": 1, "one": 1, "timestep": 1, "action": 1, "selection": 1}, {"similar": 1, "rtac": 1, "use": 1, "value": 1, "function": 1, "compute": 1, "policy": 1, "gradient": 1}, {"however": 1, "rtac": 1, "use": 1, "actual": 1, "differentiable": 1, "dynamics": 1, "rtmdp": 1, "ie": 1}, {"pass": 1, "action": 1, "next": 1, "state": 1, "therefore": 1, "need": 1, "approximate": 1, "transition": 1, "dynamics": 1}, {"time": 1, "transition": 1, "underlie": 2, "environment": 2, "model": 1, "instead": 1, "sample": 1, "possible": 1, "action": 1, "rtmdp": 1, "start": 1, "influence": 1, "next": 1, "timestep": 1}, {"7": 1, "": 2, "discussion": 1, "introduce": 1, "new": 1, "framework": 1, "reinforcement": 1, "learn": 1, "rtrl": 1, "agent": 1, "environment": 1, "step": 1, "unison": 1, "create": 1, "sequence": 1, "stateaction": 1, "pair": 1}, {"connect": 1, "rtrl": 1, "conventional": 1, "reinforcement": 1, "learn": 1, "framework": 1, "rtmdp": 1, "investigate": 1, "effect": 1, "theory": 1, "practice": 1}, {"predict": 1, "confirm": 1, "experimentally": 1, "conventional": 2, "offpolicy": 2, "algorithms": 1, "would": 1, "perform": 1, "worse": 1, "realtime": 2, "environments": 1, "propose": 1, "new": 1, "actorcritic": 1, "algorithm": 1, "rtac": 1, "avoid": 1, "problems": 1, "methods": 1, "interaction": 1, "also": 1, "allow": 1, "us": 1, "merge": 1, "actor": 1, "critic": 1, "come": 1, "additional": 1, "gain": 1, "performance": 1}, {"show": 1, "rtac": 1, "outperform": 1, "sac": 1, "standard": 1, "low": 1, "dimensional": 2, "continuous": 1, "control": 1, "benchmark": 1, "well": 1, "high": 1, "autonomous": 1, "drive": 1, "task": 1}, {"8": 1, "": 1, "acknowledgments": 1, "would": 1, "like": 1, "thank": 1, "cyril": 1, "ibrahim": 1, "build": 1, "help": 2, "us": 2, "avenue": 1, "simulator": 1, "craig": 1, "quiter": 1, "sherjil": 1, "ozair": 1, "insightful": 1, "discussions": 1, "agentenvironment": 1, "interaction": 1, "alex": 1, "pich": 1, "scott": 1, "fujimoto": 1, "bhairav": 1, "metha": 1, "jhelum": 1, "chakravorty": 1, "read": 1, "draft": 1, "paper": 1, "finally": 1, "jose": 1, "gallego": 1, "olexa": 1, "bilaniuk": 1, "many": 1, "others": 1, "mila": 1, "countless": 1, "occasion": 1, "online": 1, "offline": 1}, {"work": 1, "complete": 1, "parttime": 1, "internship": 1, "element": 1, "ai": 1, "support": 1, "open": 1, "philanthropy": 1, "project": 1}, {"reference": 1, "andrychowicz": 1, "marcin": 1, "wolski": 1, "filip": 1, "ray": 1, "alex": 1, "schneider": 1, "jonas": 1, "fong": 1, "rachel": 1, "welinder": 1, "peter": 1, "mcgrew": 1, "bob": 1, "tobin": 1, "josh": 1, "abbeel": 1, "openai": 1, "pieter": 1, "zaremba": 1, "wojciech": 1}, {"hindsight": 1, "experience": 1, "replay": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "pp": 1}, {"50485058": 1, "2017": 1}, {"bellman": 1, "richard": 1}, {"markovian": 1, "decision": 1, "process": 1}, {"journal": 1, "mathematics": 1, "mechanics": 1, "pp": 1}, {"679684": 1, "1957": 1}, {"brockman": 1, "greg": 1, "cheung": 1, "vicki": 1, "pettersson": 1, "ludwig": 1, "schneider": 1, "jonas": 1, "schulman": 1, "john": 1, "tang": 1, "jie": 1, "zaremba": 1, "wojciech": 1}, {"openai": 1, "gym": 1, "2016": 1}, {"firoiu": 1, "vlad": 1, "ju": 1, "tina": 1, "tenenbaum": 1, "joshua": 1, "b": 1}, {"human": 1, "speed": 1, "deep": 1, "reinforcement": 1, "learn": 1, "action": 1, "delay": 1}, {"corr": 1, "abs181007286": 1, "2018": 1}, {"fujimoto": 1, "scott": 1, "van": 1, "hoof": 1, "herke": 1, "meger": 1, "david": 1}, {"address": 1, "function": 1, "approximation": 1, "error": 1, "actorcritic": 1, "methods": 1}, {"arxiv": 1, "preprint": 1, "arxiv180209477": 1, "2018": 1}, {"gu": 1, "shixiang": 1, "lillicrap": 1, "timothy": 1, "sutskever": 1, "ilya": 1, "levine": 1, "sergey": 1}, {"continuous": 1, "deep": 1, "qlearning": 1, "modelbased": 1, "acceleration": 1}, {"international": 1, "conference": 1, "machine": 1, "learn": 1, "pp": 1}, {"28292838": 1, "2016": 1}, {"haarnoja": 1, "tuomas": 1, "zhou": 1, "aurick": 1, "abbeel": 1, "pieter": 1, "levine": 1, "sergey": 1}, {"soft": 1, "actorcritic": 1, "offpolicy": 1, "maximum": 1, "entropy": 1, "deep": 1, "reinforcement": 1, "learn": 1, "stochastic": 1, "actor": 1}, {"arxiv": 1, "preprint": 1, "arxiv180101290": 1, "2018a": 1}, {"haarnoja": 1, "tuomas": 1, "zhou": 1, "aurick": 1, "hartikainen": 1, "kristian": 1, "tucker": 1, "george": 1, "ha": 1, "sehoon": 1, "tan": 1, "jie": 1, "kumar": 1, "vikash": 1, "zhu": 1, "henry": 1, "gupta": 1, "abhishek": 1, "abbeel": 1, "pieter": 1, "et": 1, "al": 1}, {"soft": 1, "actorcritic": 1, "algorithms": 1, "applications": 1}, {"arxiv": 1, "preprint": 1, "arxiv181205905": 1, "2018b": 1}, {"heess": 1, "nicolas": 1, "wayne": 1, "gregory": 1, "silver": 1, "david": 1, "lillicrap": 1, "tim": 1, "erez": 1, "tom": 1, "tassa": 1, "yuval": 1}, {"learn": 1, "continuous": 1, "control": 1, "policies": 1, "stochastic": 1, "value": 1, "gradients": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "pp": 1}, {"29442952": 1, "2015": 1}, {"hwangbo": 1, "jemin": 1, "sa": 1, "inkyu": 1, "siegwart": 1, "roland": 1, "hutter": 1, "marco": 1}, {"control": 1, "quadrotor": 1, "reinforcement": 1, "learn": 1}, {"ieee": 1, "robotics": 1, "automation": 1, "letter": 1, "2420962103": 1, "2017": 1}, {"hwangbo": 1, "jemin": 1, "lee": 1, "joonho": 1, "dosovitskiy": 1, "alexey": 1, "bellicoso": 1, "dario": 1, "tsounis": 1, "vassilios": 1, "koltun": 1, "vladlen": 1, "hutter": 1, "marco": 1}, {"learn": 1, "agile": 1, "dynamic": 1, "motor": 1, "skills": 1, "legged": 1, "robots": 1}, {"science": 1, "robotics": 1, "426eaau5872": 1, "2019": 1}, {"ibrahim": 1, "cyril": 1, "ramstedt": 1, "simon": 1, "pal": 1, "christopher": 1}, {"elementaiavenue": 1, "2019": 1}, {"avenue": 1}, {"httpsgithubcom": 1, "": 1, "kingma": 1, "diederik": 1, "p": 1, "ba": 1, "jimmy": 1}, {"adam": 1, "method": 1, "stochastic": 1, "optimization": 1}, {"arxiv": 1, "preprint": 1, "arxiv14126980": 1, "2014": 1}, {"konda": 1, "vijay": 1, "r": 1, "tsitsiklis": 1, "john": 1, "n": 1, "actorcritic": 1, "algorithms": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "pp": 1}, {"10081014": 1, "2000": 1}, {"lillicrap": 1, "timothy": 1, "p": 1, "hunt": 1, "jonathan": 1, "j": 1, "pritzel": 1, "alexander": 1, "heess": 1, "nicolas": 1, "erez": 1, "tom": 1, "tassa": 1, "yuval": 1, "silver": 1, "david": 1, "wierstra": 1, "daan": 1}, {"continuous": 1, "control": 1, "deep": 1, "reinforcement": 1, "learn": 1}, {"arxiv": 1, "preprint": 1, "arxiv150902971": 1, "2015": 1}, {"9": 1, "": 1, "mnih": 1, "volodymyr": 1, "kavukcuoglu": 1, "koray": 1, "silver": 1, "david": 1, "rusu": 1, "andrei": 1, "veness": 1, "joel": 1, "bellemare": 1, "marc": 1, "g": 1, "grave": 1, "alex": 1, "riedmiller": 1, "martin": 1, "fidjeland": 1, "andreas": 1, "k": 1, "ostrovski": 1, "georg": 1, "et": 1, "al": 1}, {"humanlevel": 1, "control": 1, "deep": 1, "reinforcement": 1, "learn": 1}, {"nature": 1, "5187540529": 1, "2015": 1}, {"schulman": 1, "john": 1, "levine": 1, "sergey": 1, "abbeel": 1, "pieter": 1, "jordan": 1, "michael": 1, "moritz": 1, "philipp": 1}, {"trust": 1, "region": 1, "policy": 1, "optimization": 1}, {"international": 1, "conference": 1, "machine": 1, "learn": 1, "pp": 1}, {"18891897": 1, "2015": 1}, {"silver": 1, "david": 1, "lever": 1, "guy": 1, "heess": 1, "nicolas": 1, "degris": 1, "thomas": 1, "wierstra": 1, "daan": 1, "riedmiller": 1, "martin": 1}, {"deterministic": 1, "policy": 1, "gradient": 1, "algorithms": 1}, {"icml": 1, "2014": 1}, {"silver": 1, "david": 1, "schrittwieser": 1, "julian": 1, "simonyan": 1, "karen": 1, "antonoglou": 1, "ioannis": 1, "huang": 1, "aja": 1, "guez": 1, "arthur": 1, "hubert": 1, "thomas": 1, "baker": 1, "lucas": 1, "lai": 1, "matthew": 1, "bolton": 1, "adrian": 1, "et": 1, "al": 1}, {"master": 1, "game": 1, "go": 1, "without": 1, "human": 1, "knowledge": 1}, {"nature": 1, "5507676354": 1, "2017": 1}, {"tallec": 1, "corentin": 1, "blier": 1, "lonard": 1, "ollivier": 1, "yann": 1}, {"make": 1, "deep": 1, "qlearning": 1, "methods": 1, "robust": 1, "time": 1, "discretization": 1}, {"arxiv": 1, "preprint": 1, "arxiv190109732": 1, "2019": 1}, {"tesauro": 1, "gerald": 1}, {"tdgammon": 1, "selfteaching": 1, "backgammon": 1, "program": 1, "achieve": 1, "masterlevel": 1, "play": 1}, {"neural": 1, "computation": 1, "62215219": 1, "1994": 1}, {"todorov": 1, "emanuel": 1, "erez": 1, "tom": 1, "tassa": 1, "yuval": 1}, {"mujoco": 1, "physics": 1, "engine": 1, "modelbased": 1, "control": 1}, {"iros": 1, "pp": 1}, {"50265033": 1}, {"ieee": 1, "2012": 1}, {"isbn": 1, "9781467317375": 1}, {"url": 1, "httpdblp": 1}, {"unitrierdedbconfirosiros2012htmltodorovet12": 1}, {"travnik": 1, "jaden": 1, "b": 1, "mathewson": 1, "kory": 1, "w": 1, "sutton": 1, "richard": 1, "pilarski": 1, "patrick": 1, "reactive": 1, "reinforcement": 1, "learn": 1, "asynchronous": 1, "environments": 1}, {"frontiers": 1, "robotics": 1, "ai": 1, "579": 1, "2018": 1}, {"issn": 1, "22969144": 1, "doi": 1, "103389frobt201800079": 1}, {"url": 1, "httpswwwfrontiersinorg": 1, "article103389frobt201800079": 1}, {"van": 1, "hasselt": 1, "hado": 1, "p": 1, "guez": 1, "arthur": 1, "hessel": 1, "matteo": 1, "mnih": 1, "volodymyr": 1, "silver": 1, "david": 1}, {"learn": 1, "value": 1, "across": 1, "many": 1, "order": 1, "magnitude": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "pp": 1}, {"42874295": 1, "2016": 1}, {"walsh": 1, "thomas": 1, "j": 1, "nouri": 1, "ali": 1, "li": 1, "lihong": 1, "littman": 1, "michael": 1, "l": 1, "learn": 1, "plan": 1, "environments": 1, "delay": 1, "feedback": 1}, {"autonomous": 1, "agents": 1, "multiagent": 1, "systems": 1, "1883105": 1, "2008": 1}, {"10": 1}]
[{"regularize": 2, "approach": 1, "sparse": 1, "optimal": 2, "policy": 3, "reinforcement": 1, "learn": 1, "": 4, "xiang": 1, "li": 1, "school": 2, "mathematical": 2, "sciences": 2, "peking": 3, "university": 3, "beijing": 3, "china": 3, "lx10077pkueducn": 1, "wenhao": 1, "yang": 1, "center": 1, "data": 2, "science": 1, "yangwenhaosmspkueducn": 1, "zhihua": 1, "zhang": 1, "national": 1, "engineer": 1, "lab": 1, "big": 1, "analysis": 1, "applications": 1, "zhzhangmathpkueducn": 1, "abstract": 1, "propose": 1, "study": 1, "general": 1, "framework": 1, "markov": 1, "decision": 1, "process": 1, "mdps": 1, "goal": 1, "find": 1, "maximize": 1, "expect": 1, "discount": 1, "total": 1, "reward": 1, "plus": 1, "regularization": 1, "term": 1}, {"extant": 1, "entropyregularized": 1, "mdps": 1, "cast": 1, "framework": 1}, {"moreover": 1, "framework": 1, "many": 1, "regularization": 1, "term": 1, "bring": 1, "multimodality": 1, "sparsity": 1, "potentially": 1, "useful": 1, "reinforcement": 1, "learn": 1}, {"particular": 1, "present": 1, "sufficient": 1, "necessary": 1, "condition": 1, "induce": 1, "sparse": 1, "optimal": 1, "policy": 1}, {"also": 1, "conduct": 1, "full": 1, "mathematical": 1, "analysis": 1, "propose": 1, "regularize": 1, "mdps": 1, "include": 1, "optimality": 1, "condition": 1, "performance": 1, "error": 1, "sparseness": 1, "control": 1}, {"provide": 1, "generic": 1, "method": 1, "devise": 1, "regularization": 1, "form": 1, "propose": 1, "offpolicy": 1, "actor": 1, "critic": 1, "algorithms": 1, "complex": 1, "environment": 1, "settings": 1}, {"empirically": 1, "analyze": 1, "numerical": 1, "properties": 1, "optimal": 1, "policies": 1, "compare": 1, "performance": 1, "different": 1, "sparse": 1, "regularization": 1, "form": 1, "discrete": 1, "continuous": 1, "environments": 1}, {"1": 1, "": 2, "introduction": 1, "reinforcement": 1, "learn": 1, "rl": 1, "aim": 1, "find": 1, "optimal": 1, "policy": 1, "maximize": 1, "expect": 1, "discount": 1, "total": 1, "reward": 1, "mdp": 1, "4": 1, "36": 1}, {"easy": 1, "task": 1, "solve": 1, "nonlinear": 1, "bellman": 1, "equation": 1, "2": 1, "greedily": 1, "highdimension": 1, "action": 1, "space": 1, "function": 1, "approximation": 1, "neural": 1, "network": 1, "use": 1}, {"even": 1, "optimal": 2, "policy": 2, "obtain": 1, "precisely": 1, "often": 1, "case": 1, "deterministic": 1}, {"deterministic": 1, "policies": 1, "bad": 1, "cope": 1, "unexpected": 1, "situations": 1, "suggest": 1, "action": 1, "suddenly": 1, "unavailable": 1, "forbid": 1}, {"contrast": 1, "multimodal": 1, "policy": 1, "assign": 1, "positive": 1, "probability": 1, "mass": 1, "optimal": 2, "near": 1, "action": 1, "hence": 1, "multiple": 1, "alternatives": 1, "handle": 1, "case": 1}, {"example": 1, "autonomous": 1, "vehicle": 1, "aim": 1, "path": 1, "plan": 1, "pair": 1, "departure": 1, "destination": 1, "state": 1}, {"suggest": 1, "routine": 2, "unfortunately": 1, "congest": 1, "alternative": 1, "could": 1, "provide": 2, "multimodal": 1, "policy": 2, "cant": 1, "deterministic": 1, "without": 1, "evoke": 1, "new": 1, "computation": 1}, {"therefore": 1, "reallife": 1, "application": 1, "hope": 1, "optimal": 1, "policy": 1, "possess": 1, "thee": 1, "property": 1, "multimodality": 1}, {"": 2, "equal": 1, "contribution": 1}, {"33rd": 1, "conference": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "neurips": 1, "2019": 1, "vancouver": 1, "canada": 1}, {"entropyregularized": 1, "rl": 1, "methods": 1, "propose": 1, "handle": 1, "issue": 1}, {"specifically": 1, "entropy": 1, "bonus": 1, "term": 1, "add": 1, "expect": 1, "longterm": 1, "return": 1}, {"result": 1, "soften": 1, "nonlinearity": 1, "original": 1, "bellman": 1, "equation": 1, "also": 1, "force": 1, "optimal": 1, "policy": 1, "stochastic": 1, "desirable": 1, "problems": 1, "deal": 1, "unexpected": 1, "situations": 1, "crucial": 1}, {"prior": 1, "work": 1, "shannon": 1, "entropy": 1, "usually": 1, "use": 1}, {"optimal": 1, "policy": 1, "form": 1, "softmax": 1, "show": 1, "encourage": 1, "exploration": 1, "8": 1, "40": 1}, {"however": 1, "softmax": 1, "policy": 2, "assign": 1, "nonnegligible": 1, "probability": 1, "mass": 1, "action": 1, "include": 1, "really": 1, "terrible": 1, "dismissible": 1, "ones": 1, "may": 1, "result": 1, "unsafe": 1}, {"rl": 1, "problems": 1, "high": 1, "dimensional": 1, "action": 4, "space": 1, "sparse": 1, "distribution": 1, "prefer": 1, "model": 1, "policy": 1, "function": 1, "implicitly": 1, "filtration": 1, "ie": 1, "weed": 1, "suboptimal": 1, "maintain": 1, "near": 1, "optimal": 1}, {"thus": 1, "lee": 1, "et": 1, "al": 1}, {"19": 1, "propose": 1, "use": 1, "tsallis": 1, "entropy": 1, "39": 1, "instead": 1, "give": 1, "rise": 1, "sparse": 1, "mdp": 1, "action": 1, "nonzero": 1, "probability": 1, "state": 1, "optimal": 1, "policy": 1}, {"lee": 1, "et": 1, "al": 1}, {"20": 1, "empirically": 1, "show": 1, "general": 1, "tsallis": 1, "entropy2": 1, "also": 1, "lead": 1, "sparse": 1, "mdp": 1}, {"moreover": 1, "tsallis": 2, "regularize": 3, "rl": 3, "lower": 1, "performance": 1, "error": 1, "ie": 1, "optimal": 2, "value": 2, "closer": 1, "original": 1, "shannon": 1}, {"discussions": 1, "manifest": 1, "entropy": 1, "regularization": 1, "characterize": 1, "solution": 1, "correspond": 1, "regularize": 1, "rl": 1}, {"neu": 1, "et": 1, "al": 1}, {"28": 1, "entropyregularized": 1, "mdp": 1, "view": 1, "regularize": 1, "convex": 1, "optimization": 1, "problem": 1, "entropy": 1, "serve": 1, "regularizer": 1, "decision": 1, "variable": 1, "stationary": 1, "policy": 1}, {"geist": 1, "et": 1, "al": 1}, {"10": 1, "propose": 1, "framework": 1, "mdp": 1, "regularize": 1, "general": 1, "strongly": 1, "concave": 1, "function": 1}, {"analyze": 1, "variants": 1, "classic": 1, "algorithms": 1, "framework": 1, "provide": 1, "insight": 1, "choice": 1, "regularizers": 1}, {"hand": 1, "sparse": 1, "optimal": 1, "policy": 1, "distribution": 1, "favor": 1, "large": 1, "action": 1, "space": 1, "rl": 1, "problems": 1}, {"prior": 1, "work": 1, "lee": 1, "et": 1, "al": 1}, {"19": 1, "nachum": 1, "et": 1, "al": 1}, {"27": 1, "obtain": 1, "sparse": 1, "optimal": 1, "policy": 1, "tsallis": 1, "entropy": 1, "regularization": 1}, {"consider": 1, "diversity": 1, "generality": 1, "regularization": 1, "form": 1, "convex": 1, "optimization": 1, "natural": 1, "ask": 1, "whether": 1, "regularizations": 1, "lead": 1, "sparseness": 1}, {"answer": 1, "exist": 1, "regularizers": 1, "induce": 1, "sparsity": 1}, {"paper": 1, "propose": 1, "framework": 1, "regularize": 1, "mdps": 1, "general": 1, "form": 1, "regularizers": 1, "impose": 1, "expect": 1, "discount": 1, "total": 1, "reward": 1}, {"framework": 1, "include": 1, "entropy": 1, "regularize": 1, "mdp": 1, "special": 1, "case": 1, "imply": 1, "certain": 1, "regularizers": 1, "induce": 1, "sparseness": 1}, {"first": 1, "give": 2, "optimality": 1, "condition": 2, "regularize": 1, "mdps": 1, "framework": 1, "necessary": 1, "sufficient": 1, "show": 1, "kind": 1, "regularization": 1, "lead": 1, "sparse": 1, "optimal": 1, "policy": 1}, {"interestingly": 1, "lot": 1, "regularization": 2, "lead": 1, "sparseness": 2, "degree": 1, "control": 1, "coefficient": 1}, {"furthermore": 1, "show": 1, "regularize": 1, "mdps": 1, "regularizationdependent": 1, "performance": 1, "error": 1, "cause": 1, "regularization": 2, "term": 1, "could": 1, "guide": 1, "us": 1, "choose": 1, "effective": 1, "come": 1, "deal": 1, "problems": 1, "continuous": 1, "action": 1, "space": 1}, {"solve": 1, "regularize": 1, "mdps": 1, "employ": 1, "idea": 1, "generalize": 1, "policy": 1, "iteration": 1, "propose": 1, "offpolicy": 1, "actorcritic": 1, "algorithm": 1, "figure": 1, "performance": 1, "different": 1, "regularizers": 1}, {"2": 1, "": 2, "notation": 1, "preliminaries": 1, "markov": 2, "decision": 2, "process": 2, "reinforcement": 1, "learn": 1, "rl": 1, "problems": 1, "agents": 1, "interaction": 1, "environment": 1, "often": 1, "model": 1, "mdp": 1}, {"mdp": 1, "define": 1, "tuple": 1, "p": 1, "r": 1, "p0": 1, "": 2, "state": 1, "space": 2, "action": 2}, {"use": 1, "x": 6, "denote": 1, "p": 4, "simplex": 1, "set": 2, "": 6, "define": 1, "distributions": 1, "ie": 1, "xx": 1, "1": 1, "0": 1}, {"vertex": 1, "set": 1, "x": 4, "define": 1, "vx": 1, "": 6, "p": 1, "st": 1}, {"p": 1, "x": 1, "": 1, "1": 1}, {"p": 1, "": 7, "unknown": 1, "state": 1, "transition": 2, "probability": 1, "distribution": 1, "r": 1, "0": 1, "rmax": 1, "bound": 1, "reward": 1}, {"p0": 1, "distribution": 1, "initial": 1, "state": 1, "": 2, "0": 1, "1": 1, "discount": 1, "factor": 1}, {"optimality": 1, "condition": 1, "mdp": 1, "": 15, "goal": 1, "rl": 1, "find": 1, "stationary": 1, "policy": 1, "map": 1, "state": 1, "space": 1, "simplex": 1, "action": 1, "maximize": 1, "expect": 1, "discount": 1, "total": 1, "reward": 1, "ie": 1, "x": 1, "max": 1, "e": 1, "rst": 1, "p0": 1, "1": 1, "t0": 1, "2": 1, "general": 1, "tsallis": 1, "entropy": 1, "define": 1, "additional": 1, "realvalued": 1, "parameter": 1, "call": 1, "entropic": 1, "index": 1}, {"lee": 1, "et": 1, "al": 1}, {"20": 1, "show": 1, "entropic": 1, "index": 1, "large": 1, "enough": 1, "optimal": 1, "policy": 1, "sparse": 1}, {"2": 1, "": 8, "s0": 1, "p0": 1, "st": 1, "st1": 1, "pst": 1}, {"give": 1, "policy": 1, "": 18, "state": 1, "value": 1, "qvalue": 1, "function": 1, "define": 1, "respectively": 1, "x": 1, "v": 2, "e": 1, "rst": 1, "s0": 2, "t0": 1, "q": 1, "eas": 1, "rs": 1, "es0": 1, "sa": 1}, {"": 5, "solution": 1, "problem": 1, "1": 1, "call": 1, "optimal": 1, "policy": 1, "denote": 1}, {"optimal": 2, "policies": 1, "may": 1, "unique": 2, "mdp": 1, "state": 1, "value": 1, "denote": 1, "v": 1, "": 2}, {"actually": 1, "v": 5, "": 11, "unique": 1, "fix": 1, "point": 1, "bellman": 1, "operator": 1, "ie": 1, "max": 1, "eas": 1, "rs": 1, "es0": 1, "sa": 1, "s0": 1}, {"": 4, "often": 1, "deterministic": 1, "policy": 1, "put": 1, "probability": 1, "mass": 1, "one": 1, "action31": 1}, {"actually": 1, "obtain": 1, "greedy": 1, "action": 1, "wrt": 1}, {"optimal": 1, "qvalue": 1, "function": 1, "ie": 1, "": 4, "argmaxa": 1, "q": 1, "3": 1}, {"optimal": 1, "qvalue": 1, "obtain": 1, "state": 1, "value": 1, "v": 1, "": 1, "definition": 1}, {"summary": 1, "optimal": 2, "policy": 1, "": 11, "state": 2, "value": 1, "v": 3, "qvalue": 1, "q": 4, "satisfy": 1, "follow": 1, "optimality": 1, "condition": 1, "action": 1, "rs": 1, "es0": 1, "sa": 1, "max": 1, "argmax": 1}, {"": 17, "3": 1, "regularize": 2, "mdps": 1, "obtain": 1, "sparse": 1, "multimodal": 1, "optimal": 1, "policy": 1, "impose": 1, "general": 1, "regularization": 2, "term": 1, "objective": 1, "1": 1, "solve": 1, "follow": 1, "mdp": 1, "problem": 1, "x": 1, "max": 1, "e": 1, "rst": 1, "st": 1, "p0": 1, "2": 1, "t0": 1, "function": 1}, {"problem": 2, "2": 1, "see": 1, "rl": 1, "reward": 2, "function": 2, "sum": 1, "original": 1, "rs": 1, "term": 1, "provide": 1, "regularization": 1}, {"take": 1, "expectation": 1, "regularization": 1, "term": 1, "find": 1, "quantity": 1, "h": 1, "": 2, "eas": 1, "3": 1, "entropylike": 1, "necessarily": 1, "entropy": 1, "work": 1}, {"however": 1, "problem": 1, "2": 1, "welldefined": 1, "since": 1, "arbitrary": 1, "regularizers": 1, "would": 1, "hindrance": 1, "help": 1}, {"follow": 1, "make": 1, "assumptions": 1, "": 1}, {"31": 1, "": 3, "assumption": 1, "regularizers": 1, "regularizer": 1, "characterize": 1, "solutions": 1, "problem": 1, "2": 1}, {"order": 1, "make": 1, "problems": 1, "2": 1, "analyzable": 1, "basic": 1, "assumption": 2, "1": 1, "prerequisite": 1}, {"explanation": 1, "examples": 1, "provide": 1, "show": 1, "assumption": 1, "reasonable": 1, "strict": 1}, {"assumption": 1, "1": 4, "regularizer": 1, "x": 3, "assume": 1, "satisfy": 1, "follow": 1, "condition": 1, "interval": 1, "0": 2, "monotonicity": 1, "nonincreasing": 1, "2": 1, "nonnegativity": 1, "": 1, "3": 1, "differentiability": 1, "differentiable": 1, "4": 1, "expect": 1, "concavity": 1, "xx": 1, "strictly": 1, "concave": 1}, {"assumptions": 1, "monotonicity": 1, "nonnegativity": 1, "make": 1, "regularizer": 1, "positive": 1, "exploration": 1, "bonus": 1}, {"bonus": 1, "choose": 2, "action": 2, "high": 1, "probability": 2, "less": 1, "low": 1}, {"policy": 1, "become": 1, "deterministic": 1, "bonus": 1, "force": 1, "zero": 1}, {"assumption": 1, "differentiability": 1, "facilitate": 1, "theoretic": 1, "analysis": 1, "benefit": 1, "practical": 1, "implementation": 1, "due": 1, "widely": 1, "use": 1, "automatic": 1, "derivation": 1, "deep": 1, "learn": 1, "platforms": 1}, {"last": 1, "assumption": 1, "expect": 1, "concavity": 1, "make": 1, "h": 1, "": 1, "concave": 1, "function": 1, "wrt": 1}, {"": 1}, {"thus": 1, "solution": 1, "eqn": 1}, {"2": 1, "hardly": 1, "lie": 1, "vertex": 1, "set": 1, "": 2, "necessarily": 1, "deterministic": 1}, {"two": 1, "action": 1, "a1": 2, "": 6, "a2": 2, "obtain": 1, "maximum": 1, "v": 1, "fix": 1, "one": 1, "show": 1, "stochastic": 1, "policy": 1, "1": 2, "p": 1, "0": 1, "also": 1, "optimal": 1}, {"3": 2, "": 2, "action": 1, "simplex": 1, "va": 1, "policy": 1, "deterministic": 1}, {"byproduct": 1, "let": 1, "f": 1, "x": 1, "": 1, "xx": 1}, {"derivative": 1, "f0": 2, "x": 4, "": 2, "x0": 1, "strictly": 1, "decrease": 1, "function": 1, "0": 1, "1": 2, "thus": 1, "exist": 1}, {"simplicity": 1, "denote": 1, "g": 1, "x": 2, "": 1, "f0": 1, "1": 1}, {"plenty": 1, "options": 1, "regularizer": 1, "": 1, "satisfy": 1, "assumption": 1, "1": 1}, {"first": 1, "entropy": 1, "recover": 1, "h": 1, "": 2, "specific": 1}, {"example": 1, "x": 3, "": 5, "log": 1, "shannon": 1, "entropy": 2, "k": 2, "recover": 2, "q1": 1, "1xq1": 1, "0": 1, "tsallis": 1}, {"second": 1, "many": 1, "instance": 1, "view": 1, "entropy": 1, "serve": 1, "regularizer": 1}, {"find": 1, "two": 1, "families": 1, "function": 3, "namely": 1, "exponential": 1, "family": 2, "q": 3, "": 8, "xk": 1, "x": 1, "k": 1, "0": 2, "1": 1, "trigonometric": 1, "cosx": 1, "cos": 1, "sin": 1, "sinx": 1, "hyperparameter": 1, "2": 1}, {"since": 1, "function": 2, "simple": 1, "term": 1, "basic": 1}, {"apart": 1, "basic": 2, "function": 2, "mention": 1, "earlier": 1, "come": 1, "generic": 1, "method": 1, "combine": 1, "different": 1}, {"let": 1, "f": 1, "set": 1, "function": 1, "satisfy": 1, "assumption": 1, "1": 1}, {"proposition": 1, "1": 1, "operations": 2, "positive": 1, "addition": 1, "minimum": 1, "preserve": 1, "properties": 1, "share": 1, "among": 1, "f": 1, "therefore": 1, "finitetime": 1, "application": 1, "still": 1, "lead": 1, "available": 1, "regularizer": 1}, {"proposition": 1, "1": 3, "give": 1, "": 12, "2": 4, "f": 4, "0": 1, "min1": 2, "consider": 1, "differentiable": 1, "theoretical": 1, "analysis": 1, "minimum": 1, "two": 1, "function": 1, "may": 1, "nondifferentiable": 1, "point": 1}, {"instance": 1, "give": 1, "q": 1, "": 3, "1": 2, "minimum": 1, "logx": 1, "q1": 1, "x": 1, "unique": 1, "nondifferentiable": 1, "point": 1, "0": 1}, {"32": 1, "": 18, "optimality": 1, "sparsity": 1, "regularizer": 1, "give": 2, "similar": 1, "nonregularized": 1, "case": 1, "regularize": 2, "state": 1, "value": 1, "qvalue": 1, "function": 1, "policy": 1, "mdp": 1, "define": 1, "hx": 1, "v": 2, "e": 1, "rst": 1, "st": 1, "s0": 2, "t0": 1, "q": 1, "rs": 1, "eas": 1, "es0": 1, "sa": 1}, {"4": 1, "": 3, "solution": 1, "problem": 1, "2": 1, "call": 1, "regularize": 1, "optimal": 1, "policy": 1, "denote": 1}, {"correspond": 1, "regularize": 1, "optimal": 2, "state": 1, "value": 1, "qvalue": 1, "also": 1, "denote": 1, "v": 1, "q": 1, "respectively": 1}, {"context": 1, "clear": 1, "omit": 1, "word": 1, "regularize": 1, "simplicity": 1}, {"part": 1, "aim": 1, "show": 1, "optimality": 1, "condition": 1, "regularize": 1, "mdps": 1, "theorem": 1, "1": 1}, {"proof": 1, "theorem": 3, "1": 5, "give": 1, "appendix": 1, "b": 1, "optimal": 2, "policy": 1, "": 32, "state": 2, "value": 1, "v": 3, "qvalue": 1, "q": 3, "satisfy": 1, "follow": 1, "optimality": 2, "condition": 2, "action": 1, "rs": 1, "es0": 1, "sa": 1, "s0": 1, "max": 1, "g": 2, "0": 2, "x": 3, "as2": 1, "5": 1, "f0": 1, "p": 1, "aa": 1, "strictly": 1, "decrease": 1, "normalization": 1, "term": 1, "show": 1, "regularization": 1, "influence": 1}, {"let": 1, "f0": 2, "0": 1, "": 2, "lim": 1, "x": 1, "x0": 1, "short": 1}, {"5": 1, "show": 1, "optimal": 2, "policy": 1, "": 3, "assign": 2, "zero": 1, "probability": 2, "action": 2, "whose": 1, "qvalues": 2, "q": 1, "threshold": 1, "f0": 1, "0": 1, "positive": 1, "near": 1, "proportion": 1, "since": 1, "g": 1, "x": 1, "decrease": 1}, {"threshold": 1, "involve": 1, "": 1, "f0": 1, "0": 1}, {"": 1, "uniquely": 1, "solve": 1, "equation": 1, "obtain": 1, "plug": 1, "eqn": 1}, {"5": 1, "p": 1, "": 5, "aa": 1, "1": 1}, {"note": 1, "result": 1, "equation": 1, "involve": 1, "q": 1, "aaa": 1, "": 1}, {"thus": 1, "": 3, "actually": 1, "always": 1, "multivariate": 1, "finitevalued": 1, "function": 1, "q": 1, "aaa": 1}, {"however": 1, "value": 1, "f0": 1, "0": 1, "infinity": 1, "make": 1, "threshold": 1, "function": 1}, {"see": 1, "f0": 1, "0": 1, "": 4, "threshold": 1, "4": 1, "action": 1, "assign": 1, "positive": 1, "probability": 1, "optimal": 1, "policy": 1}, {"characterize": 1, "number": 1, "zero": 1, "probability": 1, "action": 1, "define": 1, "sparse": 1, "policy": 1, "definition": 1, "1": 1, "show": 1}, {"trivial": 1, "1": 3, "": 3}, {"instance": 1, "deterministic": 1, "optimal": 1, "policy": 1, "nonregularized": 1, "mdp": 1, "sparse": 1}, {"definition": 1, "1": 1, "give": 1, "policy": 1, "": 7, "call": 1, "sparse": 1, "satisfy": 1, "aas": 1, "6": 1, "0": 1}, {"sa": 1, "": 5, "6": 1, "0": 1, "call": 1, "sparsity": 1}, {"theorem": 1, "2": 1, "lim": 1, "f0": 1, "x": 1, "": 3, "0": 1, "6": 1, "domf0": 1, "optimal": 1, "policy": 1, "regularize": 1, "mdp": 1, "x0": 1, "sparse": 1}, {"theorem": 1, "2": 1, "provide": 1, "us": 1, "criteria": 1, "determine": 1, "whether": 1, "regularization": 1, "could": 1, "render": 1, "correspond": 1, "regularize": 1, "optimal": 1, "policy": 1, "property": 1, "sparseness": 1}, {"facilitate": 1, "understand": 1, "let": 1, "us": 1, "see": 1, "two": 1, "examples": 1}, {"x": 2, "": 9, "logx": 2, "lim": 2, "f0": 1, "1": 1, "x0": 2, "imply": 1, "optimal": 1, "policy": 1, "shannon": 1, "entropyregularized": 1, "mdp": 1, "sparsity": 1}, {"k": 2, "x": 2, "": 8, "q1": 2, "1": 2, "xq1": 1, "q": 1, "small": 2, "enough": 2, "correspond": 1, "optimal": 1, "policy": 1, "spare": 1, "lim": 1, "f0": 1}, {"whats": 1, "sparseness": 1, "property": 1, "x0": 1, "": 5, "tsallis": 1, "entropy": 1, "still": 1, "keep": 1, "1": 1, "q": 1, "small": 1, "empirically": 1, "prove": 1, "true": 1, "20": 1}, {"additionally": 1, "0": 1, "": 6, "q": 1, "1": 2, "tsallis": 1, "entropy": 1, "could": 1, "longer": 1, "lead": 1, "sparseness": 1, "due": 1, "k": 1, "lim": 2, "f0": 1, "x": 1, "1q": 1, "qxq1": 1}, {"x0": 2, "": 4, "sparseness": 1, "property": 1, "first": 1, "discuss": 1, "19": 1, "show": 1, "tsallis": 1, "entropy": 1, "k": 1, "12": 1, "q": 1, "2": 1, "devise": 1, "sparse": 1, "mdp": 1}, {"however": 1, "point": 1, "": 4, "f0": 1, "0": 1, "proper": 1, "lead": 1, "sparse": 1, "mdp": 1}, {"let": 1, "g": 2, "": 5, "f": 1, "set": 1, "satisfy": 1, "0": 1, "domf0": 1}, {"positive": 1, "combination": 1, "two": 1, "regularizers": 1, "belong": 2, "g": 4, "still": 1, "proposition": 1, "2": 3, "give": 1, "1": 2, "": 7, "0": 1}, {"however": 1, "1": 2, "": 7, "g": 3, "2": 2, "positive": 1}, {"easily": 1, "check": 1, "two": 1, "families": 1, "ie": 1, "exponential": 1, "function": 2, "trigonometric": 1, "give": 1, "section": 1, "31": 1, "also": 1, "induce": 1, "sparse": 1, "mdp": 1, "proper": 1, "": 1}, {"convenience": 1, "prefer": 1, "k": 1, "term": 1, "function": 3, "x": 1, "": 3, "q1": 2, "1xq1": 1, "define": 1, "tsallis": 1, "entropy": 1, "polynomial": 2, "q": 1, "1": 1, "degree": 1}, {"additionally": 1, "three": 1, "basic": 1, "families": 1, "function": 1, "could": 1, "combine": 1, "construct": 1, "complex": 1, "regularizers": 1, "proposition": 1, "1": 1, "2": 1}, {"control": 1, "sparsity": 1, "optimal": 1, "policy": 1}, {"theorem": 1, "2": 1, "show": 1, "0": 1, "": 2, "domf0": 1, "necessary": 1, "sufficient": 1, "optimal": 1, "policy": 1, "sparse": 1}, {"sparsity": 2, "optimal": 2, "policy": 2, "also": 1, "control": 2, "": 4, "theorem": 1, "3": 1, "show": 1, "f0": 1, "0": 1}, {"proof": 1, "detail": 1, "appendix": 1, "e": 1, "theorem": 2, "3": 1, "let": 1, "q": 1, "": 3, "define": 1, "1": 1, "assume": 1, "f0": 1, "0": 1}, {"": 6, "0": 1, "1": 1, "sparsity": 1, "optimal": 1, "policy": 1, "shrink": 1}, {"": 4, "optimal": 1, "policy": 1, "1": 1, "sparsity": 1}, {"specifically": 1, "": 7}, {"33": 1, "": 2, "properties": 2, "regularize": 2, "mdps": 2, "section": 1, "present": 1}, {"first": 1, "prove": 1, "uniqueness": 1, "optimal": 1, "policy": 1, "value": 1}, {"next": 1, "give": 1, "bind": 1, "performance": 1, "error": 1, "": 3, "optimal": 1, "policy": 2, "obtain": 2, "regularize": 1, "mdp": 2, "original": 1}, {"proof": 1, "section": 1, "need": 1, "additional": 1, "assumption": 1, "regularizers": 1}, {"assumption": 1, "2": 1, "quite": 1, "weak": 1}, {"function": 1, "introduce": 1, "section": 1, "31": 1, "satisfy": 1}, {"assumption": 1, "2": 1, "regularizer": 1, "": 5, "satisfy": 1, "f": 1, "0": 2, "lim": 1, "xx": 1, "x0": 1, "5": 1, "generic": 1, "bellman": 1, "operator": 2, "define": 2, "new": 1, "regularize": 1, "mdps": 1, "smooth": 1, "maximum": 1}, {"give": 1, "one": 1, "state": 1, "": 11, "current": 1, "value": 1, "function": 2, "v": 4, "define": 1, "x": 1, "max": 1, "q": 2, "aas": 1, "7": 1, "rs": 1, "es0": 1, "sa": 1, "s0": 1, "qvalue": 1, "derive": 1, "onestep": 1, "foresee": 1, "accord": 1}, {"definition": 1, "map": 1, "v": 1, "possible": 1, "highest": 1, "value": 1, "consider": 1, "future": 1, "discount": 1, "reward": 1, "regularization": 1, "term": 1}, {"provide": 1, "simple": 1, "upper": 1, "lower": 1, "bound": 1, "wrt": 1}, {"": 7, "ie": 1, "theorem": 1, "4": 1, "assumptions": 1, "1": 2, "2": 1, "value": 1, "function": 1, "v": 4}, {"": 3, "8": 2, "bind": 1, "show": 1, "bound": 1, "smooth": 1, "approximation": 1}, {"": 3, "0": 1, "degenerate": 1, "bellman": 1, "operator": 1}, {"moreover": 1, "prove": 1, "contraction": 1}, {"banach": 1, "fix": 2, "point": 2, "theorem": 1, "35": 1, "v": 1, "": 2, "unique": 1}, {"result": 1, "theorem": 1, "1": 1, "q": 1, "": 1, "unique": 1}, {"formally": 1, "state": 1, "conclusion": 1, "give": 1, "proof": 1, "appendix": 1, "c": 1, "performance": 1, "error": 1, "v": 4, "": 3, "general": 1, "6": 1}, {"difference": 1, "control": 1, "": 2}, {"behavior": 1, "x": 2, "around": 1, "origin": 1, "represent": 1, "regularization": 1, "ability": 1, "1": 1}, {"theorem": 1, "5": 1, "show": 1, "quite": 1, "large": 1, "mean": 1, "": 2, "close": 1, "0": 3, "due": 1, "continuity": 1, "closeness": 1, "also": 1, "determine": 1, "difference": 1}, {"result": 1, "tsallis": 1, "entropy": 2, "regularize": 2, "mdps": 2, "always": 1, "tighter": 1, "error": 1, "bound": 1, "shannon": 1, "k": 1, "value": 1, "origin": 1, "concave": 1, "function": 2, "q1": 1, "1": 2, "": 3, "xq1": 1, "q": 1, "much": 1, "lower": 1, "log": 1, "x": 1, "satisfy": 1, "assumption": 1, "2": 1}, {"theory": 1, "incorporate": 1, "result": 1, "lee": 1, "et": 1, "al": 1}, {"19": 1, "20": 1, "show": 1, "similar": 1, "performance": 1, "error": 1, "general": 1, "tsallis": 1, "entropy": 1, "rl": 1}, {"proof": 1, "theorem": 2, "5": 2, "detail": 1, "appendix": 1, "assumptions": 1, "1": 2, "2": 1, "error": 1, "v": 3, "": 9, "bound": 1, "kv": 1, "k": 1, "4": 1}, {"1": 1, "": 4, "regularize": 2, "actorcritic": 2, "solve": 1, "problem": 1, "2": 1, "complex": 1, "environments": 1, "propose": 1, "offpolicy": 1, "algorithm": 1, "rac": 1, "alternate": 1, "policy": 2, "evaluation": 1, "improvement": 1}, {"practice": 1, "apply": 1, "neural": 1, "network": 1, "parameterize": 1, "qvalue": 1, "policy": 1, "increase": 1, "expressive": 1, "power": 1}, {"particular": 1, "model": 1, "regularize": 1, "qvalue": 1, "function": 1, "q": 1, "tractable": 1, "policy": 1, "": 1}, {"use": 1, "adam": 1, "17": 1, "optimize": 1, "": 2}, {"actually": 1, "rac": 1, "create": 1, "consult": 1, "previous": 1, "work": 1, "sac": 1, "13": 1, "14": 1, "make": 1, "necessary": 1, "change": 1, "able": 1, "agnostic": 1, "form": 1, "regularization": 1}, {"goal": 1, "train": 1, "regularize": 1, "qvalue": 1, "parameters": 1, "minimize": 1, "general": 1, "bellman": 1, "residual": 1, "1": 1, "ed": 1, "q": 2, "st": 1, "": 14, "y2": 1, "9": 1, "2": 1, "replay": 1, "buffer": 1, "use": 1, "eliminate": 1, "correlation": 1, "sample": 1, "trajectory": 1, "data": 1, "target": 1, "function": 1, "define": 1, "follow": 1, "jq": 1, "rst": 1, "st1": 2, "at1": 2}, {"target": 2, "involve": 1, "regularize": 1, "qvalue": 1, "function": 1, "parameters": 1, "": 1, "update": 1, "move": 1, "average": 1, "fashion": 1, "stabilize": 1, "train": 1, "process": 1, "24": 1, "13": 1}, {"thus": 1, "gradient": 1, "jq": 1, "": 1, "wrt": 1}, {"": 9, "estimate": 1, "q": 3, "ed": 1, "st": 2}, {"j": 2, "train": 1, "policy": 1, "parameters": 1, "minimize": 1, "negative": 1, "total": 1, "reward": 1, "": 12, "ed": 1, "ea": 1, "st": 2, "ast": 2, "q": 1}, {"6": 1, "": 2, "10": 1, "rac": 1, "formally": 1, "describe": 1, "algorithm": 1, "1": 1}, {"method": 1, "alternate": 1, "data": 1, "collection": 1, "parameter": 1, "update": 1}, {"trajectory": 1, "data": 1, "collect": 1, "execute": 1, "current": 1, "policy": 1, "environments": 1, "store": 1, "replay": 1, "buffer": 1}, {"parameters": 1, "function": 1, "approximators": 1, "update": 1, "descend": 1, "along": 1, "stochastic": 1, "gradients": 1, "compute": 1, "batch": 1, "sample": 1, "replay": 1, "buffer": 1}, {"method": 1, "make": 1, "use": 1, "two": 1, "qfunctions": 1, "overcome": 1, "positive": 1, "bias": 1, "incur": 1, "overestimation": 1, "qvalue": 1, "know": 1, "yield": 1, "poor": 1, "performance": 1, "15": 1, "9": 1}, {"specifically": 1, "two": 1, "qfunctions": 1, "parametrized": 1, "different": 1, "parameters": 1, "independently": 1, "train": 1, "minimize": 1, "jq": 1, "": 1}, {"minimum": 1, "two": 1, "qfunctions": 1, "use": 1, "compute": 1, "target": 1, "value": 1, "involve": 1, "": 5, "q": 1, "j": 1}, {"computation": 1, "j": 1, "": 3, "5": 1, "experiment": 1, "investigate": 1, "performance": 1, "different": 1, "regularizers": 1, "among": 1, "diverse": 1, "environments": 1}, {"first": 1, "test": 1, "basic": 1, "combine": 1, "regularizers": 1, "two": 1, "nu": 1, "algorithm": 1, "1": 1, "regularize": 1, "actorcritic": 1, "rac": 1, "merical": 1, "environments": 1}, {"test": 1, "basic": 1, "reginput": 1, "1": 1, "": 3, "2": 1, "ularizers": 1, "atari": 1, "discrete": 1, "problems": 1}, {"end": 1, "initialization": 1, "1": 3, "": 6, "2": 1, "explore": 1, "possible": 1, "application": 1, "mujoco": 1, "iteration": 1, "control": 1, "environments": 1}, {"environment": 2, "step": 2, "sample": 1, "action": 1, "": 19, "st": 3, "receive": 2, "reward": 1, "rt": 3, "51": 1, "numerical": 2, "result": 1, "next": 1, "state": 1, "st1": 2, "two": 1, "discrete": 1, "environments": 1, "end": 1, "consider": 1, "include": 1, "simple": 1, "random": 1, "generate": 1, "gradient": 1, "mdp": 1, "50": 1, "10": 1, "gridworld": 1, "en": 1, "q": 2, "1": 1, "2": 1, "j": 1, "vironment": 1, "81": 1, "4": 1}, {"refer": 1, "ap": 1, "": 6, "j": 1, "pendix": 1, "h1": 1, "detail": 1, "settings": 1}, {"": 6, "1": 2, "2": 1, "end": 1, "regularizers": 1}, {"four": 1, "basic": 1, "regularizers": 1, "include": 1, "end": 1, "shannon": 1, "": 7, "log": 1, "x": 3, "tsallis": 1, "12": 1, "1": 2, "cos": 2, "output": 1, "2": 2, "exp": 1, "exp1": 1, "expx": 1}, {"proposition": 1, "1": 5, "2": 2, "allow": 1, "three": 1, "combine": 1, "regularizers": 1, "min": 2, "minimum": 1, "tsallis": 2, "shannon": 2, "ie": 3, "logx": 2, "21": 1, "": 8, "x": 3, "poly": 1, "positive": 2, "addition": 2, "two": 1, "polynomial": 1, "function": 1, "12": 2, "x2": 1, "3": 1, "mix": 1}, {"sparsity": 1, "convergence": 1}, {"ab": 1, "figure": 1, "1": 2, "": 3, "extremely": 1, "large": 1, "regularizers": 1}, {"c": 1, "show": 1, "probability": 1, "action": 2, "optimal": 1, "policy": 1, "give": 1, "state": 1, "vary": 1, "": 1, "one": 2, "curve": 1, "represent": 1}, {"result": 1, "validate": 1, "theorem": 1, "3": 1}, {"reasonable": 1, "explanation": 1, "large": 1, "": 3, "reduce": 1, "importance": 1, "discount": 1, "reward": 1, "sum": 1, "make": 1, "h": 2, "dominate": 1, "loss": 1, "force": 1, "optimal": 1, "policy": 1, "put": 1, "probability": 1, "mass": 1, "evenly": 1, "action": 1, "order": 1, "maximize": 1}, {"regard": 1, "ability": 1, "defend": 1, "tendency": 1, "towards": 1, "converge": 1, "uniform": 1, "distribution": 1, "sparseness": 1, "power": 1}, {"additional": 1, "experiment": 1, "appendix": 1, "h": 1, "cos": 1, "strongest": 1, "sparseness": 1, "power": 1}, {"show": 1, "convergence": 1, "speed": 1, "rpi": 1, "different": 1, "regularizers": 1}, {"": 4, "also": 1, "show": 1, "kv": 1, "v": 1, "k": 1, "bound": 1, "theorem": 1, "4": 1, "state": 1}, {"52": 1, "": 2, "atari": 1, "result": 1, "regularizers": 1}, {"test": 1, "four": 2, "basic": 1, "regularizers": 1, "across": 1, "discrete": 1, "control": 1, "task": 1, "openai": 1, "gym": 1, "benchmark": 1, "5": 1}, {"train": 1, "detail": 1, "appendix": 1, "h2": 1}, {"performance": 1}, {"figure": 1, "2": 1, "show": 1, "score": 1, "train": 1, "rac": 1, "four": 1, "regularization": 1, "form": 1, "best": 1, "performance": 1, "": 2, "001": 1, "01": 1, "10": 1}, {"except": 1, "breakout": 1, "shannon": 1, "perform": 1, "worse": 1, "three": 1, "regularizers": 1}, {"cos": 1, "perform": 3, "best": 2, "alien": 1, "seaquest": 1, "tsallis": 1, "box": 1, "exp": 1, "quite": 1, "normally": 1}, {"appendix": 1, "h2": 1, "give": 1, "result": 1, "different": 1, "": 1, "sensitive": 1, "analysis": 1}, {"general": 1, "shannon": 1, "insensitive": 1, "among": 1, "others": 1}, {"7": 1, "": 76, "10": 6, "sparsity": 3, "min": 4, "logx": 6, "21": 2, "x": 11, "poly": 2, "12": 4, "1": 11, "x2": 2, "06": 3, "cos": 6, "2": 8, "exp": 2, "exp1": 2, "expx": 2, "04": 3, "02": 3, "0": 4, "07": 1, "shannon": 2, "mix": 2, "tsallis": 2, "08": 3, "05": 2, "4": 3, "6": 3, "8": 3, "regularization": 3, "coefficients": 3, "random": 3, "mdp": 3, "error": 2, "bind": 1, "09": 1, "optimal": 2, "policy": 1, "probability": 1, "03": 1, "01": 1, "00": 2, "c": 1, "b": 2, "gridworld": 2, "20": 1, "40": 1, "60": 1, "iteration": 1, "80": 1, "100": 1, "different": 1, "regularizers": 1, "figure": 1, "show": 1, "result": 1, "policies": 1}, {"c": 1, "show": 1, "change": 1, "process": 1, "probability": 1, "action": 1, "optimal": 1, "policy": 1, "": 1, "regularize": 1, "cos": 1, "2": 1, "x": 1, "random": 1, "mdp": 1}, {"show": 1, "": 4, "error": 1, "v": 2}, {"100": 2, "": 61, "2000": 2, "600": 1, "1750": 1, "12000": 1, "500": 2, "80": 1, "cosx001": 1, "expx01": 3, "shannon001": 1, "tsallis01": 3, "10000": 1, "1500": 1, "400": 1, "score": 4, "1000": 1, "300": 1, "40": 1, "750": 1, "200": 1, "cosx01": 2, "shannon01": 3, "tsallis10": 1, "250": 1, "0": 8, "8000": 1, "60": 1, "1250": 1, "2": 5, "4": 4, "6": 4, "8": 4, "cosx10": 1, "20": 1, "10": 4, "4000": 1, "expx001": 1, "framesm": 4, "alien": 1, "6000": 1, "b": 1, "box": 1, "c": 1, "breakout": 1, "seaquest": 1, "figure": 1, "train": 1, "curve": 1, "atari": 1, "game": 1}, {"entry": 1, "legend": 1, "name": 1, "rule": 1, "regularization": 1, "form": 1, "": 2}, {"score": 1, "smooth": 1, "100": 1, "windows": 1, "shade": 1, "area": 1, "one": 1, "standard": 1, "deviation": 1}, {"53": 1, "": 2, "mujoco": 1, "result": 1, "regularizers": 1}, {"explore": 1, "basic": 1, "regularizers": 1, "across": 1, "four": 1, "continuous": 1, "control": 1, "task": 1, "openai": 1, "gym": 1, "benchmark": 1, "5": 1, "mujoco": 1, "simulator": 1, "38": 1}, {"unfortunately": 1, "cos": 1, "quite": 1, "unstable": 1, "prone": 1, "gradient": 1, "explode": 1, "problems": 1, "deep": 1, "rl": 1, "train": 1, "process": 1}, {"speculate": 1, "instableness": 1, "root": 1, "numerical": 1, "issue": 1, "probability": 1, "density": 1, "function": 1, "often": 1, "diverge": 1, "infinity": 1}, {"whats": 1, "periodicity": 1, "cos": 1, "2": 1, "x": 1, "make": 1, "gradients": 1, "vacillate": 1, "algorithm": 1, "hard": 1, "converge": 1}, {"detail": 1, "follow": 1, "experiment": 1, "give": 1, "appendix": 1, "h3": 1}, {"3000": 6, "": 42, "6000": 3, "5000": 2, "4000": 3, "12000": 1, "2000": 7, "1000": 7, "0": 8, "500": 4, "1500": 4, "iterarion": 4, "antv2": 1, "expx001": 3, "shannon01": 3, "tsallis001": 3, "2500": 4, "10000": 1, "average": 4, "reward": 4, "14000": 1, "7000": 1, "b": 1, "walkerv2": 1, "200": 1, "400": 1, "600": 1, "c": 1, "hopperv2": 1, "expx1": 1, "shannon1": 1, "tsallis1": 1, "800": 1, "8000": 1, "halfcheetahv2": 1, "figure": 1, "3": 1, "train": 1, "curve": 1, "continuous": 1, "control": 1, "benchmarks": 1}, {"curve": 1, "average": 1, "four": 1, "experiment": 1, "different": 1, "seed": 1}, {"entry": 1, "legend": 1, "name": 1, "rule": 1, "regularization": 1, "form": 1, "": 2}, {"score": 1, "smooth": 1, "30": 1, "windows": 1, "shade": 1, "area": 1, "one": 1, "standard": 1, "deviation": 1}, {"performance": 1}, {"figure": 1, "3": 1, "show": 1, "total": 1, "average": 1, "return": 1, "rollouts": 1, "train": 1, "rac": 1, "three": 1, "regularization": 2, "form": 1, "different": 1, "coefficients": 1, "001": 1, "01": 1, "1": 1}, {"curve": 1, "train": 1, "four": 1, "different": 2, "instance": 1, "random": 1, "seed": 1}, {"tsallis": 2, "perform": 1, "steadily": 1, "better": 1, "shannon": 2, "give": 1, "regularization": 1, "coefficient": 1, "": 1, "also": 1, "stable": 1, "since": 1, "shade": 1, "area": 1, "thinner": 1}, {"exp": 1, "perform": 2, "almost": 1, "good": 1, "tsallis": 1, "antv2": 1, "hopperv2": 1, "badly": 1, "rest": 1, "two": 1, "environments": 1}, {"sensitivity": 1, "analysis": 1, "provide": 1, "appendix": 1, "h3": 1, "tsallis": 1, "less": 1, "sensitive": 1, "": 1, "cos": 1, "shannon": 1}, {"8": 1, "": 3, "6": 1, "conclusion": 1, "paper": 1, "propose": 1, "unify": 1, "framework": 1, "regularize": 1, "reinforcement": 1, "learn": 1, "include": 1, "entropyregularized": 1, "rl": 1, "special": 1, "case": 1}, {"framework": 1, "regularization": 1, "function": 1, "characterize": 1, "optimal": 1, "policy": 1, "value": 1, "correspond": 1, "regularize": 1, "mdps": 1}, {"show": 1, "many": 1, "regularization": 1, "function": 2, "lead": 1, "sparse": 1, "multimodal": 1, "optimal": 1, "policy": 1, "trigonometric": 1, "exponential": 1}, {"specify": 1, "necessary": 1, "sufficient": 1, "condition": 1, "regularization": 1, "function": 1, "could": 1, "lead": 1, "sparse": 1, "optimal": 1, "policies": 1, "sparsity": 1, "control": 1, "": 1}, {"present": 1, "logical": 1, "mathematical": 1, "foundations": 1, "properties": 1, "also": 1, "conduct": 1, "experimental": 1, "result": 1}, {"acknowledgements": 1, "work": 1, "sponsor": 1, "key": 1, "project": 1, "china": 1}, {"2018aaa0101000": 1, "beijing": 1, "municipal": 1, "commission": 1, "science": 1, "technology": 1, "grant": 1}, {"181100008918005": 1, "beijing": 1, "academy": 1, "artificial": 1, "intelligence": 1, "baai": 1}, {"reference": 1, "1": 1, "kavosh": 1, "asadi": 1, "michael": 1, "l": 1, "littman": 1}, {"alternative": 1, "softmax": 1, "operator": 1, "reinforcement": 1, "learn": 1}, {"proceed": 1, "34th": 1, "international": 1, "conference": 1, "machine": 1, "learningvolume": 1, "70": 1, "page": 1, "243252": 1}, {"jmlr": 1}, {"org": 1, "2017": 1}, {"2": 1, "r": 1, "bellmann": 1}, {"dynamic": 1, "program": 1, "princeton": 1, "university": 1, "press": 1}, {"princeton": 1, "nj": 1, "1957": 1}, {"3": 1, "boris": 1, "belousov": 1, "jan": 1, "peters": 1}, {"fdivergence": 1, "constrain": 1, "policy": 1, "improvement": 1}, {"arxiv": 1, "preprint": 1, "arxiv180100056": 1, "2017": 1}, {"4": 1, "dimitri": 1, "p": 1, "bertsekas": 1}, {"neurodynamic": 1, "program": 1}, {"encyclopedia": 1, "optimization": 1, "page": 1, "25552560": 1}, {"springer": 1, "2008": 1}, {"5": 1, "greg": 1, "brockman": 1, "vicki": 1, "cheung": 1, "ludwig": 1, "pettersson": 1, "jonas": 1, "schneider": 1, "john": 1, "schulman": 1, "jie": 1, "tang": 1, "wojciech": 1, "zaremba": 1}, {"openai": 1, "gym": 1}, {"arxiv": 1, "preprint": 1, "arxiv160601540": 1, "2016": 1}, {"6": 1, "bo": 1, "dai": 1, "albert": 1, "shaw": 1, "lihong": 1, "li": 1, "lin": 1, "xiao": 1, "niao": 1, "zhen": 1, "liu": 1, "jianshu": 1, "chen": 1, "le": 1, "song": 1}, {"sbeed": 1, "convergent": 1, "reinforcement": 1, "learn": 1, "nonlinear": 1, "function": 1, "approximation": 1}, {"international": 1, "conference": 1, "machine": 1, "learn": 1, "page": 1, "11331142": 1, "2018": 1}, {"7": 1, "amir": 1, "farahmand": 1, "mohammad": 1, "ghavamzadeh": 1, "shie": 1, "mannor": 1, "csaba": 1, "szepesvri": 1}, {"regularize": 1, "policy": 1, "iteration": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "page": 1, "441448": 1, "2009": 1}, {"8": 1, "roy": 1, "fox": 1, "ari": 1, "pakman": 1, "naftali": 1, "tishby": 1}, {"tame": 1, "noise": 1, "reinforcement": 1, "learn": 1, "via": 1, "soft": 1, "update": 1}, {"arxiv": 1, "preprint": 1, "arxiv151208562": 1, "2015": 1}, {"9": 1, "scott": 1, "fujimoto": 1, "herke": 1, "van": 1, "hoof": 1, "dave": 1, "meger": 1}, {"address": 1, "function": 1, "approximation": 1, "error": 1, "actorcritic": 1, "methods": 1}, {"arxiv": 1, "preprint": 1, "arxiv180209477": 1, "2018": 1}, {"10": 1, "matthieu": 1, "geist": 1, "bruno": 1, "scherrer": 1, "olivier": 1, "pietquin": 1}, {"theory": 1, "regularize": 1, "markov": 1, "decision": 1, "process": 1}, {"arxiv": 1, "preprint": 1, "arxiv190111275": 1, "2019": 1}, {"11": 1, "jordi": 1, "graumoya": 1, "felix": 1, "leibfried": 1, "peter": 1, "vrancx": 1}, {"soft": 1, "qlearning": 1, "mutualinformation": 1, "regularization": 1}, {"2018": 1}, {"12": 1, "tuomas": 1, "haarnoja": 1, "haoran": 1, "tang": 1, "pieter": 1, "abbeel": 1, "sergey": 1, "levine": 1}, {"reinforcement": 1, "learn": 1, "deep": 1, "energybased": 1, "policies": 1}, {"arxiv": 1, "preprint": 1, "arxiv170208165": 1, "2017": 1}, {"13": 1, "tuomas": 1, "haarnoja": 1, "aurick": 1, "zhou": 1, "pieter": 1, "abbeel": 1, "sergey": 1, "levine": 1}, {"soft": 1, "actorcritic": 1, "offpolicy": 1, "maximum": 1, "entropy": 1, "deep": 1, "reinforcement": 1, "learn": 1, "stochastic": 1, "actor": 1}, {"arxiv": 1, "preprint": 1, "arxiv180101290": 1, "2018": 1}, {"9": 1, "": 1, "14": 1, "tuomas": 1, "haarnoja": 1, "aurick": 1, "zhou": 1, "kristian": 1, "hartikainen": 1, "george": 1, "tucker": 1, "sehoon": 1, "ha": 1, "jie": 1, "tan": 1, "vikash": 1, "kumar": 1, "henry": 1, "zhu": 1, "abhishek": 1, "gupta": 1, "pieter": 1, "abbeel": 1, "et": 1, "al": 1}, {"soft": 1, "actorcritic": 1, "algorithms": 1, "applications": 1}, {"arxiv": 1, "preprint": 1, "arxiv181205905": 1, "2018": 1}, {"15": 1, "hado": 1, "v": 1, "hasselt": 1}, {"double": 1, "qlearning": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "page": 1, "26132621": 1, "2010": 1}, {"16": 1, "jeffrey": 1, "johns": 1, "christopher": 1, "painterwakefield": 1, "ronald": 1, "parr": 1}, {"linear": 1, "complementarity": 1, "regularize": 1, "policy": 1, "evaluation": 1, "improvement": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "page": 1, "10091017": 1, "2010": 1}, {"17": 1, "diederik": 1, "p": 1, "kingma": 1, "jimmy": 1, "ba": 1}, {"adam": 1, "method": 1, "stochastic": 1, "optimization": 1}, {"arxiv": 1, "preprint": 1, "arxiv14126980": 1, "2014": 1}, {"18": 1, "j": 1, "zico": 1, "kolter": 1, "andrew": 1, "ng": 1}, {"regularization": 1, "feature": 1, "selection": 1, "leastsquares": 1, "temporal": 1, "difference": 1, "learn": 1}, {"proceed": 1, "26th": 1, "annual": 1, "international": 1, "conference": 1, "machine": 1, "learn": 1, "page": 1, "521528": 1}, {"acm": 1, "2009": 1}, {"19": 1, "kyungjae": 1, "lee": 1, "sungjoon": 1, "choi": 1, "songhwai": 1, "oh": 1}, {"sparse": 2, "markov": 1, "decision": 1, "process": 1, "causal": 1, "tsallis": 1, "entropy": 1, "regularization": 1, "reinforcement": 1, "learn": 1}, {"ieee": 1, "robotics": 1, "automation": 1, "letter": 1, "3314661473": 1, "2018": 1}, {"20": 1, "kyungjae": 1, "lee": 1, "sungyub": 1, "kim": 1, "sungbin": 1, "lim": 1, "sungjoon": 1, "choi": 1, "songhwai": 1, "oh": 1}, {"tsallis": 1, "reinforcement": 2, "learn": 2, "unify": 1, "framework": 1, "maximum": 1, "entropy": 1}, {"arxiv": 1, "preprint": 1, "arxiv190200137": 1, "2019": 1}, {"21": 1, "yang": 1, "liu": 2, "prajit": 1, "ramachandran": 1, "qiang": 1, "jian": 1, "peng": 1}, {"stein": 1, "variational": 1, "policy": 1, "gradient": 1}, {"arxiv": 1, "preprint": 1, "arxiv170402399": 1, "2017": 1}, {"22": 1, "jam": 1, "martens": 1, "roger": 1, "grosse": 1}, {"optimize": 1, "neural": 1, "network": 1, "kroneckerfactored": 1, "approximate": 1, "curvature": 1}, {"international": 1, "conference": 1, "machine": 1, "learn": 1, "page": 1, "24082417": 1, "2015": 1}, {"23": 1, "amir": 1, "massoud": 1, "farahmand": 1, "mohammad": 1, "ghavamzadeh": 1, "csaba": 1, "szepesvri": 1, "shie": 1, "mannor": 1}, {"regularize": 1, "fit": 1, "qiteration": 1, "plan": 1, "continuousspace": 1, "markovian": 1, "decision": 1, "problems": 1}, {"american": 1, "control": 1, "conference": 1, "2009": 1}, {"acc09": 1, "page": 1, "725730": 1}, {"ieee": 1, "2009": 1}, {"24": 1, "volodymyr": 1, "mnih": 1, "koray": 1, "kavukcuoglu": 1, "david": 1, "silver": 1, "andrei": 1, "rusu": 1, "joel": 1, "veness": 1, "marc": 1, "g": 1, "bellemare": 1, "alex": 1, "grave": 1, "martin": 1, "riedmiller": 1, "andreas": 1, "k": 1, "fidjeland": 1, "georg": 1, "ostrovski": 1, "et": 1, "al": 1}, {"humanlevel": 1, "control": 1, "deep": 1, "reinforcement": 1, "learn": 1}, {"nature": 1, "5187540529": 1, "2015": 1}, {"25": 1, "ofir": 1, "nachum": 1, "mohammad": 1, "norouzi": 1, "kelvin": 1, "xu": 1, "dale": 1, "schuurmans": 1}, {"bridge": 1, "gap": 1, "value": 1, "policy": 1, "base": 1, "reinforcement": 1, "learn": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "page": 1, "27752785": 1, "2017": 1}, {"26": 1, "ofir": 1, "nachum": 1, "mohammad": 1, "norouzi": 1, "kelvin": 1, "xu": 1, "dale": 1, "schuurmans": 1}, {"trustpcl": 1, "offpolicy": 1, "trust": 1, "region": 1, "method": 1, "continuous": 1, "control": 1}, {"arxiv": 1, "preprint": 1, "arxiv170701891": 1, "2017": 1}, {"27": 1, "ofir": 1, "nachum": 1, "yinlam": 1, "chow": 1, "mohammad": 1, "ghavamzadeh": 1}, {"path": 1, "consistency": 1, "learn": 1, "tsallis": 1, "entropy": 1, "regularize": 1, "mdps": 1}, {"arxiv": 1, "preprint": 1, "arxiv180203501": 1, "2018": 1}, {"28": 1, "gergely": 1, "neu": 1, "anders": 1, "jonsson": 1, "vicen": 1, "gmez": 1}, {"unify": 1, "view": 1, "entropyregularized": 1, "markov": 1, "decision": 1, "process": 1}, {"arxiv": 1, "preprint": 1, "arxiv170507798": 1, "2017": 1}, {"29": 1, "brendan": 1, "odonoghue": 1, "remi": 1, "munos": 1, "koray": 1, "kavukcuoglu": 1, "volodymyr": 1, "mnih": 1}, {"combine": 1, "policy": 1, "gradient": 1, "qlearning": 1}, {"arxiv": 1, "preprint": 1, "arxiv161101626": 1, "2016": 1}, {"30": 1, "jan": 1, "peters": 1, "katharina": 1, "mlling": 1, "yasemin": 1, "altun": 1}, {"relative": 1, "entropy": 1, "policy": 1, "search": 1}, {"aaai": 1, "page": 1, "16071612": 1}, {"atlanta": 1, "2010": 1}, {"31": 1, "martin": 1, "l": 1, "puterman": 1}, {"markov": 1, "decision": 1, "process": 1, "discrete": 1, "stochastic": 1, "dynamic": 1, "program": 1}, {"john": 1, "wiley": 1, "": 1, "sons": 1, "2014": 1}, {"10": 1, "": 1, "32": 1, "john": 1, "schulman": 1, "sergey": 1, "levine": 1, "pieter": 1, "abbeel": 1, "michael": 1, "jordan": 1, "philipp": 1, "moritz": 1}, {"trust": 1, "region": 1, "policy": 1, "optimization": 1}, {"international": 1, "conference": 1, "machine": 1, "learn": 1, "page": 1, "18891897": 1, "2015": 1}, {"33": 1, "john": 1, "schulman": 1, "xi": 1, "chen": 1, "pieter": 1, "abbeel": 1}, {"equivalence": 1, "policy": 1, "gradients": 1, "soft": 1, "qlearning": 1}, {"arxiv": 1, "preprint": 1, "arxiv170406440": 1, "2017": 1}, {"34": 1, "john": 1, "schulman": 1, "filip": 1, "wolski": 1, "prafulla": 1, "dhariwal": 1, "alec": 1, "radford": 1, "oleg": 1, "klimov": 1}, {"proximal": 1, "policy": 1, "optimization": 1, "algorithms": 1}, {"arxiv": 1, "preprint": 1, "arxiv170706347": 1, "2017": 1}, {"35": 1, "david": 1, "roger": 1, "smart": 1}, {"fix": 1, "point": 1, "theorems": 1, "volume": 1, "66": 1}, {"cup": 1, "archive": 1, "1980": 1}, {"36": 1, "richard": 1, "sutton": 1, "andrew": 1, "g": 1, "barto": 1}, {"reinforcement": 1, "learn": 1, "introduction": 1}, {"mit": 1, "press": 1, "2018": 1}, {"37": 1, "emanuel": 1, "todorov": 1}, {"linearlysolvable": 1, "markov": 1, "decision": 1, "problems": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "page": 1, "13691376": 1, "2007": 1}, {"38": 1, "emanuel": 1, "todorov": 1, "tom": 1, "erez": 1, "yuval": 1, "tassa": 1}, {"mujoco": 1, "physics": 1, "engine": 1, "modelbased": 1, "control": 1}, {"intelligent": 1, "robots": 1, "systems": 1, "iros": 1, "2012": 1, "ieeersj": 1, "international": 1, "conference": 1, "page": 1, "50265033": 1}, {"ieee": 1, "2012": 1}, {"39": 1, "constantino": 1, "tsallis": 1}, {"possible": 1, "generalization": 1, "boltzmanngibbs": 1, "statistics": 1}, {"journal": 1, "statistical": 1, "physics": 1, "5212479487": 1, "1988": 1}, {"40": 1, "peter": 1, "vamplew": 1, "richard": 1, "dazeley": 1, "cameron": 1, "foale": 1}, {"softmax": 1, "exploration": 1, "strategies": 1, "multiobjective": 1, "reinforcement": 1, "learn": 1}, {"neurocomputing": 1, "2637486": 1, "2017": 1}, {"41": 1, "lieven": 1, "vandenberghe": 1}, {"cvxopt": 1, "linear": 1, "quadratic": 1, "cone": 1, "program": 1, "solvers": 1}, {"online": 1, "httpcvxopt": 1}, {"orgdocumentationconeprog": 1}, {"pdf": 1, "2010": 1}, {"11": 1}]
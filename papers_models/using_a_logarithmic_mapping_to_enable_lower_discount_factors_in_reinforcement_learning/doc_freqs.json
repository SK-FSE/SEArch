[{"use": 1, "logarithmic": 1, "map": 1, "enable": 1, "lower": 1, "discount": 2, "factor": 2, "reinforcement": 2, "learn": 2, "": 4, "harm": 1, "van": 1, "seijen": 1, "microsoft": 2, "research": 2, "montral": 2, "harmvanseijenmicrosoftcom": 1, "mehdi": 1, "fatemi": 1, "mehdifatemimicrosoftcom": 1, "arash": 1, "tavakoli": 1, "imperial": 1, "college": 1, "london": 1, "atavakoliimperialacuk": 1, "abstract": 1, "effort": 1, "better": 1, "understand": 1, "different": 1, "ways": 1, "affect": 1, "optimization": 1, "process": 1, "design": 1, "set": 1, "experiment": 1, "study": 1, "effect": 1, "isolation": 1}, {"analysis": 1, "reveal": 1, "common": 1, "perception": 1, "poor": 1, "performance": 1, "low": 1, "discount": 1, "factor": 1, "cause": 1, "small": 1, "actiongaps": 1, "require": 1, "revision": 1}, {"propose": 1, "alternative": 1, "hypothesis": 1, "identify": 1, "sizedifference": 1, "actiongap": 1, "across": 1, "statespace": 1, "primary": 1, "cause": 1}, {"introduce": 1, "new": 1, "method": 1, "enable": 1, "homogeneous": 1, "actiongaps": 1, "map": 1, "value": 1, "estimate": 1, "logarithmic": 1, "space": 1}, {"prove": 1, "convergence": 1, "method": 1, "standard": 1, "assumptions": 1, "demonstrate": 1, "empirically": 1, "indeed": 1, "enable": 1, "lower": 1, "discount": 1, "factor": 1, "approximate": 1, "reinforcementlearning": 1, "methods": 1}, {"turn": 1, "allow": 1, "tackle": 1, "class": 1, "reinforcementlearning": 1, "problems": 1, "challenge": 1, "solve": 1, "traditional": 1, "methods": 1}, {"1": 1, "": 2, "introduction": 1, "reinforcement": 1, "learn": 1, "rl": 1, "objective": 2, "one": 1, "want": 1, "optimize": 2, "often": 1, "best": 1, "describe": 1, "undiscounted": 2, "sum": 1, "reward": 1, "eg": 1, "maximize": 1, "total": 1, "score": 1, "game": 1, "discount": 1, "factor": 1, "merely": 1, "introduce": 1, "avoid": 1, "optimization": 1, "challenge": 1, "occur": 1, "directly": 1, "bertsekas": 1, "tsitsiklis": 1, "1996": 1}, {"scenario": 1, "discount": 1, "factor": 1, "play": 1, "role": 1, "hyperparameter": 1, "tune": 1, "obtain": 1, "better": 1, "performance": 1, "true": 1, "objective": 1}, {"furthermore": 1, "practical": 1, "reason": 1, "policy": 1, "evaluate": 1, "finite": 1, "amount": 1, "time": 1, "make": 1, "effective": 1, "performance": 1, "metric": 1, "finitehorizon": 2, "undiscounted": 2, "objective1": 1, "gain": 1, "better": 1, "understand": 1, "interaction": 1, "discount": 1, "factor": 1, "objective": 1, "design": 1, "number": 1, "experiment": 1, "study": 1, "relation": 1}, {"one": 1, "surprise": 1, "find": 1, "problems": 1, "low": 1, "discount": 2, "factor": 1, "result": 1, "better": 1, "asymptotic": 1, "performance": 1, "finitehorizon": 1, "undiscounted": 1, "objective": 1, "indirectly": 1, "optimize": 1, "proxy": 1, "infinitehorizon": 1, "sum": 1}, {"motivate": 1, "us": 1, "look": 1, "deeper": 1, "effect": 1, "discount": 1, "factor": 1, "optimization": 1, "process": 1}, {"analyze": 1, "practice": 1, "performance": 1, "low": 1, "discount": 1, "factor": 1, "tend": 1, "fall": 1, "flat": 1, "combine": 1, "function": 1, "approximation": 1, "especially": 1, "task": 1, "long": 1, "horizons": 1}, {"specifically": 1, "refute": 1, "number": 1, "common": 1, "hypotheses": 1, "present": 1, "new": 1, "one": 1, "instead": 1, "identify": 1, "primary": 1, "culprit": 1, "size1": 1, "example": 1, "seminal": 1, "work": 1, "mnih": 1, "et": 1, "al": 1}, {"2015": 1, "undiscounted": 1, "score": 1, "atari": 1, "game": 2, "report": 1, "timelimit": 1, "5": 1, "minutes": 1, "per": 1}, {"33rd": 1, "conference": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "neurips": 1, "2019": 1, "vancouver": 1, "canada": 1}, {"difference": 2, "action": 2, "gap": 1, "ie": 1, "value": 1, "best": 1, "secondbest": 1, "state": 1, "across": 1, "statespace": 1}, {"main": 1, "contribution": 1, "new": 1, "method": 1, "yield": 1, "homogeneous": 1, "actiongap": 1, "size": 1, "sparsereward": 1, "problems": 1}, {"achieve": 1, "map": 1, "update": 2, "target": 1, "logarithmic": 1, "space": 2, "perform": 1, "instead": 1}, {"prove": 1, "convergence": 1, "method": 1, "standard": 1, "condition": 1}, {"finally": 1, "demonstrate": 1, "empirically": 1, "method": 1, "achieve": 1, "much": 1, "better": 1, "performance": 1, "low": 1, "discount": 1, "factor": 1, "previously": 1, "possible": 1, "provide": 1, "support": 1, "evidence": 1, "new": 1, "hypothesis": 1}, {"combine": 1, "analytical": 1, "result": 1, "exist": 1, "task": 1, "low": 1, "discount": 1, "factor": 1, "outperform": 1, "higher": 1, "ones": 1, "asymptotically": 1, "suggest": 1, "method": 1, "unlock": 1, "performance": 1, "certain": 1, "problems": 1, "achievable": 1, "contemporary": 1, "rl": 1, "methods": 1}, {"2": 1, "": 11, "problem": 1, "set": 3, "consider": 1, "markov": 1, "decision": 1, "process": 1, "mdp": 1, "puterman": 1, "1994": 1, "hs": 1, "p": 3, "r": 4, "s0": 2, "denote": 1, "state": 2, "action": 1, "reward": 1, "function": 2, "transition": 1, "probability": 1, "0": 1, "1": 1, "start": 1, "distribution": 1}, {"time": 1, "step": 1, "agent": 1, "observe": 1, "state": 1, "st": 1, "": 2, "take": 1, "action": 1}, {"agent": 1, "observe": 1, "next": 1, "state": 1, "st1": 2, "": 8, "draw": 1, "transition": 1, "probability": 1, "distribution": 1, "p": 1, "st": 1, "reward": 1, "rt": 1, "rst": 1}, {"terminal": 1, "state": 2, "one": 1, "enter": 1, "terminate": 1, "interaction": 1, "environment": 1, "mathematically": 1, "interpret": 1, "absorb": 1, "transition": 1, "correspond": 1, "reward": 1, "0": 1}, {"behavior": 1, "agent": 1, "define": 1, "policy": 1, "": 8, "time": 1, "step": 1, "take": 1, "input": 1, "history": 1, "state": 1, "action": 3, "reward": 1, "s0": 1, "a0": 1, "r0": 1, "s1": 1, "a1": 1, "rt1": 1, "st": 1, "output": 1, "distribution": 1, "accordance": 1, "select": 1}, {"action": 1, "depend": 2, "current": 2, "state": 2, "st": 2, "": 2, "call": 2, "policy": 3, "stationary": 1, "one": 1, "nonstationary": 1}, {"define": 1, "task": 1, "combination": 1, "mdp": 1, "performance": 1, "metric": 1, "f": 1, "": 1}, {"metric": 1, "f": 1, "function": 1, "take": 1, "input": 1, "policy": 1, "": 3, "output": 1, "score": 1, "represent": 1, "performance": 1}, {"contrast": 1, "define": 1, "learn": 1, "metric": 2, "fl": 1, "agent": 1, "optimize": 1}, {"within": 1, "context": 1, "paper": 1, "unless": 1, "otherwise": 1, "state": 1, "performance": 1, "metric": 2, "f": 3, "consider": 2, "expect": 2, "finitehorizon": 1, "undiscounted": 1, "sum": 2, "reward": 2, "startstate": 1, "distribution": 1, "learn": 1, "fl": 3, "infinitehorizon": 1, "discount": 2, "h1": 1, "": 18, "x": 2, "e": 2, "ri": 2, "1": 1, "i0": 2, "horizon": 1, "h": 1, "factor": 1, "hyperparameters": 1, "respectively": 1}, {"optimal": 1, "policy": 2, "task": 1, "": 4, "maximize": 1, "metric": 1, "f": 1, "mdp": 1}, {"note": 1, "general": 1, "": 2, "nonstationary": 1, "policy": 1}, {"particular": 1, "optimal": 1, "policy": 1, "depend": 1, "besides": 1, "current": 1, "state": 1, "time": 1, "step": 1}, {"denote": 1, "policy": 1, "optimal": 1, "wrt": 1}, {"learn": 1, "metric": 1, "fl": 1, "l": 1, "": 1}, {"fl": 1, "finitehorizon": 1, "objective": 1, "exist": 1, "stationary": 1, "optimal": 2, "policy": 2, "considerably": 1, "simplify": 1, "learn": 2, "problem2": 1, "due": 1, "difference": 1, "performance": 1, "metrics": 1, "wrt": 1}, {"learn": 1, "metric": 1, "need": 1, "optimal": 1, "wrt": 1}, {"performance": 1, "metric": 1}, {"call": 1, "difference": 1, "performance": 1, "l": 2, "": 13, "measure": 1, "f": 4, "metric": 2, "gap": 2, "relation": 1, "analyze": 1, "section": 1, "31": 1}, {"consider": 1, "modelfree": 1, "valuebased": 1, "methods": 1}, {"methods": 1, "aim": 1, "find": 1, "good": 1, "policy": 2, "iteratively": 1, "improve": 1, "estimate": 1, "optimal": 2, "actionvalue": 1, "function": 1, "q": 1, "": 1, "generally": 1, "predict": 1, "expect": 1, "discount": 1, "sum": 1, "reward": 1, "l": 1, "condition": 1, "stateaction": 1, "pair": 1}, {"canonical": 1, "example": 1, "qlearning": 1, "watkins": 1, "dayan": 1, "1992": 1, "update": 1, "estimate": 1, "follow": 1, "": 17, "0": 2, "qt1": 1, "st": 2, "1": 1, "qt": 1, "rt": 1, "max": 1, "q": 1, "2": 2, "t1": 1, "main": 1, "reason": 1, "optimize": 1, "infinitehorizon": 1, "objective": 1, "rather": 1, "finitehorizon": 1, "one": 1, "attractive": 1, "choice": 1}, {"2": 1, "": 3, "0": 1, "1": 1, "stepsize": 1}, {"actionvalue": 1, "function": 2, "commonly": 1, "estimate": 1, "use": 1, "approximator": 1, "weight": 1, "vector": 1, "": 2, "qs": 1}, {"deep": 2, "qnetworks": 1, "dqn": 1, "mnih": 1, "et": 1, "al": 1, "2015": 1, "use": 2, "neural": 1, "network": 2, "function": 2, "approximator": 1, "iteratively": 1, "improve": 1, "estimate": 1, "q": 1, "minimize": 1, "sequence": 1, "loss": 1, "": 21, "li": 1, "esars0": 1, "yidqn": 2, "qs": 2, "2": 1, "3": 1, "4": 1, "0": 3, "r": 1, "max": 1, "i1": 2, "weight": 1, "vector": 1, "previous": 1, "iteration": 1, "encode": 1, "separate": 1, "target": 1}, {"analysis": 1, "discount": 1, "factor": 1, "effect": 2, "metric": 1, "gap": 1, "": 39, "b": 2, "c": 2, "1": 5, "5": 3, "4": 1, "60": 1, "3": 2, "2": 1, "0": 1, "00": 4, "58": 1, "03": 1, "performance": 3, "31": 1, "56": 1, "54": 1, "52": 1, "50": 1, "02": 4, "04": 3, "06": 3, "08": 3, "10": 3, "01": 1, "figure": 1, "illustrations": 1, "three": 1, "different": 1, "task": 1, "blue": 1, "diamond": 1, "start": 1, "position": 1, "green": 1, "circle": 2, "positive": 1, "object": 2, "red": 1, "negative": 1, "gray": 1, "arrows": 1, "wind": 1, "direction": 1, "number": 1, "indicate": 1, "reward": 1}, {"graph": 1, "show": 1, "performanceas": 1, "measure": 1, "f": 1, "task": 1, "": 2, "black": 1, "dot": 1, "line": 2, "l": 1, "red": 1, "solid": 1, "function": 1, "discount": 1, "factor": 1, "learn": 1, "metric": 1}, {"difference": 1, "two": 1, "represent": 1, "metric": 1, "gap": 1}, {"question": 1, "central": 1, "section": 1, "follow": 1, "give": 1, "finitehorizon": 1, "undiscounted": 1, "performance": 1, "metric": 3, "say": 1, "relation": 1, "discount": 1, "factor": 1, "learn": 1, "gap": 1}, {"study": 1, "problem": 1, "design": 1, "variety": 1, "different": 1, "task": 1, "measure": 1, "dependence": 1, "metric": 1, "gap": 1, "discount": 1, "factor": 1}, {"figure": 1, "1": 1, "illustrate": 1, "three": 1, "task": 2, "well": 1, "metric": 1, "gap": 1, "function": 1, "discount": 1, "factor": 1}, {"task": 1, "agent": 1, "start": 1, "particular": 1, "position": 1, "collect": 2, "reward": 1, "positive": 1, "object": 2, "avoid": 1, "negative": 1}, {"transition": 1, "dynamics": 1, "task": 2, "b": 1, "deterministic": 1, "whereas": 1, "c": 1, "wind": 1, "blow": 1, "direction": 1, "arrows": 1, "make": 1, "agent": 1, "move": 1, "towards": 1, "leave": 1, "40": 1, "chance": 1, "regardless": 1, "perform": 1, "action": 1}, {"three": 1, "task": 1, "horizon": 1, "performance": 1, "metric": 1, "12": 1}, {"task": 1, "small": 1, "negative": 1, "reward": 2, "trade": 1, "large": 1, "positive": 1, "receive": 1, "later": 1, "high": 1, "discount": 1, "factor": 1, "result": 1, "smaller": 1, "metric": 1, "gap": 1}, {"contrast": 1, "task": 1, "b": 1, "low": 1, "discount": 1, "factor": 1, "result": 1, "smaller": 1, "metric": 1, "gap": 1}, {"reason": 1, "high": 1, "discount": 1, "factor": 1, "optimal": 1, "learn": 1, "policy": 1, "take": 1, "longer": 1, "route": 1, "first": 1, "try": 1, "collect": 1, "large": 1, "object": 2, "go": 1, "small": 1}, {"however": 1, "performance": 1, "metric": 1, "horizon": 1, "12": 1, "enough": 1, "time": 1, "take": 1, "long": 1, "route": 1, "get": 1, "reward": 1}, {"low": 1, "discount": 1, "factor": 1, "take": 1, "shorter": 1, "route": 1, "first": 1, "go": 1, "smaller": 1, "object": 2, "able": 1, "collect": 1, "time": 1}, {"task": 1, "c": 1, "tradeoff": 1, "make": 1, "risk": 2, "fall": 1, "negative": 1, "object": 1, "due": 1, "domain": 1, "stochasticity": 1, "versus": 1, "take": 1, "longer": 1, "detour": 1, "minimize": 1}, {"task": 1, "optimal": 2, "policy": 1, "": 2, "nonstationary": 1, "action": 1, "depend": 1, "time": 1, "step": 1}, {"however": 1, "learn": 1, "objective": 1, "fl": 1, "finitehorizon": 1, "stationary": 1, "optimal": 1, "policy": 1, "l": 1, "": 1}, {"hence": 1, "metric": 1, "gap": 1, "cannot": 1, "reduce": 1, "0": 1, "value": 1, "discount": 1, "factor": 1}, {"best": 1, "discount": 1, "factor": 1, "something": 1, "high": 1, "low": 1}, {"policy": 1, "l": 1, "derive": 1, "infinitehorizon": 1, "metric": 1, "preclude": 1, "learn": 1, "finitelength": 1, "train": 1, "episodes": 1}, {"example": 1, "consider": 1, "use": 1, "qlearning": 1, "learn": 1, "l": 1, "task": 1, "figure": 1, "1": 1}, {"uniformly": 1, "random": 1, "behavior": 1, "policy": 1, "train": 1, "episodes": 1, "length": 1, "12": 1, "horizon": 1, "performance": 1, "metric": 1, "nonzero": 1, "probability": 1, "stateaction": 1, "pair": 1, "visit": 1, "within": 1, "episode": 1}, {"hence": 1, "right": 1, "stepsize": 1, "decay": 1, "schedule": 1, "convergence": 1, "limit": 1, "guarantee": 1, "jaakkola": 1, "et": 1, "al": 1, "1994": 1}, {"key": 1, "detail": 1, "enable": 1, "3": 1, "": 1, "figure": 1, "2": 1, "chain": 1, "task": 1, "consist": 1, "50": 1, "state": 1, "two": 1, "terminal": 1, "ones": 1}, {"nonterminal": 1, "state": 1, "two": 1, "action": 2, "al": 1, "result": 1, "transition": 1, "leave": 1, "probability": 2, "1": 1, "": 2, "p": 2, "right": 1, "vice": 1, "versa": 1, "ar": 1}, {"reward": 1, "0": 1, "except": 1, "transition": 1, "farleft": 1, "farright": 1, "terminal": 1, "state": 1, "result": 1, "rl": 1, "rr": 1, "": 1, "respectively": 1}, {"state": 2, "reach": 1, "final": 1, "time": 1, "step": 1, "treat": 1, "terminal": 1, "value": 1, "0": 1, "default": 1, "normal": 1, "bootstrapping": 1, "occur": 1, "pardo": 1, "et": 1, "al": 1, "2018": 1}, {"finitehorizon": 1, "performance": 1, "metric": 2, "essential": 1, "observe": 1, "strong": 1, "dependence": 1, "gap": 1, "": 1}, {"example": 1, "task": 1, "b": 1, "performance": 1, "metric": 1, "would": 1, "measure": 1, "number": 1, "step": 1, "take": 1, "collect": 1, "object": 1, "similar": 1, "graph": 1, "obtain": 1}, {"general": 1, "examples": 1, "section": 1, "demonstrate": 1, "best": 1, "discount": 1, "factor": 1, "taskdependent": 1, "anywhere": 1, "range": 1, "0": 1, "1": 1}, {"32": 1, "": 2, "optimization": 1, "effect": 2, "keep": 1, "experiment": 1, "simple": 1, "possible": 1, "remove": 1, "exploration": 1, "perform": 1, "update": 2, "sweep": 2, "entire": 1, "stateaction": 1, "space": 1, "use": 1, "stepsize": 1, "0001": 1, "measure": 1, "performance": 1, "end": 1}, {"figure": 1, "3": 1, "show": 1, "performance": 3, "early": 1, "learn": 1, "average": 2, "first": 1, "10": 1, "000": 3, "sweep": 2, "well": 1, "final": 1, "100": 1, "110": 1}, {"average": 1, "performance": 1, "": 1, "study": 1, "optimization": 1, "effect": 1, "function": 2, "approximation": 2, "use": 2, "linear": 1, "feature": 1, "construct": 1, "tilecoding": 1, "sutton": 1, "1996": 1, "tilewidths": 1, "1": 1, "2": 1, "3": 1, "5": 1}, {"tilewidth": 1, "w": 2, "correspond": 1, "binary": 1, "feature": 1, "nonzero": 1, "neighbour": 1, "state": 1, "zero": 1, "remain": 1, "ones": 1}, {"number": 1, "offset": 1, "tile": 1, "value": 1, "function": 1, "represent": 1}, {"hence": 1, "errorfree": 1, "reconstruction": 1, "optimal": 1, "actionvalue": 1, "function": 1, "possible": 1, "principle": 1, "discount": 1, "factor": 1}, {"note": 1, "width": 1, "1": 1, "representation": 1, "reduce": 1, "tabular": 1, "one": 1}, {"average": 1, "performance": 2, "": 1, "l": 1, "give": 2, "theoretical": 1, "limit": 1, "agent": 1, "achieve": 1, "learn": 1, "metric": 1}, {"however": 1, "discount": 2, "factor": 2, "also": 1, "affect": 1, "optimization": 1, "process": 1, "find": 1, "l": 1, "could": 1, "challenge": 1, "others": 1}, {"section": 1, "use": 1, "task": 1, "show": 1, "figure": 1, "2": 1, "evaluate": 1, "correlation": 1, "discount": 1, "factor": 1, "hard": 1, "could": 1, "find": 1, "l": 1, "": 1}, {"easy": 1, "see": 1, "policy": 1, "always": 1, "take": 1, "leave": 1, "action": 1, "al": 1, "maximize": 1, "discount": 2, "undiscounted": 1, "sum": 1, "reward": 1, "factor": 1, "horizon": 1, "value": 1, "respectively": 1}, {"define": 1, "learn": 1, "metric": 2, "fl": 1, "1": 1, "use": 1, "different": 1, "performance": 1, "f": 1, "": 1}, {"specifically": 1, "define": 1, "f": 1, "1": 1, "policy": 1, "take": 1, "al": 1, "every": 1, "state": 1, "0": 1, "otherwise": 1}, {"metric": 1, "gap": 1, "set": 1, "f": 1, "fl": 1, "0": 1, "optimal": 1, "performance": 1, "": 3, "l": 1, "1": 1}, {"10": 4, "08": 4, "06": 4, "04": 4, "": 7, "w": 8, "1": 2, "2": 2, "02": 4, "3": 2, "5": 2, "00": 2, "experiment": 1, "demonstrate": 1, "common": 1, "empirical": 1, "observation": 1, "use": 1, "function": 1, "approximation": 1, "low": 1, "discount": 1, "factor": 1, "work": 1, "well": 1, "sparsereward": 1, "domains": 1}, {"specifically": 1, "main": 1, "figure": 1, "3": 1, "early": 1, "performance": 3, "observations": 1, "1": 1, "sharp": 1, "drop": 1, "final": 2, "top": 1, "botfor": 1, "discount": 1, "factor": 1, "threshold": 2, "2": 1, "value": 1, "tom": 1, "chain": 1, "task": 1}, {"depend": 1, "tilewidth": 1, "larger": 1, "ones": 1, "result": 1, "worse": 1, "ie": 1, "higher": 1, "threshold": 1, "value": 1, "3": 1, "tabular": 1, "representation": 1, "perform": 1, "well": 1, "discount": 1, "factor": 1}, {"commonly": 1, "believe": 1, "action": 1, "gap": 1, "strong": 1, "influence": 1, "optimization": 1, "process": 1, "bellemare": 1, "et": 1, "al": 1, "2016": 1, "farahmand": 1, "2011": 1}, {"action": 2, "gap": 1, "state": 2, "define": 1, "difference": 1, "q": 1, "best": 2, "second": 1}, {"examine": 1, "common": 1, "belief": 1, "start": 1, "evaluate": 1, "two": 1, "straightforward": 1, "hypotheses": 1, "involve": 1, "action": 4, "gap": 4, "1": 1, "lower": 2, "discount": 2, "factor": 2, "cause": 2, "poor": 2, "performance": 2, "result": 2, "smaller": 2, "2": 1, "relative": 1, "ie": 1, "4": 1, "": 1, "state": 2, "divide": 1, "maximum": 1, "actionvalue": 1}, {"since": 1, "hypotheses": 1, "support": 1, "result": 1, "figure": 1, "3": 1, "perform": 1, "experiment": 1, "test": 1}, {"test": 1, "first": 1, "hypothesis": 1, "perform": 1, "experiment": 1, "reward": 1, "factor": 1, "100": 1, "larger": 1}, {"turn": 1, "increase": 1, "action": 1, "gap": 1, "factor": 1, "100": 1, "well": 1}, {"hence": 1, "validate": 1, "first": 1, "hypothesis": 1, "change": 1, "improve": 1, "ie": 1, "lower": 1, "threshold": 1, "value": 1, "performance": 1, "fall": 1, "flat": 1}, {"test": 1, "second": 1, "hypothesis": 1, "push": 1, "actionvalues": 1, "100": 1, "additional": 1, "reward": 1, "reduce": 1, "relative": 1, "actiongap": 1}, {"hence": 1, "validate": 1, "second": 1, "hypothesis": 1, "performance": 1, "degrade": 1, "variation": 1}, {"however": 1, "neither": 1, "modifications": 1, "cause": 1, "significant": 1, "change": 1, "early": 1, "final": 1, "performance": 1, "invalidate": 1, "hypotheses": 1}, {"correspond": 1, "graph": 1, "find": 1, "supplementary": 1, "material": 1}, {"two": 1, "nave": 1, "actiongap": 2, "hypotheses": 1, "fail": 1, "propose": 1, "alternative": 1, "hypothesis": 1, "lower": 1, "discount": 1, "factor": 1, "cause": 1, "20": 1, "poor": 1, "performance": 1, "result": 1, "larger": 1, "difference": 1, "size": 1, "across": 1, "statespace": 1}, {"illustrate": 1, "statement": 1, "10": 2, "difference": 1, "actiongap": 3, "size": 1, "define": 1, "metric": 1, "call": 1, "deviation": 1, "": 1, "aim": 1, "capture": 1, "notion": 1, "0": 1, "02": 1, "04": 1, "06": 1, "08": 1, "variations": 1}, {"specifically": 1, "let": 2, "x": 1, "random": 1, "variable": 1, "": 2, "subset": 1, "state": 1, "nonzero": 1, "action": 1, "gap": 1}, {"x": 1, "draw": 1, "uniformly": 1, "random": 1, "state": 2, "": 2, "output": 1, "figure": 1, "4": 1, "actiongap": 1, "deviation": 1, "log10": 1, "ags": 2, "action": 1, "gap": 1, "function": 1, "discount": 1, "factor": 1}, {"define": 1, "": 1, "standard": 1, "deviation": 1, "variable": 1, "x": 1}, {"figure": 2, "4": 1, "plot": 1, "": 1, "function": 1, "discount": 1, "factor": 1, "task": 1, "2": 1}, {"test": 1, "new": 1, "hypothesis": 1, "develop": 1, "method": 1, "reduce": 1, "actiongap": 1, "deviation": 1, "": 1, "low": 1, "discount": 1, "factor": 1, "without": 1, "change": 1, "optimal": 1, "policy": 1}, {"next": 1, "section": 1}, {"4": 1, "": 3, "logarithmic": 2, "qlearning": 2, "section": 1, "introduce": 1, "new": 1, "method": 1, "reduce": 1, "actiongap": 1, "deviation": 1, "sparsereward": 1, "domains": 1}, {"present": 1, "method": 2, "three": 1, "step": 2, "add": 1, "layer": 1, "complexity": 1, "order": 1, "extend": 1, "generality": 1}, {"supplementary": 1, "material": 1, "prove": 1, "convergence": 1, "method": 1, "general": 1, "form": 1}, {"first": 1, "step": 1, "consider": 1, "domains": 1, "deterministic": 1, "dynamics": 1, "reward": 1, "either": 1, "positive": 1, "zero": 1}, {"41": 1, "": 2, "deterministic": 1, "domains": 1, "positive": 1, "reward": 1, "method": 1, "base": 1, "general": 1, "approach": 1, "use": 1, "pohlen": 1, "et": 1, "al": 1}, {"2018": 1, "map": 1, "update": 2, "target": 1, "different": 1, "space": 2, "perform": 1, "instead": 1}, {"indicate": 1, "map": 1, "function": 1, "f": 2, "": 2, "inverse": 1, "1": 1}, {"value": 1, "map": 1, "space": 1, "update": 1, "follow": 1, "": 15, "e": 3, "t1": 1, "st": 2, "1": 2, "q": 3, "f": 2, "rt": 1, "max": 1, "st1": 1, "a0": 1}, {"5": 1, "0": 1, "": 1, "e": 1, "equation": 1, "estimate": 2, "expect": 2, "return": 2, "note": 1, "q": 1, "map": 1, "different": 1, "space": 1}, {"obtain": 1, "regular": 1, "qvalue": 1, "inverse": 1, "map": 2, "apply": 1, "e": 1, "update": 1, "occur": 1, "space": 1, "": 1, "measure": 1, "wrt": 1}, {"q": 2, "e": 3, "action": 1, "abest": 1, "": 3, "qs": 1, "a2nd": 1, "best": 1}, {"gap": 1, "state": 1, "define": 1, "map": 2, "space": 1, "qs": 1, "reduce": 1, "": 1, "propose": 1, "use": 1, "logarithmic": 1, "function": 1}, {"specifically": 1, "propose": 1, "follow": 1, "map": 2, "function": 2, "": 8, "3": 1, "6": 1, "2": 1, "inverse": 1, "f": 1, "1": 1, "x": 1, "exdc": 1, "k": 2, "c": 1, "hyperparameters": 1}, {"1": 1, "": 16, "f": 1, "x": 1, "c": 1, "lnx": 1, "k": 4, "reg": 1, "log": 3, "40": 1, "50": 1, "200": 1, "0": 3, "025": 1, "050": 1, "075": 1, "100": 1, "understand": 1, "effect": 1, "6": 1, "plot": 1, "base": 1, "action": 1, "gap": 1, "logarithmic": 1, "space": 1, "variation": 1, "chain": 1, "task": 1, "figure": 1, "2": 1, "use": 1, "rr": 1, "p": 1}, {"also": 1, "plot": 1, "": 1, "base": 1, "action": 1, "regular": 1, "space": 1}, {"figure": 2, "5": 2, "show": 1, "appropriate": 1, "value": 2, "k": 1, "actiongap": 2, "deviation": 2, "almost": 1, "reduce": 1, "0": 1, "low": 1, "function": 1, "discount": 1, "factor": 1}, {"": 1}, {"set": 2, "k": 1, "high": 1, "increase": 2, "deviation": 1, "little": 1, "low": 2, "lot": 1, "5": 1, "": 1, "discount": 1, "factor": 1}, {"short": 1, "k": 1, "control": 1, "smallest": 1, "qvalue": 1, "still": 2, "accurately": 1, "represent": 1, "ie": 1, "action": 1, "gap": 1, "logspace": 1, "significant": 1}, {"roughly": 1, "smallest": 1, "value": 1, "still": 1, "accurately": 1, "represent": 1, "": 2, "k": 1}, {"word": 1, "cutoff": 1, "point": 1, "lie": 1, "approximately": 1, "state": 1, "take": 1, "k": 1, "time": 1, "step": 1, "experience": 1, "1": 1, "reward": 1}, {"set": 1, "k": 1, "high": 1, "cause": 1, "action": 1, "0": 1, "value": 2, "regular": 1, "space": 1, "large": 1, "negative": 1, "logspace": 1}, {"increase": 2, "action": 1, "gap": 1, "substantially": 1, "correspond": 1, "state": 1, "thus": 1, "result": 1, "overall": 1, "actiongap": 1, "deviation": 1}, {"parameters": 1, "c": 1, "scale": 1, "shift": 1, "value": 1, "logarithmic": 1, "space": 1, "effect": 1, "actiongap": 1, "deviation": 1}, {"parameter": 1, "control": 1, "initialization": 1, "qvalues": 1}, {"set": 1, "follow": 1, "": 7, "c": 2, "lnqinit": 1, "k": 2, "7": 1, "1": 1, "ensure": 1, "f": 1, "0": 1, "qinit": 1, "value": 1}, {"useful": 1, "practice": 1, "eg": 1, "e": 2, "enable": 1, "standard": 1, "initialization": 1, "methods": 1, "produce": 1, "use": 1, "neural": 1, "network": 1, "represent": 1, "q": 2, "value": 2, "correspond": 1, "qinit": 1, "output": 1, "around": 1, "0": 1, "still": 1, "ensure": 1, "initialize": 1, "regular": 1, "space": 1}, {"parameter": 1, "c": 1, "scale": 1, "value": 1, "logspace": 1}, {"tabular": 1, "linear": 1, "methods": 1, "scale": 1, "value": 1, "affect": 1, "optimization": 1, "process": 1}, {"nevertheless": 1, "deep": 1, "rl": 1, "methods": 1, "advance": 1, "optimization": 2, "techniques": 1, "commonly": 1, "use": 1, "thus": 1, "scale": 1, "impact": 1, "process": 1, "significantly": 1}, {"experiment": 2, "except": 1, "deep": 1, "rl": 1, "fix": 1, "accord": 1, "equation": 1, "qinit": 1, "": 2, "0": 1, "use": 1, "c": 1, "1": 1}, {"stochastic": 2, "environments": 1, "approach": 1, "describe": 1, "section": 1, "cause": 1, "issue": 1, "average": 2, "sample": 1, "logspace": 2, "produce": 1, "underestimate": 1, "compare": 1, "regular": 1, "space": 1, "map": 1, "result": 1}, {"specifically": 1, "x": 1, "random": 1, "variable": 1, "e": 1, "lnx": 1, "": 1, "ln": 1, "ex": 1, "ie": 1, "jensens": 1, "inequality": 1}, {"fortunately": 1, "within": 1, "specific": 1, "context": 1, "way": 1, "around": 1, "limitation": 1, "discuss": 1, "next": 1, "section": 1}, {"42": 1, "": 3, "stochastic": 2, "domains": 1, "positive": 1, "reward": 1, "stepsize": 1, "generally": 1, "conflate": 1, "two": 1, "form": 1, "average": 3, "update": 1, "target": 1, "due": 1, "environment": 1, "stochasticity": 1, "case": 1, "function": 1, "approximation": 1, "different": 1, "state": 1}, {"amend": 1, "method": 1, "stochastic": 2, "environments": 1, "ideally": 1, "would": 1, "separate": 1, "form": 1, "average": 3, "perform": 1, "update": 1, "target": 1, "regular": 1, "space": 1, "different": 1, "state": 1, "logspace": 1}, {"separation": 1, "hard": 1, "achieve": 2, "approach": 1, "present": 1, "inspire": 1, "observation": 1, "many": 1, "e": 1, "f": 1, "q": 1, "": 1, "even": 1, "environment": 1, "stochastic": 1}, {"benefit": 1}, {"particular": 1, "enable": 1, "convergence": 1, "q": 1, "let": 1, "log": 1, "stepsize": 2, "average": 2, "logspace": 1, "reg": 1, "regular": 1, "space": 1}, {"amend": 1, "approach": 1, "previous": 1, "section": 1, "compute": 1, "alternative": 1, "update": 1, "target": 1, "base": 1, "perform": 1, "average": 1, "operation": 1, "regular": 1, "space": 1}, {"specifically": 1, "update": 2, "target": 2, "ut": 4, "transform": 1, "alternative": 1, "follow": 1, "": 12, "e": 3, "st": 2, "reg": 1, "f": 2, "1": 2, "q": 2, "8": 1, "st1": 1, "a0": 1}, {"modify": 1, "update": 2, "target": 1, "ut": 3, "use": 1, "": 16, "rt": 1, "maxa0": 1, "f": 2, "1": 1, "q": 3, "logspace": 1, "e": 3, "t1": 1, "st": 3, "log": 1}, {"q": 1, "9": 2, "": 8, "rms": 1, "error": 1, "note": 1, "reg": 1, "1": 1, "ut": 2, "update": 2, "reduce": 1, "5": 1, "previous": 1, "section": 1, "log": 1}, {"020": 1, "condition": 2, "convergence": 1, "discuss": 1, "next": 1, "section": 1, "log": 2, "": 4, "0001": 1, "reg": 3, "1": 1, "001": 1, "01": 1, "one": 1, "go": 1, "0": 1, "limit": 1}, {"015": 1, "log": 2, "": 3, "1": 1, "reg": 3, "0001": 1, "practical": 1, "point": 1, "view": 1, "use": 1, "fix": 1, "value": 2, "010": 1, "set": 1, "sufficiently": 1, "small": 1, "keep": 1, "005": 1, "underestimation": 1, "due": 1, "average": 1, "logspace": 1, "0000": 1, "250": 1, "500": 1, "750": 1, "1000": 1, "control": 1}, {"illustrate": 1, "plot": 1, "rms": 2, "error": 2, "": 4, "update": 1, "sweep": 1, "positivereward": 1, "variant": 1, "chain": 1, "task": 1, "rr": 1, "0": 1, "rl": 1, "1": 1, "p": 1, "figure": 1, "6": 1}, {"025": 1}, {"rms": 1, "error": 1, "plot": 1, "base": 1, "difference": 1, "1": 1, "e": 1, "": 1, "f": 1, "qs": 1, "q": 1, "stateaction": 1, "pair": 1}, {"use": 2, "tilewidth": 1, "1": 1, "correspond": 1, "6": 1, "": 2, "tabular": 1, "representation": 1, "k": 1, "200": 1}, {"note": 1, "reg": 1, "": 1, "1": 1, "reduce": 1, "method": 1, "one": 1, "previous": 1, "section": 1, "error": 1, "never": 1, "come": 1, "close": 1, "zero": 1}, {"43": 1, "": 2, "stochastic": 1, "domains": 1, "positive": 2, "andor": 1, "negative": 2, "reward": 2, "consider": 1, "general": 1, "case": 1, "zero": 1}, {"might": 1, "seem": 1, "generalize": 1, "negative": 2, "reward": 1, "simply": 1, "replace": 1, "x": 3, "map": 1, "function": 1, "6": 1, "": 2, "sufficiently": 1, "large": 1, "constant": 1, "prevent": 1, "become": 1}, {"problem": 1, "approach": 1, "demonstrate": 1, "empirically": 1, "decrease": 1, "": 1, "low": 1, "discount": 1, "factor": 1}, {"hence": 1, "section": 1, "present": 1, "alternative": 1, "approach": 1, "base": 1, "decompose": 1, "qvalue": 1, "function": 2, "two": 1}, {"consider": 1, "decomposition": 1, "reward": 1, "rt": 8, "two": 1, "components": 1, "": 10, "follow": 1, "r": 1, "0": 2}, {"0": 2, "otherwise": 2, "": 4, "10": 1, "note": 1, "rt": 5, "always": 1, "nonnegative": 1, "time": 1}, {"decompose": 1, "observe": 1, "reward": 2, "manner": 1, "two": 2, "components": 1, "use": 1, "train": 1, "separate": 1, "e": 2, "": 7, "represent": 1, "value": 1, "function": 2, "map": 1, "space": 1, "correspond": 1, "qvalue": 1, "q": 2, "rt": 2, "play": 1, "role": 1}, {"train": 1, "value": 1, "function": 1, "construct": 1, "follow": 1, "update": 1, "target": 1, "": 23, "1": 4, "e": 4, "ut": 1, "rt": 1, "f": 3, "q": 3, "u": 1, "r": 1, "11": 1, "t1": 4, "0": 1, "at1": 1, "arg": 1, "maxa0": 1, "qt": 1, "st1": 1, "a0": 1}, {"update": 2, "target": 1, "st1": 1, "": 6, "f": 1, "e": 2, "q": 2, "modify": 1, "ut": 2, "respectively": 2, "base": 2, "8": 1, "use": 1, "9": 1}, {"actionselection": 1, "time": 1, "base": 1, "qt": 2, "": 8, "define": 1, "follow": 1, "1": 2, "e": 2, "f": 2, "q": 2, "12": 1, "supplementary": 1, "material": 1, "prove": 1, "convergence": 1, "logarithmic": 1, "qlearning": 2, "similar": 1, "condition": 1, "regular": 1}, {"particular": 1, "product": 1, "logt": 1, "": 1, "regt": 1, "satisfy": 1, "condition": 1, "regular": 1, "qlearning": 1}, {"one": 1, "additional": 1, "condition": 1, "regt": 1, "": 1, "state": 1, "go": 1, "zero": 1, "limit": 1}, {"compute": 1, "": 1, "full": 1, "version": 1, "chain": 1, "task": 1}, {"reg": 1, "log": 3, "bias": 1, "e": 2, "": 4, "q": 2, "generalize": 1, "20": 1, "two": 1, "function": 1, "plusonly": 1, "minonly": 1, "definition": 1, "situation": 1}, {"consider": 1, "three": 1, "generalizations": 1, "log": 4, "e": 4, "": 7, "plusonly": 1, "2": 1, "10": 2, "1": 1, "base": 3, "actiongaps": 2, "q": 3, "minonly": 1, "3": 1, "0": 1, "02": 1, "04": 1, "06": 1, "08": 1}, {"furthermore": 1, "actiongaps": 1, "q": 1, "plot": 1, "version": 1, "resolve": 1, "issue": 1, "negative": 1, "reward": 1, "navely": 1, "add": 1, "value": 1, "": 1, "1": 1, "input": 1, "logfunction": 1, "log": 1, "bias": 1}, {"plot": 1, "": 3, "variants": 1, "figure": 2, "7": 2, "use": 1, "k": 1, "200": 1, "actiongap": 1, "deviation": 1, "together": 1, "regular": 1, "qlearning": 1, "reg": 1}, {"interestingly": 1, "function": 1, "discount": 1, "factor": 1}, {"log": 1, "plusonly": 1, "variant": 1, "": 1, "small": 1, "discount": 1, "factor": 1}, {"analysis": 1, "show": 1, "reason": 1, "optimal": 1, "policy": 1, "chance": 1, "agent": 1, "move": 1, "state": 3, "close": 1, "positive": 1, "terminal": 2, "negative": 1, "small": 2, "mean": 1, "k": 1, "": 2, "200": 1, "make": 1, "actiongaps": 1, "e": 1, "homogeneous": 1}, {"however": 1, "see": 1, "next": 1, "section": 1, "performance": 1, "k": 1, "": 2, "200": 1, "q": 2, "e": 1, "good": 1, "discount": 1, "factor": 1, "demonstrate": 1, "homogeneous": 1, "actiongaps": 1, "huge": 1, "issue": 1}, {"argue": 1, "could": 1, "behavior": 1, "relate": 1, "nature": 1, "positive": 2, "negative": 2, "reward": 3, "might": 1, "worthwhile": 1, "travel": 1, "long": 1, "distance": 1, "get": 1, "avoid": 1, "typically": 1, "shorthorizon": 1, "challenge": 1}, {"7": 1, "": 1, "test": 1, "method": 1, "return": 1, "full": 1, "version": 1, "chain": 1, "task": 1, "performance": 1, "metric": 1, "f": 1, "use": 1, "section": 1, "32": 1, "measure": 1, "whether": 1, "greedy": 1, "policy": 1, "optimal": 1}, {"use": 2, "k": 1, "": 5, "200": 1, "reg": 2, "01": 1, "log": 2, "001": 1, "value": 2, "equal": 1, "section": 1, "32": 1}, {"figure": 1, "8": 1, "plot": 1, "result": 1, "early": 1, "learn": 1, "well": 1, "final": 1, "performance": 1}, {"compare": 1, "graph": 2, "figure": 1, "3": 1, "show": 1, "logarithmic": 1, "qlearning": 2, "successfully": 1, "resolve": 1, "optimization": 1, "issue": 1, "regular": 1, "relate": 1, "use": 1, "low": 1, "discount": 1, "factor": 1, "conjunction": 1, "function": 1, "approximation": 1}, {"combine": 1, "observation": 1, "section": 1, "31": 1, "best": 1, "discount": 1, "factor": 1, "taskdependent": 1, "convergence": 1, "proof": 1, "supplementary": 1, "material": 1, "guarantee": 1, "logarithmic": 2, "qlearning": 4, "converge": 1, "policy": 1, "regular": 1, "result": 1, "demonstrate": 1, "able": 1, "solve": 2, "task": 1, "challenge": 1}, {"specifically": 1, "finitehorizon": 1, "performance": 2, "metric": 2, "use": 1, "task": 1, "gap": 1, "substantially": 1, "smaller": 1, "lower": 1, "discount": 2, "factor": 2, "fall": 1, "flat": 1, "due": 1, "function": 1, "approximation": 1}, {"average": 2, "performance": 5, "": 14, "experiment": 1, "5": 3, "10": 4, "08": 4, "06": 4, "04": 4, "w": 8, "1": 2, "2": 2, "3": 2, "00": 2, "02": 4, "figure": 1, "8": 1, "early": 1, "finally": 1, "test": 1, "approach": 1, "complex": 1, "set": 1, "compar": 1, "top": 1, "final": 1, "boting": 1, "dqn": 1, "mnih": 1, "et": 1, "al": 1, "2015": 1, "variant": 1, "tom": 1, "chain": 1, "task": 1}, {"implement": 1, "method": 1, "refer": 1, "logdqn3": 1, "enable": 1, "easy": 1, "baseline": 1, "comparisons": 1, "use": 1, "dopamine": 1, "framework": 1, "experiment": 1, "castro": 1, "et": 1, "al": 1, "2018": 1}, {"framework": 1, "contain": 2, "opensource": 1, "code": 1, "several": 1, "important": 1, "deep": 1, "rl": 1, "methods": 2, "also": 1, "result": 1, "obtain": 1, "set": 1, "60": 1, "game": 1, "arcade": 1, "learn": 1, "environment": 1, "bellemare": 1, "et": 2, "al": 2, "2013": 1, "machado": 1, "2018": 1}, {"mean": 1, "direct": 1, "comparison": 1, "important": 1, "baselines": 1, "possible": 1}, {"logdqn": 1, "implementation": 1, "consist": 1, "modification": 1, "dopamines": 1, "dqn": 1, "code": 1}, {"specifically": 1, "e": 4, "": 6, "q": 3, "final": 1, "output": 1, "layer": 1, "order": 1, "adapt": 1, "dqns": 1, "model": 1, "provide": 1, "estimate": 2, "half": 1}, {"double": 1, "size": 1, "half": 1, "use": 1, "estimate": 1, "q": 3, "": 4, "e": 4, "update": 1, "layer": 1, "share": 1, "remain": 1, "unchanged": 1}, {"q": 3, "use": 2, "sample": 1, "replay": 1, "memory": 2, "require": 1, "modification": 1, "footprint": 1, "e": 2, "": 2, "update": 1, "simultaneously": 1, "single": 1, "pass": 1, "change": 1}, {"furthermore": 1, "q": 1, "model": 1, "computational": 1, "cost": 1, "logdqn": 1, "dqn": 1, "similar": 1}, {"implementation": 1, "detail": 1, "provide": 1, "supplementary": 1, "material": 1}, {"publish": 1, "dopamine": 1, "baselines": 1, "obtain": 1, "stochastic": 1, "version": 1, "atari": 1, "use": 1, "sticky": 1, "action": 3, "machado": 1, "et": 1, "al": 1, "2018": 1, "25": 1, "probability": 1, "environment": 1, "execute": 1, "previous": 1, "time": 1, "step": 1, "instead": 1, "agents": 1, "new": 1}, {"hence": 1, "conduct": 1, "logdqn": 1, "experiment": 1, "stochastic": 1, "version": 1, "atari": 1, "well": 1}, {"dopamine": 1, "provide": 1, "baselines": 1, "60": 1, "game": 3, "total": 1, "consider": 1, "subset": 1, "55": 1, "human": 1, "score": 2, "publish": 1, "humannormalized": 1, "compute": 1, "define": 1, "scoreagent": 1, "": 2, "scorerandom": 1}, {"scorehuman": 1, "": 1, "scorerandom": 1, "use": 1, "table": 1, "2": 1, "wang": 1, "et": 1, "al": 1}, {"2016": 1, "retrieve": 1, "human": 1, "random": 1, "score": 1}, {"13": 1, "": 1, "optimize": 1, "hyperparameters": 1, "use": 1, "subset": 1, "6": 1, "game": 1}, {"particular": 1, "perform": 1, "scan": 1, "discount": 1, "factor": 1, "": 5, "084": 1, "099": 1}, {"dqn": 1, "": 4, "099": 1, "optimal": 1, "logdqn": 1, "best": 1, "value": 1, "range": 1, "096": 1}, {"try": 1, "lower": 1, "": 5, "value": 1, "well": 1, "01": 1, "05": 1, "improve": 1, "overall": 1, "performance": 1, "6": 1, "game": 1}, {"hyperparameters": 1, "logdqn": 1, "use": 1, "k": 1, "": 4, "100": 1, "c": 1, "05": 1, "log": 1, "00025": 1, "reg": 1, "01": 1}, {"3": 1, "": 5, "code": 1, "experiment": 1, "find": 1, "httpsgithubcommicrosoftlogrl": 1, "8": 1, "doubledunk": 1, "ski": 1, "stargunner": 1, "kangaroo": 1, "krull": 1, "assault": 1, "icehockey": 1, "jamesbond": 1, "hero": 1, "beamrider": 1, "amidar": 1, "centipede": 1, "gopher": 1, "mspacman": 1, "riverraid": 1, "timepilot": 1, "alien": 1, "solaris": 1, "venture": 1, "kungfumaster": 1, "asteroids": 1, "bowl": 1, "montezumarevenge": 1, "pitfall": 1, "fishingderby": 1, "privateeye": 1, "gravitar": 1, "robotank": 1, "berzerk": 1, "seaquest": 1, "box": 1, "yarsrevenge": 1, "demonattack": 1, "pong": 1, "phoenix": 1, "crazyclimber": 1, "bankheist": 1, "atlantis": 1, "qbert": 1, "upndown": 1, "battlezone": 1, "roadrunner": 1, "enduro": 1, "choppercommand": 1, "freeway": 1, "namethisgame": 1, "tennis": 1, "spaceinvaders": 1, "wizardofwor": 1, "zaxxon": 1, "tutankham": 1, "asterix": 1, "frostbite": 1, "breakout": 1, "videopinball": 1, "300": 1, "200": 1, "100": 2, "0": 1, "figure": 1, "10": 1, "relative": 1, "performance": 1, "logdqn": 1, "wrt": 1}, {"dqn": 2, "positive": 1, "percentage": 1, "mean": 1, "logdqn": 1, "outperform": 1}, {"orange": 1, "bar": 3, "indicate": 3, "performance": 3, "difference": 3, "larger": 1, "50": 2, "darkblue": 1, "10": 2, "lightblue": 1, "smaller": 1}, {"product": 1, "log": 1, "reg": 1, "000025": 1, "value": 1, "default": 1, "stepsize": 1, "": 1, "dqn": 1}, {"use": 1, "different": 1, "value": 1, "positive": 2, "negative": 2, "head": 3, "set": 1, "base": 1, "7": 1, "qinit": 2, "": 2, "1": 1, "0": 1}, {"result": 1, "hyperparameter": 1, "optimization": 1, "well": 1, "implementation": 1, "detail": 1, "provide": 1, "supplementary": 1, "material": 1}, {"figure": 1, "10": 1, "show": 1, "performance": 1, "logdqn": 1, "compare": 1, "dqn": 1, "per": 1, "game": 1, "use": 2, "comparison": 1, "equation": 1, "wang": 1, "et": 1, "al": 1}, {"2016": 1, "scorelogdqn": 1, "": 2, "scoredqn": 1}, {"maxscoredqn": 1, "": 3, "scorehuman": 1, "scorerandom": 1, "scorelogdqndqn": 1, "compute": 1, "average": 1, "last": 2, "10": 1, "learn": 1, "curve": 1, "ie": 1, "20": 1, "epochs": 1}, {"figure": 1, "9": 1, "show": 1, "mean": 1, "median": 1, "humannormalized": 1, "score": 1, "logdqn": 1, "well": 1, "dqn": 1}, {"also": 1, "plot": 1, "performance": 1, "baselines": 1, "dopamine": 1, "provide": 1, "c51": 1, "bellemare": 1, "et": 3, "al": 3, "2017": 1, "implicit": 1, "quantile": 1, "network": 1, "dabney": 1, "2018": 2, "rainbow": 1, "hessel": 1}, {"baselines": 2, "reference": 1, "attempt": 1, "combine": 1, "technique": 1, "techniques": 1, "make": 1, "use": 1}, {"6": 1, "": 4, "mean": 1, "median": 1, "discussion": 1, "future": 1, "work": 1, "result": 1, "provide": 1, "strong": 1, "evidence": 1, "hypothesis": 1, "large": 1, "differences": 1, "actiongap": 1, "size": 1, "detrimental": 1, "performance": 1, "approximate": 1, "rl": 1}, {"possible": 1, "explanation": 1, "could": 1, "optimize": 1, "l2": 1, "norm": 1, "3": 1, "might": 1, "drive": 1, "towards": 1, "average": 1, "squarederror": 1, "similar": 1, "across": 1, "statespace": 1}, {"however": 1, "errorlandscape": 1, "require": 1, "bring": 1, "approximation": 1, "error": 1, "actiongap": 2, "across": 2, "statespace": 2, "different": 2, "shape": 1, "order": 1, "magnitude": 1, "size": 1}, {"mismatch": 1, "require": 1, "errorlandscape": 1, "produce": 1, "l2": 1, "norm": 1, "might": 1, "lead": 1, "ineffective": 1, "use": 1, "function": 1, "approximator": 1}, {"experiment": 1, "need": 1, "confirm": 1}, {"figure": 1, "9": 1, "humannormalized": 1, "mean": 1, "leave": 1, "median": 1, "right": 1, "score": 1, "55": 1, "atari": 1, "game": 1, "logdqn": 1, "various": 1, "algorithms": 1}, {"strong": 1, "performance": 1, "observe": 1, "": 2, "096": 1, "deep": 1, "rl": 1, "set": 1, "unlikely": 1, "solely": 1, "due": 1, "difference": 1, "metric": 1, "gap": 1}, {"suspect": 1, "also": 1, "effect": 1, "play": 1, "make": 1, "logdqn": 1, "effective": 1}, {"hand": 1, "much": 1, "lower": 1, "discount": 2, "factor": 2, "performance": 1, "good": 1, "high": 1}, {"believe": 1, "possible": 1, "reason": 1, "could": 1, "since": 1, "low": 2, "value": 1, "different": 1, "original": 1, "dqn": 2, "settings": 1, "hyperparameters": 1, "might": 1, "longer": 1, "ideal": 1, "discount": 1, "factor": 1, "region": 1}, {"interest": 1, "future": 1, "direction": 1, "would": 1, "reevaluate": 1, "hyperparameters": 1, "low": 1, "discount": 1, "factor": 1, "region": 1}, {"9": 1, "": 1, "acknowledgments": 1, "like": 1, "thank": 1, "kimia": 1, "nadjahi": 1, "contributions": 1, "convergence": 1, "proof": 1, "early": 1, "version": 1, "logarithmic": 1, "qlearning": 1}, {"early": 1, "version": 2, "ultimately": 1, "replace": 1, "significantly": 1, "improve": 1, "require": 1, "different": 1, "convergence": 1, "proof": 1}, {"reference": 1, "dimitri": 1, "p": 1, "bertsekas": 1, "john": 1, "n": 1, "tsitsiklis": 1}, {"neurodynamic": 1, "program": 1}, {"athena": 1, "scientific": 1, "1996": 1}, {"volodymyr": 1, "mnih": 1, "koray": 1, "kavukcuoglu": 1, "david": 1, "silver": 1, "andrei": 1, "rusu": 1, "joel": 1, "veness": 1, "marc": 1, "g": 1, "bellemare": 1, "alex": 1, "grave": 1, "martin": 1, "riedmiller": 1, "andreas": 1, "k": 1, "fidjeland": 1, "georg": 1, "ostrovski": 1, "stig": 1, "petersen": 1, "charles": 1, "beattie": 1, "amir": 1, "sadik": 1, "ioannis": 1, "antonoglou": 1, "helen": 1, "king": 1, "dharshan": 1, "kumaran": 1, "daan": 1, "wierstra": 1, "shane": 1, "legg": 1, "demis": 1, "hassabis": 1}, {"humanlevel": 1, "control": 1, "deep": 1, "reinforcement": 1, "learn": 1}, {"nature": 1, "5187540529533": 1, "2015": 1}, {"martin": 1, "l": 1, "puterman": 1}, {"markov": 1, "decision": 1, "process": 1, "discrete": 1, "stochastic": 1, "dynamic": 1, "program": 1}, {"john": 1, "wiley": 1, "": 1, "sons": 1, "1994": 1}, {"christopher": 1, "j": 1, "c": 1, "h": 1, "watkins": 1, "peter": 1, "dayan": 1}, {"qlearning": 1}, {"machine": 1, "learn": 1, "83279292": 1, "1992": 1}, {"tommi": 1, "jaakkola": 1, "michael": 1, "jordan": 1, "satinder": 1, "p": 1, "singh": 1}, {"convergence": 1, "stochastic": 1, "iterative": 1, "dynamic": 1, "program": 1, "algorithms": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "page": 1, "703710": 1, "1994": 1}, {"fabio": 1, "pardo": 1, "arash": 1, "tavakoli": 1, "vitaly": 1, "levdik": 1, "petar": 1, "kormushev": 1}, {"time": 1, "limit": 1, "reinforcement": 1, "learn": 1}, {"proceed": 1, "35th": 1, "international": 1, "conference": 1, "machine": 1, "learn": 1, "volume": 1, "80": 1, "page": 1, "40454054": 1, "2018": 1}, {"richard": 1, "sutton": 1}, {"generalization": 1, "reinforcement": 1, "learn": 1, "successful": 1, "examples": 1, "use": 1, "sparse": 1, "coarse": 1, "cod": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "page": 1, "10381044": 1, "1996": 1}, {"marc": 1, "g": 1, "bellemare": 1, "georg": 1, "ostrovski": 1, "arthur": 1, "guez": 1, "philip": 1, "thomas": 1, "rmi": 1, "munos": 1}, {"increase": 1, "action": 1, "gap": 1, "new": 1, "operators": 1, "reinforcement": 1, "learn": 1}, {"proceed": 1, "30th": 1, "aaai": 1, "conference": 1, "artificial": 1, "intelligence": 1, "page": 1, "14761483": 1, "2016": 1}, {"amirmassoud": 1, "farahmand": 1}, {"actiongap": 1, "phenomenon": 1, "reinforcement": 1, "learn": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "page": 1, "172180": 1, "2011": 1}, {"tobias": 1, "pohlen": 1, "bilal": 1, "piot": 1, "todd": 1, "hester": 1, "mohammad": 1, "gheshlaghi": 1, "azar": 1, "dan": 1, "horgan": 1, "david": 1, "budden": 1, "gabriel": 1, "barthmaron": 1, "hado": 1, "van": 1, "hasselt": 1, "john": 1, "quan": 1, "mel": 1, "vecerk": 1, "matteo": 1, "hessel": 1, "rmi": 1, "munos": 1, "olivier": 1, "pietquin": 1}, {"observe": 1, "look": 1, "achieve": 1, "consistent": 1, "performance": 1, "atari": 1}, {"arxiv": 1, "preprint": 1, "arxiv180511593": 1, "2018": 1}, {"pablo": 1, "samuel": 1, "castro": 1, "subhodeep": 1, "moitra": 1, "carles": 1, "gelada": 1, "saurabh": 1, "kumar": 1, "marc": 1, "g": 1, "bellemare": 1}, {"dopamine": 1, "research": 1, "framework": 1, "deep": 1, "reinforcement": 1, "learn": 1}, {"arxiv": 1, "preprint": 1, "arxiv181206110": 1, "2018": 1}, {"marc": 1, "g": 1, "bellemare": 1, "yavar": 1, "naddaf": 1, "joel": 1, "veness": 1, "michael": 1, "bowl": 1}, {"arcade": 1, "learn": 1, "environment": 1, "evaluation": 1, "platform": 1, "general": 1, "agents": 1}, {"journal": 1, "artificial": 1, "intelligence": 1, "research": 1, "47": 1, "253279": 1, "2013": 1}, {"marlos": 1, "c": 1, "machado": 1, "marc": 1, "g": 1, "bellemare": 1, "erik": 1, "talvitie": 1, "joel": 1, "veness": 1, "matthew": 1, "hausknecht": 1, "michael": 1, "bowl": 1}, {"revisit": 1, "arcade": 1, "learn": 1, "environment": 1, "evaluation": 1, "protocols": 1, "open": 1, "problems": 1, "general": 1, "agents": 1}, {"journal": 1, "artificial": 1, "intelligence": 1, "research": 1, "61523562": 1, "2018": 1}, {"ziyu": 1, "wang": 1, "tom": 1, "schaul": 1, "matteo": 1, "hessel": 1, "hado": 1, "van": 1, "hasselt": 1, "marc": 1, "lanctot": 1, "nando": 1, "de": 1, "freitas": 1}, {"duel": 1, "network": 1, "architectures": 1, "deep": 1, "reinforcement": 1, "learn": 1}, {"proceed": 1, "33rd": 1, "international": 1, "conference": 1, "machine": 1, "learn": 1, "volume": 1, "48": 1, "page": 1, "19952003": 1, "2016": 1}, {"marc": 1, "g": 1, "bellemare": 1, "dabney": 1, "rmi": 1, "munos": 1}, {"distributional": 1, "perspective": 1, "reinforcement": 1, "learn": 1}, {"proceed": 1, "34th": 1, "international": 1, "conference": 1, "machine": 1, "learn": 1, "volume": 1, "70": 1, "page": 1, "449458": 1, "2017": 1}, {"10": 1, "": 1, "dabney": 1, "georg": 1, "ostrovski": 1, "david": 1, "silver": 1, "rmi": 1, "munos": 1}, {"implicit": 1, "quantile": 1, "network": 1, "distributional": 1, "reinforcement": 1, "learn": 1}, {"proceed": 1, "35th": 1, "international": 1, "conference": 1, "machine": 1, "learn": 1, "volume": 1, "80": 1, "page": 1, "10961105": 1, "2018": 1}, {"matteo": 1, "hessel": 1, "joseph": 1, "modayil": 1, "hado": 1, "van": 1, "hasselt": 1, "tom": 1, "schaul": 1, "georg": 1, "ostrovski": 1, "dabney": 1, "dan": 1, "horgan": 1, "bilal": 1, "piot": 1, "mohammad": 1, "azar": 1, "david": 1, "silver": 1}, {"rainbow": 1, "combine": 1, "improvements": 1, "deep": 1, "reinforcement": 1, "learn": 1}, {"proceed": 1, "32nd": 1, "aaai": 1, "conference": 1, "artificial": 1, "intelligence": 1, "page": 1, "32153222": 1, "2018": 1}, {"11": 1}]
[{"distributional": 1, "reward": 2, "decomposition": 1, "reinforcement": 2, "learn": 2, "": 5, "zichuan": 1, "lin": 1, "tsinghua": 2, "university": 2, "linzc16mailstsinghuaeducn": 1, "tao": 1, "qin": 1, "microsoft": 3, "research": 3, "taoqinmicrosoftcom": 1, "li": 1, "zhao": 1, "lizomicrosoftcom": 1, "guangwen": 1, "yang": 2, "ygwtsinghuaeducn": 1, "derek": 1, "uc": 1, "san": 1, "diego": 1, "dyang1206gmailcom": 1, "tieyan": 1, "liu": 1, "tyliumicrosoftcom": 1, "abstract": 1, "many": 1, "rl": 2, "task": 2, "specific": 1, "properties": 2, "leverage": 1, "modify": 1, "exist": 1, "algorithms": 1, "adapt": 1, "improve": 1, "performance": 1, "general": 1, "class": 1, "multiple": 1, "channel": 1}, {"environments": 1, "full": 1, "reward": 1, "decompose": 1, "subrewards": 1, "obtain": 1, "different": 1, "channel": 1}, {"exist": 1, "work": 1, "reward": 3, "decomposition": 1, "either": 1, "require": 1, "prior": 2, "knowledge": 2, "environment": 1, "decompose": 2, "full": 1, "without": 1, "degrade": 1, "performance": 1}, {"paper": 1, "propose": 1, "distributional": 2, "reward": 3, "decomposition": 2, "reinforcement": 1, "learn": 1, "drdrl": 1, "novel": 1, "algorithm": 1, "capture": 1, "multiple": 1, "channel": 1, "structure": 1, "set": 1}, {"empirically": 1, "method": 1, "capture": 1, "multichannel": 1, "structure": 1, "discover": 1, "meaningful": 1, "reward": 1, "decomposition": 1, "without": 1, "requirements": 1, "prior": 1, "knowledge": 1}, {"consequently": 1, "agent": 1, "achieve": 1, "better": 1, "performance": 1, "exist": 1, "methods": 1, "environments": 1, "multiple": 1, "reward": 1, "channel": 1}, {"1": 1, "": 2, "introduction": 1, "reinforcement": 1, "learn": 1, "achieve": 1, "great": 1, "success": 1, "decision": 1, "make": 1, "problems": 1, "since": 1, "deep": 1, "qlearning": 1, "propose": 1, "mnih": 1, "et": 1, "al": 1}, {"2015": 1}, {"general": 2, "rl": 3, "algorithms": 2, "deeply": 1, "study": 1, "focus": 1, "task": 1, "specific": 1, "properties": 1, "utilize": 1, "modify": 1, "achieve": 1, "better": 1, "performance": 1}, {"specifically": 1, "focus": 1, "rl": 1, "environments": 1, "multiple": 1, "reward": 2, "channel": 1, "full": 1, "available": 1}, {"reward": 1, "decomposition": 1, "propose": 1, "investigate": 1, "properties": 1}, {"example": 1, "atari": 1, "game": 1, "seaquest": 1, "reward": 1, "environment": 1, "decompose": 1, "subrewards": 1, "shoot": 1, "shark": 1, "rescue": 1, "divers": 1}, {"reward": 2, "decomposition": 1, "view": 1, "total": 1, "sum": 1, "subrewards": 1, "usually": 1, "disentangle": 1, "obtain": 1, "independently": 1, "sprague": 1, "ballard": 1, "2003": 2, "russell": 1, "zimdars": 1, "van": 1, "seijen": 1, "et": 1, "al": 1}, {"2017": 1, "grimm": 1, "singh": 1, "2019": 1, "aim": 1, "decompose": 1, "total": 1, "reward": 1, "subrewards": 1}, {"subrewards": 1, "may": 1, "leverage": 1, "learn": 1, "better": 1, "policies": 1}, {"van": 1, "seijen": 1, "et": 1, "al": 1}, {"2017": 1, "propose": 1, "split": 1, "state": 1, "different": 1, "substates": 1, "subreward": 1, "obtain": 1, "train": 1, "general": 1, "value": 2, "function": 2, "learn": 1, "multiple": 1, "subrewards": 1}, {"architecture": 1, "rather": 1, "limit": 1, "due": 1, "require": 1, "prior": 1, "knowledge": 1, "split": 1, "substates": 1}, {"grimm": 1, "singh": 1, "2019": 1, "propose": 1, "general": 1, "method": 1, "reward": 1, "decomposition": 1, "via": 1, "maximize": 1, "": 2, "contribute": 1, "internship": 1, "microsoft": 1, "research": 1}, {"33rd": 1, "conference": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "neurips": 1, "2019": 1, "vancouver": 1, "canada": 1}, {"disentanglement": 1, "subrewards": 1}, {"work": 1, "explicit": 1, "reward": 1, "decomposition": 1, "learn": 1, "via": 1, "maximize": 1, "disentanglement": 1, "two": 1, "subrewards": 1, "estimate": 1, "actionvalue": 1, "function": 1}, {"however": 1, "work": 1, "require": 1, "environment": 1, "reset": 1, "arbitrary": 1, "state": 2, "apply": 1, "general": 1, "rl": 1, "settings": 1, "hardly": 1, "revisit": 1}, {"furthermore": 1, "despite": 1, "meaningful": 1, "reward": 2, "decomposition": 2, "achieve": 1, "fail": 1, "utilize": 1, "learn": 1, "better": 1, "policies": 1}, {"paper": 1, "propose": 1, "distributional": 2, "reward": 2, "decomposition": 1, "reinforcement": 1, "learn": 1, "drdrl": 1, "rl": 2, "algorithm": 1, "capture": 1, "latent": 1, "multiplechannel": 1, "structure": 1, "set": 1}, {"distributional": 1, "rl": 3, "differ": 1, "valuebased": 2, "estimate": 1, "distribution": 1, "rather": 1, "expectation": 1, "return": 1, "therefore": 1, "capture": 1, "richer": 1, "information": 1}, {"propose": 1, "rl": 1, "algorithm": 1, "estimate": 1, "distributions": 1, "subreturns": 2, "combine": 1, "get": 1, "distribution": 1, "total": 1, "return": 1}, {"order": 1, "avoid": 1, "naive": 1, "decomposition": 1, "01": 1, "halfhalf": 1, "propose": 1, "disentanglement": 1, "regularization": 1, "term": 1, "encourage": 1, "subreturns": 1, "diverge": 1}, {"better": 1, "separate": 1, "reward": 1, "channel": 2, "also": 1, "design": 1, "network": 1, "learn": 1, "different": 2, "state": 1, "representations": 1}, {"test": 1, "algorithm": 1, "choose": 1, "atari": 1, "game": 1, "multiple": 1, "reward": 1, "channel": 1}, {"empirically": 1, "method": 1, "follow": 1, "achievements": 1, "": 1, "discover": 1, "meaningful": 1, "reward": 1, "decomposition": 1}, {"": 1, "require": 1, "external": 1, "information": 1}, {"": 1, "achieve": 1, "better": 1, "performance": 1, "exist": 1, "rl": 1, "methods": 1}, {"2": 1, "": 2, "background": 1, "consider": 1, "general": 1, "reinforcement": 1, "learn": 1, "set": 1, "interaction": 1, "agent": 1, "environment": 1, "view": 1, "markov": 1, "decision": 1, "process": 1, "mdp": 1}, {"denote": 1, "state": 2, "space": 2, "x": 2, "": 5, "action": 1, "transition": 1, "function": 2, "p": 2, "actionstate": 1, "dependent": 1, "reward": 1, "r": 2, "discount": 1, "factor": 1, "write": 1, "mdp": 1}, {"give": 1, "fixedppolicy": 1, "": 11, "reinforcement": 1, "learn": 1, "estimate": 1, "actionvalue": 1, "function": 1, "define": 1, "q": 1, "x": 2, "t0": 1, "rt": 2, "xt": 2, "stateaction": 1, "pair": 1, "time": 1, "x0": 1, "a0": 1, "correspond": 1, "reward": 1}, {"bellman": 1, "equation": 1, "characterize": 1, "actionvalue": 1, "function": 1, "temporal": 1, "equivalence": 1, "give": 1, "q": 2, "x": 5, "": 12, "rx": 1, "0e": 1, "0": 4, "x0": 1, "a0": 1, "p": 1}, {"maximize": 1, "total": 1, "return": 1, "give": 2, "e": 1, "q": 6, "x0": 3, "": 24, "a0": 3, "one": 1, "common": 1, "approach": 1, "find": 1, "fix": 1, "point": 1, "bellman": 1, "optimality": 1, "operator": 1, "h": 1, "x": 3, "rx": 1, "e0": 1, "max": 2, "0": 3, "temporal": 1, "difference": 1, "td": 1, "error": 1, "2": 2, "rt": 1, "xt1": 2, "xt": 2, "sample": 1, "st": 1, "along": 1, "trajectory": 1}, {"mnih": 1, "et": 1, "al": 1}, {"2015": 1, "propose": 1, "deep": 1, "qnetworks": 1, "dqn": 1, "learn": 1, "actionvalue": 1, "function": 1, "neural": 1, "network": 1, "achieve": 1, "humanlevel": 1, "performance": 1, "atari57": 1, "benchmark": 1}, {"21": 1, "": 2, "reward": 2, "decomposition": 3, "study": 1, "also": 1, "lead": 1, "state": 1, "laversannefinot": 1, "et": 1, "al": 1}, {"2018": 1, "thomas": 1, "et": 1, "al": 1}, {"2017": 1, "state": 1, "decomposition": 1, "leverage": 1, "learn": 1, "different": 1, "policies": 1}, {"extend": 1, "work": 2, "grimm": 1, "singh": 1, "2019": 1, "explore": 1, "decomposition": 1, "reward": 1, "function": 1, "directly": 1, "consider": 1, "relate": 1}, {"denote": 1, "ith": 1, "i12n": 1, "": 24, "subreward": 2, "function": 4, "stateaction": 1, "pair": 1, "x": 4, "ri": 3, "complete": 1, "reward": 1, "give": 1, "r": 1, "n": 1, "i1": 1, "2": 1, "consider": 1, "subvalue": 1, "ui": 3, "correspond": 1, "policy": 1, "x0": 2, "a0": 2, "ext": 1, "xt": 3, "t0": 1, "arg": 1, "max": 1, "p": 1}, {"work": 1, "reward": 2, "decomposition": 1, "consider": 1, "meaningful": 1, "obtain": 1, "independently": 1, "ie": 1}, {"obtain": 1, "rj": 1, "": 1, "reward": 1, "obtainable": 1}, {"two": 2, "evaluate": 1, "desiderata": 1, "work": 1, "propose": 1, "follow": 1, "value": 1, "": 6, "x": 1, "jindependent": 1, "r1": 1}, {"": 1}, {"": 1}, {"": 10, "rn": 1, "es": 1, "ij": 1, "sui": 1, "j": 1, "1": 1, "i6j": 1, "jnontrivial": 1, "r1": 1}, {"": 1}, {"": 1}, {"": 14, "rn": 1, "es": 1, "n": 1, "x": 1, "ii": 1, "sui": 1, "2": 1, "i1": 1, "ij": 1, "weight": 1, "control": 1, "simplicity": 1, "set": 1, "1": 1, "work": 1}, {"train": 1, "network": 1, "would": 1, "maximize": 1, "jnontrivial": 1, "": 1, "jindependent": 1, "achieve": 1, "desire": 1, "reward": 1, "decomposition": 1}, {"22": 1, "": 2, "distributional": 1, "reinforcement": 2, "learn": 2, "settings": 1, "environment": 1, "deterministic": 1}, {"moreover": 1, "general": 1, "people": 1, "train": 1, "rl": 1, "model": 1, "greedy": 1, "policy": 1, "allow": 1, "exploration": 1, "make": 1, "agent": 1, "also": 1, "stochastic": 1}, {"better": 1, "analyze": 1, "randomness": 1, "set": 1, "bellemare": 1, "et": 1, "al": 1}, {"2017": 1, "propose": 1, "c51": 1, "algorithm": 1, "conduct": 1, "theoretical": 1, "analysis": 1, "distributional": 1, "rl": 1}, {"distributional": 1, "p": 1, "rl": 1, "reward": 1, "rt": 2, "view": 1, "random": 1, "variable": 1, "total": 1, "return": 1, "define": 1, "z": 1, "": 3, "t0": 1}, {"expectation": 1, "z": 2, "traditional": 1, "actionvalue": 1, "q": 1, "distributional": 1, "bellman": 1, "optimality": 1, "operator": 1, "give": 1, "": 10, "0": 3, "zx": 1, "rx": 1, "x": 2, "arg": 1, "max": 1, "ez": 1, "a0": 1, "random": 1, "variable": 1, "b": 3, "satisfy": 1, "follow": 1, "distribution": 1}, {"random": 1, "variable": 1, "characterize": 1, "categorical": 1, "distribution": 1, "fix": 1, "set": 1, "value": 1, "c51": 1, "outperform": 1, "previous": 1, "variants": 1, "dqn": 1, "atari": 1, "domain": 1}, {"3": 1, "31": 1, "": 2, "distributional": 2, "reward": 3, "decomposition": 2, "reinforcement": 2, "learn": 2, "many": 1, "environments": 1, "multiple": 1, "source": 1, "agent": 1, "receive": 1, "show": 1, "figure": 1, "1b": 1}, {"method": 1, "mainly": 1, "design": 1, "environments": 1, "property": 1}, {"distributional": 1, "set": 1, "assume": 1, "reward": 1, "subrewards": 1, "random": 1, "variables": 1, "denote": 1, "r": 1, "ri": 1, "respectively": 1}, {"architecture": 1, "categorical": 1, "distribution": 1, "pand": 1, "": 3, "subreturn": 1, "zi": 1, "t0": 1, "rit": 1, "output": 1, "network": 1, "denote": 1, "fi": 1, "x": 1}, {"note": 1, "case": 1, "subreturns": 1, "independent": 1, "ie": 1}, {"p": 1, "zi": 1, "": 1, "v": 1}, {"": 3, "p": 1, "zi": 1, "vzj": 1}, {"theoretically": 1, "need": 1, "fij": 1, "x": 1, "j": 1, "obtain": 1, "distribution": 1, "full": 1, "return": 1}, {"call": 1, "architecture": 1, "nonfactorial": 1, "model": 2, "fulldistribution": 1}, {"nonfactorial": 1, "model": 1, "architecture": 1, "show": 1, "appendix": 1}, {"however": 1, "experiment": 1, "show": 1, "use": 1, "approximation": 1, "form": 1, "p": 2, "zi": 2, "": 4, "v": 1, "vzj": 1, "fi": 1, "x": 2, "require": 1, "perform": 1, "much": 1, "better": 1, "brutally": 1, "compute": 1, "fij": 1, "j": 1, "believe": 1, "due": 1, "increase": 1, "sample": 1, "number": 1}, {"paper": 1, "approximate": 1, "conditional": 1, "probability": 1, "p": 2, "zi": 2, "": 3, "vzj": 1, "v": 1}, {"consider": 1, "categorical": 1, "distribution": 1, "function": 1, "fi": 1, "fj": 1, "number": 1, "atoms": 1, "k": 3, "kth": 1, "atom": 1, "denote": 1, "ak": 2, "value": 1, "": 4, "a0": 1, "kl": 1, "1": 1, "l": 1, "constant": 1}, {"let": 1, "random": 1, "variable": 1, "3": 1, "": 27, "convolution": 1, "1": 3, "2": 2, "softmax": 3, "n": 1, "b": 1, "figure": 1, "distributional": 1, "reward": 1, "decomposition": 1, "network": 1, "architecture": 1}, {"b": 1, "examples": 3, "multiple": 1, "reward": 3, "channel": 1, "atari": 1, "game": 1, "top": 1, "row": 2, "show": 2, "seaquest": 1, "submarine": 1, "receive": 2, "shoot": 2, "shark": 1, "rescue": 2, "divers": 1, "bottom": 1, "hero": 2, "bat": 1, "people": 1}, {"zi": 6, "": 27, "fi": 3, "zj": 5, "fj": 3, "basic": 1, "probability": 1, "theory": 1, "know": 1, "distribution": 1, "function": 1, "z": 1, "convolution": 1, "fv": 1, "p": 5, "v": 5, "k": 2, "x": 2, "ak": 5, "k1": 1, "3": 1}, {"k1": 1, "": 9, "use": 1, "n": 1, "subreturns": 1, "distribution": 1, "function": 1, "total": 1, "return": 1, "give": 1, "f": 1, "f1": 1, "f2": 1, "fn": 1, "denote": 1, "linear": 1, "1dconvolution": 1}, {"reward": 2, "decomposition": 1, "explicitly": 1, "do": 1, "algorithm": 1, "derive": 1, "decompose": 1, "pn": 1, "use": 1, "train": 1, "agents": 1}, {"recall": 1, "total": 1, "return": 1, "z": 2, "": 14, "i1": 4, "zi": 2, "follow": 1, "bellman": 1, "equation": 1, "naturally": 1, "n": 3, "x": 3, "tz": 1, "r": 1, "0": 1, "ri": 1, "zi0": 2, "4": 1, "represent": 1, "subreturn": 1, "next": 1, "stateaction": 1, "pair": 1}, {"note": 1, "access": 1, "sample": 1, "full": 1, "reward": 1, "r": 1, "subrewards": 2, "ri": 2, "arbitrary": 1, "better": 1, "visualization": 1, "direct": 1, "way": 1, "derive": 1, "give": 1, "": 3, "zi": 1, "zi0": 1, "5": 1, "next": 1, "section": 1, "present": 1, "example": 1, "take": 1, "expectation": 1, "eri": 1}, {"note": 1, "reward": 1, "decomposition": 1, "latent": 1, "need": 1, "ri": 1, "algorithm": 1, "eq": 1}, {"5": 1, "provide": 1, "approach": 1, "visualize": 1, "reward": 1, "decomposition": 1}, {"32": 1, "": 2, "disentangle": 2, "subreturns": 1, "obtain": 1, "meaningful": 1, "reward": 1, "decomposition": 1, "want": 1, "subrewards": 1}, {"inspire": 1, "grimm": 1, "singh": 1, "2019": 1, "compute": 1, "disentanglement": 1, "distributions": 1, "two": 1, "subreturns": 1, "f": 2, "j": 1, "state": 1, "x": 1, "follow": 1, "value": 1, "ij": 1, "jdisentang": 1, "": 5, "dkl": 2, "fxarg": 2, "maxa": 2, "ezi": 1, "ezj": 1, "denote": 1, "crossentropy": 1, "term": 1, "kl": 1, "divergence": 1}, {"4": 1, "": 4, "6": 1, "ij": 1, "intuitively": 1, "jdisentang": 1, "estimate": 2, "disentanglement": 1, "subreturns": 1, "zi": 1, "zj": 1, "first": 1, "obtain": 1, "action": 2, "maximize": 1, "ezi": 1, "ezj": 1, "respectively": 1, "compute": 1, "kl": 1, "divergence": 1, "two": 1, "total": 1, "return": 1}, {"zi": 1, "zj": 1, "independent": 1, "action": 1, "maximize": 1, "two": 1, "subreturns": 1, "would": 2, "different": 1, "difference": 1, "reflect": 1, "estimation": 1, "total": 1, "return": 1}, {"maximize": 1, "value": 1, "expect": 1, "meaningful": 1, "reward": 2, "decomposition": 1, "learn": 1, "independent": 1}, {"33": 1, "": 2, "project": 1, "bellman": 1, "update": 1, "regularization": 1, "follow": 1, "work": 1, "c51": 1, "bellemare": 1, "et": 1, "al": 1}, {"2017": 1, "use": 1, "project": 1, "bellman": 1, "update": 1, "algorithm": 1}, {"apply": 1, "bellman": 1, "optimality": 1, "operator": 1, "atoms": 1, "z": 1, "shift": 1, "rt": 1, "shrink": 1, "": 1}, {"however": 1, "compute": 1, "loss": 1, "usually": 1, "kl": 1, "divergence": 1, "z": 3, "require": 1, "two": 1, "categorical": 1, "distributions": 1, "define": 1, "set": 2, "atoms": 2, "target": 1, "distribution": 1, "would": 1, "need": 1, "project": 1, "original": 1, "bellman": 1, "update": 1}, {"consider": 1, "sample": 1, "transition": 1, "x": 2, "r": 2, "x0": 1, "": 12, "projection": 1, "operator": 1, "propose": 1, "c51": 2, "give": 1, "1": 3, "v": 1, "aj": 2, "vmax": 1, "min": 1, "fx0": 1, "a0": 1, "zx": 1, "ai": 1, "7": 1, "l": 1, "j0": 1, "0": 1, "ba": 1, "number": 1, "atoms": 1, "bound": 1, "argument": 1, "b": 1}, {"sample": 1, "loss": 1, "x": 1, "r": 1, "x0": 1, "": 2, "would": 1, "give": 1, "crossentropy": 1, "term": 1, "kl": 1, "divergence": 1, "z": 2, "lxarx0": 1, "dkl": 1, "zx": 1, "azx": 1}, {"8": 1, "": 4, "let": 1, "f": 1, "neural": 1, "network": 1, "parameterized": 1, "combine": 1, "distributional": 1, "td": 1, "error": 1, "disentanglement": 1, "jointly": 1, "update": 1}, {"sample": 1, "transition": 1, "x": 3, "r": 1, "x0": 1, "": 8, "update": 1, "minimize": 1, "follow": 1, "objective": 1, "function": 1, "ij": 1, "lxarx0": 1, "jdisentang": 1, "9": 1, "ji": 1, "denote": 1, "learn": 1, "rate": 1}, {"34": 1, "": 3, "multichannel": 1, "state": 2, "representation": 1, "one": 1, "complication": 1, "approach": 1, "outline": 1, "often": 1, "distribution": 1, "fi": 1, "cannot": 1, "distinguish": 1, "distributions": 1, "eg": 1, "fj": 1, "j": 1, "6": 1, "learn": 1, "since": 1, "depend": 1, "feature": 1, "input": 1}, {"bring": 1, "difficulties": 1, "maximize": 1, "disentanglement": 1, "jointly": 1, "train": 1, "different": 1, "distribution": 1, "function": 1, "exchangeable": 1}, {"naive": 1, "idea": 1, "split": 1, "state": 1, "feature": 1, "x": 1, "n": 1, "piece": 1, "eg": 1, "x1": 1, "": 4, "x2": 1, "xn": 1, "distribution": 1, "depend": 1, "different": 1, "substatefeatures": 1}, {"however": 1, "empirically": 1, "find": 1, "method": 1, "enough": 1, "help": 1, "learn": 1, "good": 1, "disentangle": 1, "subreturns": 1}, {"address": 1, "problem": 1, "utilize": 1, "idea": 1, "similar": 1, "universal": 1, "value": 1, "function": 1, "approximation": 1, "uvfa": 1, "schaul": 1, "et": 1, "al": 1}, {"2015": 1}, {"key": 1, "idea": 1, "take": 1, "onehot": 3, "embed": 3, "additional": 1, "input": 1, "condition": 1, "categorical": 1, "distribution": 1, "function": 1, "apply": 1, "elementwise": 1, "multiplication": 1, "": 8, "force": 1, "interaction": 1, "state": 1, "feature": 2, "fi": 2, "x": 2, "ei": 2, "10": 1, "denote": 2, "ith": 1, "element": 1, "one": 1, "onelayer": 1, "nonlinear": 1, "neural": 1, "network": 1, "update": 1, "backpropagation": 1, "train": 1}, {"way": 1, "agent": 1, "explicitly": 1, "learn": 1, "different": 2, "distribution": 1, "function": 1, "channel": 1}, {"complete": 1, "network": 1, "architecture": 1, "show": 1, "figure": 1, "1a": 1}, {"4": 1, "": 2, "experiment": 1, "result": 1, "test": 1, "algorithm": 1, "game": 1, "arcade": 1, "learn": 1, "environment": 1, "ale": 1, "bellemare": 1, "et": 1, "al": 1}, {"2013": 1}, {"conduct": 1, "experiment": 1, "six": 1, "atari": 1, "game": 1, "complicate": 1, "rule": 1, "5": 1, "": 86, "20000": 5, "return": 6, "15000": 2, "10000": 6, "5000": 2, "70000": 1, "60000": 2, "50000": 2, "40000": 3, "30000": 4, "0": 12, "rainbow": 19, "rd3": 6, "rd2": 6, "20": 6, "40": 6, "60": 6, "80": 6, "epoch": 6, "100": 6, "stargunner": 1, "hero": 1, "asterix": 1, "16000": 1, "14000": 1, "12000": 1, "8000": 1, "6000": 1, "4000": 1, "2000": 1, "gopher": 1, "upndown": 1, "25000": 1, "17500": 1, "12500": 1, "7500": 1, "2500": 1, "seaquest": 1, "figure": 1, "2": 1, "performance": 1, "comparison": 1}, {"rdn": 1, "represent": 1, "use": 1, "nchannel": 1, "reward": 1, "decomposition": 1}, {"train": 1, "curve": 1, "average": 1, "three": 1, "random": 1, "seed": 1}, {"simple": 1, "rule": 1}, {"study": 1, "implement": 1, "algorithm": 1, "base": 1, "rainbow": 1, "hessel": 1, "et": 1, "al": 1}, {"2018": 1, "advance": 1, "variant": 1, "c51": 1, "bellemare": 1, "et": 1, "al": 1}, {"2017": 1, "achieve": 1, "stateoftheart": 1, "result": 1, "atari": 1, "game": 1, "domain": 1}, {"replace": 1, "update": 1, "rule": 1, "rainbow": 1, "eq": 1}, {"9": 1, "network": 1, "architecture": 2, "rainbow": 1, "convolute": 1, "show": 1, "figure": 1, "1a": 1}, {"rainbow": 1, "qvalue": 1, "bound": 1, "vmin": 2, "": 4, "vmax": 2, "10": 1}, {"method": 1, "bind": 1, "categorical": 1, "vmax": 1, "distribution": 1, "subreturn": 1, "zi": 1, "": 6, "1": 1, "2": 1, "n": 3, "range": 1, "vmin": 1}, {"rainbow": 1, "use": 1, "categorical": 1, "distribution": 1, "": 1, "51": 1, "atoms": 1}, {"fair": 1, "comparison": 1, "assign": 1, "k": 1, "": 1, "b": 1, "n": 1, "c": 1, "atoms": 1, "distribution": 1, "subreturn": 1, "result": 1, "network": 2, "capacity": 1, "original": 1, "architecture": 1}, {"code": 1, "build": 1, "upon": 1, "dopamine": 1, "framework": 1, "castro": 1, "et": 1, "al": 1}, {"2018": 1}, {"use": 1, "default": 1, "welltuned": 1, "hyperparameter": 1, "set": 1, "dopamine": 1}, {"update": 1, "rule": 1, "eq": 1}, {"9": 1, "set": 1, "": 2, "00001": 1}, {"run": 1, "agents": 1, "100": 1, "epochs": 1, "025": 1, "million": 2, "train": 1, "step": 2, "0125": 1, "evaluation": 1}, {"evaluation": 1, "follow": 1, "common": 1, "practice": 1, "van": 1, "hasselt": 1, "et": 1, "al": 1}, {"2016": 1, "start": 2, "game": 1, "30": 1, "noop": 1, "action": 1, "provide": 1, "random": 1, "position": 1, "agent": 1}, {"experiment": 1, "perform": 1, "nvidia": 1, "tesla": 1, "v100": 1, "16gb": 1, "graphics": 1, "card": 1}, {"41": 1, "": 2, "comparison": 1, "rainbow": 2, "verify": 1, "architecture": 1, "achieve": 1, "reward": 1, "decomposition": 1, "without": 1, "degrade": 1, "performance": 1, "compare": 1, "methods": 1}, {"however": 1, "able": 1, "compare": 1, "method": 1, "van": 1, "seijen": 1, "et": 1, "al": 1}, {"2017": 1, "grimm": 1, "singh": 1, "2019": 1, "since": 1, "require": 1, "either": 1, "predefined": 1, "state": 1, "preprocessing": 1, "specificstate": 1, "resettable": 1, "environments": 1}, {"test": 1, "reward": 1, "decomposition": 1, "rd": 1, "2": 1, "3": 1, "channel": 1, "eg": 1, "rd2": 1, "rd3": 1}, {"result": 1, "show": 1, "figure": 1, "2": 1}, {"find": 1, "methods": 1, "perform": 1, "significantly": 1, "better": 1, "rainbow": 1, "environments": 1, "test": 1}, {"imply": 1, "distributional": 1, "reward": 1, "decomposition": 1, "method": 1, "help": 1, "accelerate": 1, "learn": 1, "process": 1}, {"also": 1, "discover": 1, "environments": 1, "rd3": 1, "perform": 1, "better": 1, "rd2": 1, "rest": 1, "two": 1, "similar": 1, "performance": 1}, {"conjecture": 1, "due": 1, "intrinsic": 1, "settings": 1, "environments": 1}, {"example": 1, "seaquest": 1, "upndown": 1, "rule": 1, "relatively": 1, "complicate": 1, "rd3": 1, "characterize": 1, "complex": 1, "reward": 1, "better": 1}, {"however": 1, "simple": 1, "environments": 1, "like": 1, "gopher": 1, "asterix": 1, "rd2": 2, "rd3": 2, "obtain": 1, "similar": 1, "performance": 1, "sometimes": 1, "even": 1, "outperform": 1}, {"42": 1, "": 2, "reward": 2, "decomposition": 2, "analysis": 1, "use": 1, "seaquest": 1, "illustrate": 1}, {"figure": 1, "3": 1, "show": 1, "subrewards": 1, "obtain": 1, "take": 1, "expectation": 1, "lhs": 1, "eq5": 1, "original": 1, "reward": 1, "along": 1, "actual": 1, "trajectory": 1}, {"observe": 1, "r1": 2, "": 4, "er1": 1, "r2": 1, "er2": 1, "basically": 1, "add": 1, "original": 1, "reward": 1, "r": 1, "dominate": 1, "submarine": 1, "close": 1, "surface": 1, "ie": 1}, {"rescue": 1, "divers": 1, "6": 3, "": 28, "1": 2, "3": 3, "2": 2, "r1": 6, "071": 1, "r2": 6, "010": 1, "058": 1, "040": 1, "4": 2, "060": 1, "039": 1, "007": 1, "097": 1, "5": 2, "006": 1, "098": 1, "026": 1, "072": 1, "figure": 1, "reward": 1, "decomposition": 1, "along": 1, "trajectory": 1}, {"subrewards": 2, "r1": 1, "r2": 1, "usually": 1, "add": 1, "original": 2, "reward": 2, "r": 1, "see": 1, "proportion": 1, "greatly": 1, "depend": 1, "obtain": 1}, {"refill": 1, "oxygen": 1}, {"submarine": 1, "score": 1, "shoot": 1, "shark": 1, "r2": 1, "become": 1, "main": 1, "source": 1, "reward": 1}, {"also": 1, "monitor": 1, "distribution": 1, "different": 1, "subreturns": 1, "agent": 1, "play": 1, "game": 1}, {"figure": 1, "4": 1, "submarine": 1, "float": 1, "surface": 1, "rescue": 1, "divers": 1, "refill": 1, "oxygen": 1, "z1": 1, "higher": 1, "value": 1}, {"figure": 1, "4": 1, "b": 1, "submarine": 1, "dive": 1, "sea": 1, "shoot": 1, "shark": 1, "expect": 1, "value": 1, "z2": 1, "orange": 1, "higher": 1, "z1": 1, "blue": 1}, {"result": 1, "imply": 1, "reward": 1, "decomposition": 1, "indeed": 1, "capture": 1, "different": 1, "source": 1, "return": 1, "case": 1, "shoot": 1, "shark": 1, "rescue": 1, "diversrefilling": 1, "oxygen": 1}, {"also": 1, "provide": 1, "statistics": 1, "action": 1, "quantitative": 1, "analysis": 1, "support": 1, "argument": 1}, {"figure": 2, "6a": 1, "count": 1, "occurrence": 1, "action": 1, "obtain": 1, "arg": 2, "maxa": 2, "ez1": 1, "": 2, "ez2": 1, "single": 1, "trajectory": 1, "use": 1, "policy": 1, "4": 1}, {"see": 1, "z1": 1, "prefer": 2, "go": 2, "z2": 1, "fire": 1}, {"43": 1, "": 2, "visualization": 1, "saliency": 2, "map": 2, "better": 1, "understand": 1, "roles": 1, "different": 1, "subrewards": 1, "train": 1, "drdrl": 1, "agent": 1, "two": 1, "channel": 1, "n2": 1, "compute": 1, "simonyan": 1, "et": 1, "al": 1}, {"2013": 1}, {"specifically": 1, "visualize": 1, "salient": 1, "part": 1, "image": 1, "see": 1, "different": 1, "subpolicies": 1, "compute": 1, "absolute": 1, "value": 1, "jacobian": 1, "x": 2, "qi": 1, "arg": 1, "maxa0": 1, "qx": 1, "a0": 1, "": 1, "channel": 1}, {"figure": 1, "5": 1, "show": 1, "visualization": 1, "result": 1}, {"find": 1, "channel": 2, "1": 1, "red": 1, "region": 2, "focus": 1, "refill": 1, "oxygen": 1, "2": 1, "green": 1, "pay": 1, "attention": 1, "shoot": 1, "shark": 2, "well": 1, "position": 1, "likely": 1, "appear": 1}, {"44": 1, "": 4, "direct": 1, "control": 1, "use": 1, "induce": 1, "subpolicies": 2, "also": 1, "provide": 1, "videos2": 1, "run": 1, "define": 1, "arg": 1, "maxa": 1, "ezi": 1}, {"clarify": 1, "subpolicies": 1, "never": 1, "roll": 1, "train": 1, "evaluation": 1, "use": 1, "compute": 1, "ij": 1, "jdisentang": 1, "eq": 1}, {"6": 1}, {"execute": 1, "subpolicies": 1, "observe": 1, "difference": 1, "main": 1, "policy": 1, "pm": 1, "": 3, "arg": 1, "maxa": 1, "e": 1, "i1": 1, "zi": 1, "get": 1, "better": 1, "visual": 1, "effect": 1, "reward": 1, "decomposition": 1}, {"take": 1, "seaquest": 1, "figure": 1, "6b": 1, "example": 1, "two": 1, "subpolicies": 1, "show": 1, "distinctive": 1, "preference": 1}, {"z1": 1, "mainly": 1, "capture": 1, "reward": 1, "survive": 1, "rescue": 1, "divers": 1, "1": 1, "tend": 1, "stay": 1, "close": 1, "surface": 1}, {"however": 1, "z2": 1, "represent": 1, "return": 1, "gain": 1, "shoot": 1, "shark": 1, "2": 1, "appear": 1, "much": 1, "aggressive": 1, "1": 1, "": 1}, {"also": 1, "without": 1, "1": 1, "see": 1, "2": 1, "die": 1, "quickly": 1, "due": 1, "oxygen": 1}, {"2": 3, "": 7, "httpssitesgooglecomviewdrdpaper": 1, "7": 1, "1": 2, "b": 1, "figure": 1, "4": 1, "illustration": 1, "subreturns": 1, "discriminate": 1, "different": 1, "stage": 1, "game": 1}, {"figure": 2, "submarine": 2, "refill": 1, "oxygen": 1, "b": 1, "shoot": 1, "shark": 1}, {"figure": 1, "5": 1, "subdistribution": 1, "saliency": 1, "map": 1, "atari": 1, "game": 1, "seaquest": 1, "train": 1, "drdrl": 1, "two": 1, "channel": 1, "n2": 1}, {"one": 1, "channel": 2, "learn": 2, "pay": 2, "attention": 2, "oxygen": 1, "another": 1, "shark": 1}, {"5": 1, "": 2, "relate": 2, "work": 2, "method": 1, "closely": 1, "previous": 1, "reward": 1, "decomposition": 1}, {"reward": 1, "function": 1, "decomposition": 1, "study": 1, "among": 1, "others": 1, "russell": 1, "zimdars": 1, "2003": 2, "sprague": 1, "ballard": 1}, {"earlier": 1, "work": 2, "mainly": 1, "focus": 1, "achieve": 1, "optimal": 1, "policy": 1, "give": 1, "decompose": 2, "reward": 2, "function": 1, "several": 1, "recent": 1, "attempt": 1, "learn": 1, "latent": 1}, {"van": 1, "seijen": 1, "et": 1, "al": 1}, {"2017": 1, "construct": 1, "easytolearn": 1, "value": 1, "function": 3, "decompose": 1, "reward": 2, "environment": 1, "n": 1, "different": 1}, {"ensure": 1, "learn": 1, "decomposition": 1, "nontrivial": 1, "van": 1, "seijen": 1, "et": 1, "al": 1}, {"2017": 1, "propose": 1, "split": 1, "state": 2, "different": 2, "piece": 2, "follow": 1, "domain": 1, "knowledge": 1, "fee": 1, "reward": 1, "function": 1, "branch": 1}, {"method": 1, "accelerate": 1, "learn": 1, "process": 1, "always": 1, "require": 1, "many": 1, "predefined": 1, "preprocessing": 1, "techniques": 1}, {"8": 1, "": 3, "b": 1, "figure": 1, "6": 1, "action": 1, "statistics": 1, "example": 1, "trajectory": 1, "seaquest": 1}, {"b": 1, "direct": 1, "control": 1, "use": 1, "two": 1, "induce": 1, "subpolicies": 1, "1": 2, "": 4, "arg": 2, "maxa": 2, "ez1": 1, "2": 2, "ez2": 1, "top": 2, "picture": 2, "show": 2, "prefer": 2, "stay": 1, "keep": 1, "agent": 1, "alive": 1, "bottom": 1, "aggressive": 1, "action": 1, "shoot": 1, "shark": 1}, {"work": 1, "explore": 1, "learn": 1, "reward": 1, "decomposition": 1, "network": 1, "endtoend": 1}, {"grimm": 1, "singh": 1, "2019": 1, "investigate": 1, "learn": 1, "independentlyobtainable": 1, "reward": 1, "function": 1}, {"learn": 1, "interest": 1, "reward": 1, "decomposition": 1, "method": 1, "require": 1, "environments": 1, "resettable": 1, "specific": 1, "state": 2, "since": 1, "need": 1, "multiple": 1, "trajectories": 1, "start": 1, "compute": 1, "objective": 1, "function": 1}, {"besides": 1, "method": 1, "aim": 1, "learn": 1, "different": 1, "optimal": 1, "policies": 1, "decompose": 1, "reward": 1, "function": 1}, {"different": 1, "work": 1, "method": 1, "learn": 1, "meaningful": 1, "implicit": 1, "reward": 1, "decomposition": 1, "without": 1, "requirements": 1, "prior": 1, "knowledge": 1}, {"also": 1, "method": 1, "leverage": 1, "decompose": 1, "subrewards": 1, "find": 1, "better": 1, "behaviour": 1, "single": 1, "agent": 1}, {"work": 1, "also": 1, "relate": 1, "horde": 1, "sutton": 1, "et": 1, "al": 1}, {"2011": 1}, {"horde": 1, "architecture": 1, "consist": 1, "large": 1, "number": 1, "subagents": 1, "learn": 2, "parallel": 1, "via": 1, "offpolicy": 1}, {"demon": 1, "train": 1, "separate": 1, "general": 1, "value": 1, "function": 2, "gvf": 1, "base": 1, "policy": 1, "pseudoreward": 1}, {"pseudoreward": 1, "featurebased": 1, "signal": 1, "encode": 1, "useful": 1, "information": 1}, {"horde": 1, "architecture": 1, "focus": 1, "build": 1, "general": 1, "knowledge": 1, "world": 1, "encode": 1, "via": 1, "large": 1, "number": 1, "gvfs": 1}, {"uvfa": 1, "schaul": 1, "et": 1, "al": 1}, {"2015": 1, "extend": 1, "horde": 1, "along": 1, "different": 2, "direction": 1, "enable": 1, "value": 1, "function": 1, "generalize": 1, "across": 1, "goals": 1}, {"method": 1, "focus": 1, "learn": 2, "implicit": 1, "reward": 1, "decomposition": 1, "order": 1, "efficiently": 1, "control": 1, "policy": 1}, {"6": 1, "": 2, "conclusion": 1, "paper": 1, "propose": 1, "distributional": 2, "reward": 3, "decomposition": 2, "reinforcement": 1, "learn": 1, "drdrl": 1, "novel": 1, "algorithm": 1, "capture": 1, "multiple": 1, "channel": 1, "structure": 1, "set": 1}, {"algorithm": 1, "significantly": 1, "outperform": 1, "stateoftheart": 1, "rl": 1, "methods": 1, "rainbow": 1, "atari": 1, "game": 1, "multiple": 1, "reward": 1, "channel": 1}, {"also": 1, "provide": 1, "interest": 1, "experimental": 1, "analysis": 1, "get": 1, "insight": 1, "algorithm": 1}, {"future": 1, "might": 1, "try": 1, "develop": 1, "reward": 1, "decomposition": 1, "method": 1, "base": 1, "quantile": 1, "network": 1, "dabney": 1, "et": 1, "al": 1}, {"2018ab": 1}, {"acknowledgments": 1, "work": 1, "support": 1, "part": 1, "national": 1, "key": 1, "research": 1, "": 1, "development": 1, "plan": 1, "china": 1, "grant": 1}, {"2016yfa0602200": 1, "2017yfa0604500": 1, "center": 1, "high": 1, "performance": 1, "compute": 1, "system": 1, "simulation": 1, "pilot": 1, "national": 1, "laboratory": 1, "marine": 1, "science": 1, "technology": 1, "qingdao": 1}, {"9": 1, "": 1, "reference": 1, "marc": 1, "g": 1, "bellemare": 1, "yavar": 1, "naddaf": 1, "joel": 1, "veness": 1, "michael": 1, "bowl": 1}, {"arcade": 1, "learn": 1, "environment": 1, "evaluation": 1, "platform": 1, "general": 1, "agents": 1}, {"journal": 1, "artificial": 1, "intelligence": 1, "research": 1, "47": 1, "253279": 1, "2013": 1}, {"marc": 1, "g": 1, "bellemare": 1, "dabney": 1, "rmi": 1, "munos": 1}, {"distributional": 1, "perspective": 1, "reinforcement": 1, "learn": 1}, {"proceed": 1, "34th": 1, "international": 1, "conference": 1, "machine": 1, "learningvolume": 1, "70": 1, "page": 1, "449458": 1}, {"jmlr": 1}, {"org": 1, "2017": 1}, {"pablo": 1, "samuel": 1, "castro": 1, "subhodeep": 1, "moitra": 1, "carles": 1, "gelada": 1, "saurabh": 1, "kumar": 1, "marc": 1, "g": 1, "bellemare": 1}, {"dopamine": 1, "research": 1, "framework": 1, "deep": 1, "reinforcement": 1, "learn": 1}, {"2018": 1}, {"url": 1, "http": 1, "arxivorgabs181206110": 1}, {"dabney": 1, "georg": 1, "ostrovski": 1, "david": 1, "silver": 1, "remi": 1, "munos": 1}, {"implicit": 1, "quantile": 1, "network": 1, "distributional": 1, "reinforcement": 1, "learn": 1}, {"international": 1, "conference": 1, "machine": 1, "learn": 1, "page": 1, "11041113": 1, "2018a": 1}, {"dabney": 1, "mark": 1, "rowland": 1, "marc": 1, "g": 1, "bellemare": 1, "rmi": 1, "munos": 1}, {"distributional": 1, "reinforcement": 1, "learn": 1, "quantile": 1, "regression": 1}, {"thirtysecond": 1, "aaai": 1, "conference": 1, "artificial": 1, "intelligence": 1, "2018b": 1}, {"christopher": 1, "grimm": 1, "satinder": 1, "singh": 1}, {"learn": 1, "independentlyobtainable": 1, "reward": 1, "function": 1}, {"arxiv": 1, "preprint": 1, "arxiv190108649": 1, "2019": 1}, {"matteo": 1, "hessel": 1, "joseph": 1, "modayil": 1, "hado": 1, "van": 1, "hasselt": 1, "tom": 1, "schaul": 1, "georg": 1, "ostrovski": 1, "dabney": 1, "dan": 1, "horgan": 1, "bilal": 1, "piot": 1, "mohammad": 1, "azar": 1, "david": 1, "silver": 1}, {"rainbow": 1, "combine": 1, "improvements": 1, "deep": 1, "reinforcement": 1, "learn": 1}, {"thirtysecond": 1, "aaai": 1, "conference": 1, "artificial": 1, "intelligence": 1, "2018": 1}, {"adrien": 1, "laversannefinot": 1, "alexandre": 1, "pr": 1, "pierreyves": 1, "oudeyer": 1}, {"curiosity": 1, "drive": 1, "exploration": 1, "learn": 1, "disentangle": 1, "goal": 1, "space": 1}, {"arxiv": 1, "preprint": 1, "arxiv180701521": 1, "2018": 1}, {"volodymyr": 1, "mnih": 1, "koray": 1, "kavukcuoglu": 1, "david": 1, "silver": 1, "andrei": 1, "rusu": 1, "joel": 1, "veness": 1, "marc": 1, "g": 1, "bellemare": 1, "alex": 1, "grave": 1, "martin": 1, "riedmiller": 1, "andreas": 1, "k": 1, "fidjeland": 1, "georg": 1, "ostrovski": 1, "et": 1, "al": 1}, {"humanlevel": 1, "control": 1, "deep": 1, "reinforcement": 1, "learn": 1}, {"nature": 1, "5187540529": 1, "2015": 1}, {"stuart": 1, "j": 1, "russell": 1, "andrew": 1, "zimdars": 1}, {"qdecomposition": 1, "reinforcement": 1, "learn": 1, "agents": 1}, {"proceed": 1, "20th": 1, "international": 1, "conference": 1, "machine": 1, "learn": 1, "icml03": 1, "page": 1, "656": 1, "663": 1, "2003": 1}, {"tom": 1, "schaul": 1, "daniel": 1, "horgan": 1, "karol": 1, "gregor": 1, "david": 1, "silver": 1}, {"universal": 1, "value": 1, "function": 1, "approximators": 1}, {"international": 1, "conference": 1, "machine": 1, "learn": 1, "page": 1, "13121320": 1, "2015": 1}, {"karen": 1, "simonyan": 1, "andrea": 1, "vedaldi": 1, "andrew": 1, "zisserman": 1}, {"deep": 1, "inside": 1, "convolutional": 1, "network": 1, "visualise": 1, "image": 1, "classification": 1, "model": 1, "saliency": 1, "map": 1}, {"arxiv": 1, "preprint": 1, "arxiv13126034": 1, "2013": 1}, {"nathan": 1, "sprague": 1, "dana": 1, "ballard": 1}, {"multiplegoal": 1, "reinforcement": 1, "learn": 1, "modular": 1, "sarsa": 1, "0": 1}, {"2003": 1}, {"richard": 1, "sutton": 1, "joseph": 1, "modayil": 1, "michael": 1, "delp": 1, "thomas": 1, "degris": 1, "patrick": 1, "pilarski": 1, "adam": 1, "white": 1, "doina": 1, "precup": 1}, {"horde": 1, "scalable": 1, "realtime": 1, "architecture": 1, "learn": 1, "knowledge": 1, "unsupervised": 1, "sensorimotor": 1, "interaction": 1}, {"10th": 1, "international": 1, "conference": 1, "autonomous": 1, "agents": 1, "multiagent": 1, "systemsvolume": 1, "2": 1, "page": 1, "761768": 1}, {"international": 1, "foundation": 1, "autonomous": 1, "agents": 1, "multiagent": 1, "systems": 1, "2011": 1}, {"valentin": 1, "thomas": 1, "jules": 1, "pondard": 1, "emmanuel": 1, "bengio": 2, "marc": 1, "sarfati": 1, "philippe": 1, "beaudoin": 1, "mariejean": 1, "meurs": 1, "joelle": 1, "pineau": 1, "doina": 1, "precup": 1, "yoshua": 1}, {"independently": 1, "controllable": 1, "feature": 1}, {"arxiv": 1, "preprint": 1, "arxiv170801289": 1, "2017": 1}, {"hado": 1, "van": 1, "hasselt": 1, "arthur": 1, "guez": 1, "david": 1, "silver": 1}, {"deep": 1, "reinforcement": 1, "learn": 1, "double": 1, "qlearning": 1}, {"thirtieth": 1, "aaai": 1, "conference": 1, "artificial": 1, "intelligence": 1, "2016": 1}, {"harm": 1, "van": 1, "seijen": 1, "mehdi": 1, "fatemi": 1, "joshua": 1, "romoff": 1, "romain": 1, "laroche": 1, "tavian": 1, "barnes": 1, "jeffrey": 1, "tsang": 1}, {"hybrid": 1, "reward": 1, "architecture": 1, "reinforcement": 1, "learn": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "page": 1, "53925402": 1, "2017": 1}, {"10": 1}]
[{"modelbased": 1, "reinforcement": 2, "learn": 2, "adversarial": 1, "train": 1, "online": 1, "recommendation": 1, "": 8, "xueying": 1, "bai": 1, "jian": 1, "guan": 1, "hongning": 1, "wang": 1, "department": 3, "computer": 3, "science": 3, "stony": 1, "brook": 1, "university": 3, "technology": 1, "tsinghua": 1, "virginia": 1, "xubaicsstonybrookedu": 1, "jguan19mailstsinghuaeducn": 1, "hw5xvirginiaedu": 1, "abstract": 1, "well": 1, "suit": 1, "optimize": 1, "policies": 1, "recommender": 1, "systems": 1}, {"current": 1, "solutions": 1, "mostly": 1, "focus": 1, "modelfree": 1, "approach": 1, "require": 1, "frequent": 1, "interactions": 1, "real": 1, "environment": 1, "thus": 1, "expensive": 1, "model": 1, "learn": 1}, {"offline": 1, "evaluation": 1, "methods": 1, "importance": 1, "sample": 1, "alleviate": 1, "limitations": 1, "usually": 1, "request": 1, "large": 2, "amount": 1, "log": 1, "data": 1, "work": 1, "well": 1, "action": 1, "space": 1}, {"work": 1, "propose": 1, "modelbased": 1, "reinforcement": 1, "learn": 2, "solution": 1, "model": 1, "useragent": 1, "interaction": 1, "offline": 1, "policy": 1, "via": 1, "generative": 1, "adversarial": 1, "network": 1}, {"reduce": 1, "bias": 1, "learn": 1, "model": 1, "policy": 1, "use": 1, "discriminator": 1, "evaluate": 1, "quality": 1, "generate": 2, "data": 1, "scale": 1, "reward": 1}, {"theoretical": 1, "analysis": 1, "empirical": 1, "evaluations": 1, "demonstrate": 1, "effectiveness": 1, "solution": 1, "learn": 1, "policies": 1, "offline": 1, "generate": 1, "data": 1}, {"1": 1, "": 2, "introduction": 1, "recommender": 1, "systems": 1, "successful": 1, "connect": 1, "users": 1, "interest": 1, "content": 1, "variety": 1, "application": 1, "domains": 1}, {"however": 1, "users": 1, "diverse": 1, "interest": 1, "behavior": 1, "pattern": 1, "small": 1, "fraction": 1, "items": 1, "present": 1, "user": 1, "even": 1, "less": 1, "feedback": 1, "record": 1}, {"give": 1, "relatively": 1, "little": 1, "information": 1, "usersystem": 1, "interactions": 2, "large": 1, "state": 1, "action": 1, "space": 1, "2": 1, "thus": 1, "bring": 1, "considerable": 1, "challenge": 1, "construct": 1, "useful": 1, "recommendation": 1, "policy": 1, "base": 1, "historical": 1}, {"important": 1, "develop": 1, "solutions": 1, "learn": 1, "users": 1, "preferences": 1, "sparse": 1, "user": 1, "feedback": 1, "click": 1, "purchase": 1, "11": 1, "13": 1, "improve": 1, "utility": 1, "recommender": 1, "systems": 1}, {"users": 1, "interest": 1, "shortterm": 1, "longterm": 1, "reflect": 1, "different": 1, "type": 1, "feedback": 1, "35": 1}, {"example": 1, "click": 2, "generally": 1, "consider": 1, "shortterm": 1, "feedback": 1, "reflect": 1, "users": 2, "immediate": 1, "interest": 2, "interaction": 1, "purchase": 1, "reveal": 1, "longterm": 1, "usually": 1, "happen": 1, "several": 1}, {"consider": 1, "users": 2, "shortterm": 2, "longterm": 2, "interest": 1, "frame": 1, "recommender": 1, "system": 1, "reinforcement": 1, "learn": 1, "rl": 1, "agent": 1, "aim": 1, "maximize": 1, "overall": 1, "satisfaction": 1, "without": 1, "sacrifice": 1, "recommendations": 1, "utility": 1, "28": 1}, {"classical": 1, "modelfree": 1, "rl": 1, "methods": 1, "require": 1, "collect": 1, "large": 1, "quantities": 1, "data": 1, "interact": 1, "environment": 1, "eg": 1, "population": 1, "users": 1}, {"therefore": 1, "without": 1, "interact": 1, "real": 1, "users": 1, "recommender": 1, "cannot": 1, "easily": 1, "probe": 1, "reward": 1, "previously": 1, "unexplored": 1, "regions": 1, "state": 1, "action": 1, "space": 1}, {"however": 1, "prohibitively": 1, "expensive": 1, "recommender": 1, "interact": 1, "users": 1, "reward": 1, "model": 1, "update": 1, "bad": 1, "recommendations": 1, "eg": 1, "exploration": 1, "hurt": 1, "user": 2, "satisfaction": 1, "increase": 1, "risk": 1, "drop": 1}, {"case": 1, "prefer": 1, "recommender": 1, "learn": 1, "policy": 1, "fully": 1, "utilize": 1, "log": 1, "data": 1, "acquire": 1, "policies": 1, "eg": 1, "previously": 1, "deploy": 1, "systems": 1, "": 2, "author": 1, "contribute": 1, "equally": 1}, {"33rd": 1, "conference": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "neurips": 1, "2019": 1, "vancouver": 1, "canada": 1}, {"instead": 1, "direct": 1, "interactions": 1, "users": 1}, {"purpose": 1, "take": 1, "modelbased": 1, "learn": 2, "approach": 1, "work": 1, "estimate": 1, "model": 1, "user": 1, "behavior": 1, "offline": 1, "data": 1, "use": 1, "interact": 1, "agent": 1, "obtain": 1, "improve": 1, "policy": 1, "simultaneously": 1}, {"modelbased": 1, "rl": 1, "strong": 1, "advantage": 1, "sample": 1, "efficient": 1, "help": 1, "reduce": 1, "noise": 1, "offline": 1, "data": 1}, {"however": 1, "advantage": 1, "easily": 1, "diminish": 1, "due": 1, "inherent": 1, "bias": 1, "model": 1, "approximation": 1, "real": 1, "environment": 1}, {"moreover": 1, "dramatic": 1, "change": 1, "subsequent": 1, "policy": 1, "update": 2, "impose": 1, "risk": 1, "decrease": 1, "user": 1, "satisfaction": 1, "ie": 1, "inconsistent": 1, "recommendations": 1, "across": 1, "model": 1}, {"address": 1, "issue": 1, "introduce": 1, "adversarial": 1, "train": 1, "recommenders": 1, "policy": 1, "learn": 1, "offline": 1, "data": 1}, {"discriminator": 1, "train": 1, "differentiate": 1, "simulate": 1, "interaction": 1, "trajectories": 1, "real": 1, "ones": 1, "debias": 1, "user": 1, "behavior": 1, "model": 1, "improve": 1, "policy": 1, "learn": 1}, {"best": 1, "knowledge": 1, "first": 1, "work": 1, "explore": 1, "adversarial": 1, "train": 1, "modelbased": 1, "rl": 1, "framework": 1, "recommendation": 1}, {"theoretically": 1, "empirically": 1, "demonstrate": 1, "value": 1, "propose": 1, "solution": 1, "policy": 1, "evaluation": 1}, {"together": 1, "main": 1, "contributions": 1, "work": 1, "follow": 1, "": 1, "avoid": 1, "high": 1, "interaction": 1, "cost": 1, "propose": 1, "unify": 1, "solution": 1, "effectively": 1, "utilize": 1, "log": 1, "offline": 1, "data": 1, "modelbased": 1, "rl": 1, "algorithms": 1, "integrate": 1, "via": 1, "adversarial": 1, "train": 1}, {"enable": 1, "robust": 1, "recommendation": 1, "policy": 1, "learn": 1}, {"": 1, "propose": 1, "model": 1, "verify": 1, "theoretical": 1, "analysis": 1, "extensive": 1, "empirical": 1, "evaluations": 1}, {"experiment": 1, "result": 1, "demonstrate": 1, "solutions": 2, "better": 1, "sample": 1, "efficiency": 1, "stateoftheart": 1, "baselines": 1, "2": 2, "": 3, "relate": 1, "work": 1, "deep": 2, "rl": 2, "recommendation": 1, "study": 1, "utilize": 1, "news": 1, "music": 1, "video": 1, "recommendations": 1, "17": 1, "15": 1, "38": 1}, {"however": 1, "exist": 1, "solutions": 1, "modelfree": 1, "methods": 1, "thus": 1, "explicitly": 1, "model": 1, "agentuser": 1, "interactions": 1}, {"methods": 1, "valuebased": 1, "approach": 1, "deep": 1, "qlearning": 1, "20": 1, "present": 1, "unique": 1, "advantage": 1, "seamless": 1, "offpolicy": 1, "learn": 1, "prone": 1, "instability": 1, "function": 1, "approximation": 1, "30": 1, "19": 1}, {"policys": 1, "convergence": 1, "algorithms": 1, "wellstudied": 1}, {"contrast": 1, "policybased": 1, "methods": 1, "policy": 1, "gradient": 1, "14": 1, "remain": 1, "stable": 1, "suffer": 1, "data": 1, "bias": 1, "without": 1, "realtime": 1, "interactive": 1, "control": 1, "due": 1, "learn": 1, "infrastructure": 1, "constraints": 1}, {"oftentimes": 1, "importance": 1, "sample": 1, "22": 1, "adopt": 1, "address": 1, "bias": 1, "instead": 1, "result": 1, "huge": 1, "variance": 1, "2": 1}, {"work": 1, "rely": 1, "policy": 2, "gradient": 1, "base": 1, "rl": 1, "approach": 1, "particular": 1, "reinforce": 1, "34": 1, "simultaneously": 1, "estimate": 2, "user": 1, "behavior": 1, "model": 1, "provide": 1, "reliable": 1, "environment": 1, "update": 1, "agent": 1}, {"modelbased": 2, "rl": 2, "algorithms": 1, "incorporate": 1, "model": 1, "environment": 1, "predict": 1, "reward": 1, "unseen": 1, "stateaction": 1, "pair": 1}, {"know": 1, "general": 1, "outperform": 1, "modelfree": 1, "solutions": 1, "term": 1, "sample": 1, "complexity": 1, "7": 1, "apply": 1, "successfully": 1, "control": 1, "robotic": 1, "systems": 1, "simulation": 1, "real": 1, "world": 1, "5": 1, "18": 1, "21": 1, "6": 1}, {"furthermore": 1, "dynaq": 1, "29": 1, "24": 1, "integrate": 1, "modelfree": 1, "modelbased": 1, "rl": 1, "generate": 1, "sample": 1, "learn": 1, "addition": 1, "real": 1, "interaction": 1, "data": 1}, {"gu": 1, "et": 1, "al": 1}, {"10": 1, "extend": 1, "ideas": 1, "neural": 1, "network": 1, "model": 1, "peng": 1, "et": 1, "al": 1}, {"24": 1, "apply": 1, "method": 1, "taskcompletion": 1, "dialogue": 1, "policy": 1, "learn": 1}, {"however": 1, "efficient": 1, "modelbased": 1, "algorithms": 1, "use": 1, "relatively": 1, "simple": 1, "function": 1, "approximations": 1, "actually": 1, "difficulties": 1, "highdimensional": 1, "space": 1, "nonlinear": 1, "dynamics": 1, "thus": 1, "lead": 1, "huge": 1, "approximation": 1, "bias": 1}, {"offline": 2, "evaluation": 2, "problems": 1, "offpolicy": 1, "learn": 1, "22": 1, "25": 1, "26": 1, "policy": 1, "generally": 1, "pervasive": 1, "challenge": 1, "rl": 1, "recommender": 1, "systems": 1, "particular": 1}, {"policy": 1, "evolve": 1, "distribution": 1, "expectation": 1, "gradient": 1, "compute": 1}, {"especially": 1, "scenario": 1, "recommender": 1, "systems": 1, "item": 1, "catalogue": 1, "user": 1, "behavior": 1, "change": 2, "rapidly": 1, "substantial": 1, "policy": 3, "require": 1, "therefore": 1, "feasible": 1, "take": 1, "classic": 1, "approach": 1, "27": 1, "1": 1, "constrain": 1, "update": 2, "new": 1, "data": 1, "collect": 1}, {"multiple": 1, "offpolicy": 1, "estimators": 1, "leverage": 1, "inversepropensity": 2, "score": 2, "cap": 1, "various": 1, "variance": 1, "control": 1, "measure": 1, "develop": 1, "33": 1, "32": 1, "31": 1, "8": 1, "purpose": 1}, {"rl": 1, "adversarial": 1, "train": 1, "yu": 1, "et": 1, "al": 1}, {"36": 1, "propose": 1, "seqgan": 1, "extend": 1, "gans": 1, "rllike": 1, "generator": 1, "sequence": 1, "generation": 1, "problem": 1, "reward": 1, "signal": 1, "provide": 1, "discriminator": 1, "end": 1, "episode": 1, "via": 1, "monte": 1, "carlo": 1, "sample": 1, "approach": 1}, {"generator": 1, "take": 1, "sequential": 1, "action": 1, "learn": 1, "policy": 1, "use": 1, "estimate": 1, "cumulative": 1, "reward": 1}, {"solution": 1, "generator": 1, "consist": 1, "two": 1, "components": 1, "ie": 1, "recommendation": 1, "agent": 1, "user": 1, "behavior": 1, "model": 1, "2": 1, "": 1, "implementation": 1, "available": 1, "httpsgithubcomjianguanthuirecgan": 1}, {"2": 1, "": 1, "model": 1, "interactive": 1, "process": 1, "via": 1, "adversarial": 1, "train": 1, "policy": 1, "gradient": 1}, {"different": 1, "sequence": 2, "generation": 1, "task": 1, "aim": 1, "generate": 1, "similar": 1, "give": 1, "observations": 1, "leverage": 1, "adversarial": 1, "train": 2, "help": 1, "reduce": 2, "bias": 1, "user": 1, "model": 1, "variance": 1, "agent": 1}, {"agent": 1, "learn": 1, "interactions": 1, "user": 1, "behavior": 1, "model": 1, "store": 1, "log": 1, "offline": 1, "data": 1}, {"best": 1, "knowledge": 1, "first": 1, "work": 1, "utilize": 1, "adversarial": 1, "train": 1, "improve": 1, "model": 1, "approximation": 1, "policy": 1, "learn": 1, "offline": 1, "data": 1}, {"3": 1, "": 2, "problem": 2, "statement": 1, "learn": 1, "policy": 1, "offline": 1, "data": 1, "deploy": 1, "online": 1, "maximize": 1, "cumulative": 1, "reward": 1, "collect": 1, "interactions": 1, "users": 1}, {"address": 1, "problem": 1, "modelbased": 1, "reinforcement": 1, "learn": 1, "solution": 1, "explicitly": 1, "model": 1, "users": 1, "behavior": 1, "pattern": 1, "data": 1}, {"problem": 1, "recommender": 1, "form": 1, "learn": 1, "agent": 1, "generate": 1, "action": 2, "policy": 1, "give": 1, "recommendation": 1, "list": 1, "k": 1, "items": 1}, {"every": 1, "time": 1, "interactions": 1, "agent": 2, "environment": 1, "ie": 1, "users": 1, "system": 1, "set": 1, "": 17, "n": 2, "sequence": 2, "1": 1, "record": 1, "ith": 1, "contain": 1, "action": 2, "user": 2, "behaviors": 1, "reward": 2, "ai0": 1, "ci0": 1, "r0i": 1, "ai1": 1, "ci1": 1, "r1i": 1, "ait": 3, "cit": 2, "rti": 2, "represent": 1, "eg": 2, "make": 1, "purchase": 1, "associate": 1, "behavior": 1, "correspond": 1, "agents": 1, "click": 1, "recommend": 1, "item": 1}, {"simplicity": 1, "rest": 1, "paper": 1, "drop": 1, "superscript": 1, "represent": 1, "general": 1, "sequence": 1, "": 2}, {"base": 1, "observe": 1, "sequence": 1, "policy": 1, "": 6, "learn": 1, "maximize": 1, "expect": 1, "cumulative": 1, "reward": 1, "pt": 1, "e": 1, "t0": 1, "rt": 1, "end": 1, "time": 1}, {"assumption": 1, "narrow": 1, "scope": 1, "discussion": 1, "study": 1, "typical": 1, "type": 1, "user": 2, "behavior": 1, "ie": 1, "click": 4, "make": 1, "follow": 1, "assumptions": 1, "1": 1, "time": 1, "must": 1, "one": 1, "item": 1, "recommendation": 2, "list": 2, "2": 1, "items": 2, "influence": 1, "users": 1, "future": 1, "behaviors": 1, "3": 1, "reward": 1, "relate": 1}, {"example": 1, "take": 1, "users": 1, "purchase": 2, "reward": 1, "happen": 1, "click": 1, "items": 1}, {"learn": 1, "framework": 1, "markov": 1, "decision": 1, "process": 1, "environment": 1, "consist": 1, "state": 2, "set": 2, "action": 1, "transition": 1, "distribution": 1, "p": 1, "": 6, "reward": 1, "function": 1, "fr": 1, "r": 1, "map": 1, "stateaction": 1, "pair": 1, "realvalued": 1, "scalar": 1}, {"paper": 1, "environment": 1, "model": 2, "user": 1, "behavior": 1, "u": 1, "learn": 1, "offline": 1, "log": 1, "data": 1}, {"reflect": 1, "interaction": 1, "history": 1, "time": 1, "p": 1, "capture": 1, "transition": 1, "user": 1, "behaviors": 1}, {"meanwhile": 1, "base": 2, "assumptions": 1, "mention": 1, "time": 1, "environment": 1, "generate": 2, "users": 1, "click": 3, "ct": 2, "items": 1, "recommend": 1, "agent": 1, "hisher": 1, "probabilities": 1, "current": 1, "state": 1, "reward": 2, "function": 1, "fr": 1, "rt": 1, "item": 1, "": 1}, {"recommendation": 1, "policy": 1, "learn": 2, "offline": 1, "data": 2, "sample": 1, "user": 1, "behavior": 1, "model": 1, "ie": 1, "modelbased": 1, "rl": 1, "solution": 1}, {"incorporate": 1, "adversarial": 1, "train": 1, "modelbased": 1, "policy": 1, "learn": 1, "1": 1, "improve": 1, "user": 1, "model": 1, "ensure": 1, "sample": 1, "data": 2, "close": 1, "true": 1, "distribution": 1, "2": 1, "utilize": 1, "discriminator": 1, "scale": 1, "reward": 1, "generate": 1, "sequence": 1, "reduce": 1, "bias": 1, "value": 1, "estimation": 1}, {"propose": 1, "solution": 1, "contain": 1, "interactive": 1, "model": 1, "construct": 1, "u": 1, "adversarial": 1, "policy": 1, "learn": 1, "approach": 1}, {"name": 1, "solution": 1, "interactive": 1, "recommender": 1, "gin": 1, "irecgan": 1, "short": 1}, {"overview": 1, "propose": 1, "solution": 1, "show": 1, "figure": 1, "1": 1}, {"4": 1, "": 2, "interactive": 2, "model": 3, "recommendation": 2, "present": 1, "consist": 1, "two": 1, "components": 1, "1": 1, "user": 2, "behavior": 1, "u": 1, "generate": 2, "click": 1, "recommend": 1, "items": 1, "correspond": 1, "reward": 1, "2": 1, "agent": 1, "recommendations": 1, "accord": 1, "policy": 1}, {"u": 1, "interact": 1, "generate": 1, "user": 1, "behavior": 1, "sequence": 1, "adversarial": 1, "policy": 1, "learn": 1}, {"user": 2, "behavior": 2, "model": 2, "give": 1, "users": 1, "click": 2, "observations": 1, "c0": 1, "": 5, "c1": 1, "ct1": 1, "u": 1, "first": 1, "project": 1, "item": 1, "embed": 1, "vector": 1, "eu": 1, "time": 1, "3": 1}, {"state": 1, "sut": 2, "represent": 1, "summary": 1, "click": 1, "history": 1, "ie": 1, "": 4, "hu": 1, "eu0": 1, "eu1": 1, "eut1": 1}, {"use": 3, "recurrent": 1, "neural": 1, "network": 1, "model": 1, "state": 2, "transition": 1, "p": 1, "user": 2, "side": 3, "thus": 1, "sut": 2, "": 4, "hu": 1, "sut1": 1, "eut1": 1, "3": 1, "different": 1, "embeddings": 1, "agent": 1, "superscript": 1, "u": 1, "denote": 1, "difference": 1, "accordingly": 1}, {"3": 1, "": 44, "0": 7, "mlp": 3, "environment": 1, "reward": 1, "1": 8, "2": 4, "mc": 1, "search": 1, "discriminator": 3, "score": 1, "real": 1, "offline": 1, "data": 2, "generate": 1, "interactive": 1, "model": 2, "adversarial": 1, "policy": 1, "learn": 1, "b": 1, "train": 1, "figure": 1, "overview": 1, "irecgan": 1}, {"u": 1, "denote": 1, "agent": 1, "model": 2, "user": 1, "behavior": 1, "discriminator": 1, "respectively": 1}, {"irecgan": 1, "u": 2, "interact": 1, "generate": 1, "recommendation": 2, "sequence": 1, "close": 1, "true": 1, "data": 1, "distribution": 1, "jointly": 1, "reduce": 1, "bias": 1, "improve": 1, "quality": 1, "hu": 1, "": 2, "function": 1, "rnn": 1, "family": 1, "like": 1, "gru": 1, "4": 1, "lstm": 1, "12": 1, "cells": 1}, {"give": 1, "action": 1, "": 18, "at1": 1, "atk": 1, "ie": 1, "topk": 1, "recommendations": 1, "time": 1, "compute": 1, "probability": 1, "click": 2, "among": 1, "recommend": 3, "items": 2, "via": 1, "softmax": 1, "function": 1, "xat": 1, "vc": 1, "wc": 2, "sut": 3, "bc": 2, "eut": 2, "pct": 1, "expvic": 1, "expvjc": 1, "1": 1, "j1": 1, "c": 1, "k": 1, "v": 1, "r": 1, "transform": 1, "vector": 1, "indicate": 1, "evaluate": 1, "quality": 1, "item": 1, "ati": 1, "state": 1, "embed": 1, "matrix": 2, "weight": 1, "correspond": 1, "bias": 1, "term": 1}, {"assumption": 1, "target": 1, "reward": 5, "relate": 1, "click": 1, "items": 1, "rt": 2, "sut": 3, "": 9, "calculate": 1, "fr": 2, "wr": 2, "br": 2, "eut": 1, "2": 1, "weight": 1, "matrix": 1, "correspond": 1, "bias": 1, "term": 1, "map": 1, "function": 1, "set": 1, "accord": 1, "definition": 1, "specific": 1, "recommender": 1, "systems": 1}, {"example": 1, "make": 1, "rt": 3, "purchase": 2, "click": 1, "item": 1, "ct": 1, "": 3, "1": 1, "0": 1, "otherwise": 1, "fr": 1, "realize": 1, "sigmoid": 1, "function": 1, "binary": 1, "output": 1}, {"base": 1, "eq": 1, "1": 1, "2": 1, "take": 1, "categorical": 1, "reward": 2, "user": 1, "behavior": 1, "model": 1, "u": 1, "estimate": 1, "offline": 1, "data": 1, "": 10, "via": 1, "maximum": 1, "likelihood": 1, "estimation": 1, "x": 1, "xti": 1, "max": 1, "log": 2, "pcit": 1, "sut": 2, "ait": 1, "p": 2, "prti": 1, "cit": 1, "3": 1, "t0": 1, "parameter": 1, "balance": 1, "loss": 1, "click": 1, "prediction": 2, "ti": 1, "length": 1, "observation": 1, "sequence": 1}, {"learn": 1, "user": 2, "behavior": 1, "model": 1, "click": 1, "reward": 1, "recommendation": 1, "list": 1, "sample": 1, "eq": 1, "1": 1, "2": 1, "accordingly": 1}, {"agent": 2, "take": 1, "action": 1, "base": 1, "environments": 1, "provide": 1, "state": 1}, {"however": 1, "practice": 1, "users": 1, "state": 1, "observable": 1, "recommender": 1, "system": 1}, {"besides": 1, "discuss": 1, "23": 1, "state": 1, "agent": 1, "take": 1, "action": 1, "may": 1, "different": 1, "users": 1, "generate": 1, "click": 1, "reward": 1}, {"result": 1, "build": 1, "different": 1, "state": 2, "model": 1, "agent": 1, "side": 1, "learn": 1}, {"similar": 1, "user": 1, "side": 2, "give": 1, "project": 1, "click": 1, "vectors": 1, "ea0": 1, "": 8, "ea2": 1, "eat1": 2, "model": 1, "state": 2, "agent": 2, "sit": 2, "ha": 2, "sat1": 1, "denote": 1, "maintain": 1, "time": 1, "choose": 1, "rnn": 1, "cell": 1}, {"initial": 1, "state": 1, "sa0": 1, "first": 1, "recommendation": 1, "draw": 1, "distribution": 1, "": 1}, {"simply": 1, "denote": 1, "s0": 1, "rest": 1, "paper": 1}, {"note": 1, "although": 1, "agent": 1, "also": 1, "model": 1, "state": 2, "base": 1, "users": 1, "click": 1, "history": 1, "might": 1, "create": 1, "different": 1, "sequence": 1, "user": 1, "side": 1}, {"base": 1, "current": 1, "state": 1, "sit": 1, "": 2, "agent": 1, "generate": 1, "sizek": 1, "recommendation": 1, "list": 1, "entire": 1, "set": 1, "items": 1, "action": 1}, {"probability": 1, "item": 1, "include": 1, "policy": 1, "": 12, "expwia": 1, "sit": 2, "bai": 2, "pc": 1, "j1": 1, "expwj": 1, "st": 1, "bj": 1, "4": 2, "wia": 1, "ith": 1, "row": 1, "action": 1, "weight": 1, "matrix": 1, "wa": 1, "c": 1, "entire": 1, "set": 1, "recommendation": 1, "candidates": 1, "correspond": 1, "bias": 1, "term": 1}, {"follow": 1, "2": 1, "generate": 1, "sample": 1, "without": 1, "replacement": 1, "accord": 1, "eq": 1, "4": 1}, {"unlike": 1, "3": 1, "consider": 1, "combinatorial": 1, "effect": 1, "among": 1, "k": 1, "items": 1, "simply": 1, "assume": 1, "users": 1, "evaluate": 1, "independently": 1, "indicate": 1, "eq": 1, "1": 1}, {"5": 1, "": 2, "adversarial": 1, "policy": 3, "learn": 2, "use": 1, "gradient": 1, "method": 1, "reinforce": 1, "34": 1, "forthe": 1, "agents": 1, "base": 1, "generate": 1, "offline": 1, "data": 1}, {"generate": 1, "0t": 1, "": 17, "a0": 1, "c0": 1, "r0": 1, "ct": 3, "rt": 2, "0": 1, "c": 3, "obtain": 1, "a0t1": 1, "eq": 3, "4": 1, "uc": 1, "0t1": 2, "2": 1, "ur": 1, "1": 1}, {"c": 1, "u": 1, "": 5, "represent": 1, "click": 1, "sequence": 1, "a0": 1, "c0": 1, "r0": 1, "generate": 1, "s0": 2, "accordingly": 1}, {"generation": 1, "sequence": 1, "end": 1, "time": 1, "ct": 1, "": 2, "cend": 2, "stop": 1, "symbol": 1}, {"distributions": 1, "generate": 1, "offline": 1, "data": 2, "denote": 1, "g": 1, "respectively": 1}, {"follow": 1, "discussions": 1, "explicitly": 1, "differentiate": 1, "": 2, "distribution": 1, "specify": 1}, {"since": 1, "start": 1, "train": 1, "u": 1, "offline": 1, "data": 1, "introduce": 1, "inherent": 1, "bias": 1, "observations": 1, "specific": 1, "model": 1, "choices": 1}, {"bias": 2, "affect": 1, "sequence": 1, "generation": 1, "thus": 1, "may": 1, "cause": 1, "value": 1, "estimation": 1}, {"reduce": 1, "effect": 1, "bias": 1, "apply": 1, "adversarial": 1, "train": 2, "control": 1, "u": 1}, {"discriminator": 1, "also": 1, "use": 1, "rescale": 1, "generate": 1, "reward": 1, "r": 1, "policy": 1, "learn": 1}, {"therefore": 1, "learn": 1, "agent": 1, "consider": 1, "sequence": 1, "generation": 1, "target": 1, "reward": 1}, {"51": 1, "": 2, "adversarial": 2, "train": 2, "leverage": 1, "encourage": 1, "irecgan": 1, "model": 1, "generate": 1, "highquality": 1, "sequence": 1, "capture": 1, "intrinsic": 1, "pattern": 1, "real": 1, "data": 1, "distribution": 1}, {"discriminator": 1, "use": 1, "evaluate": 1, "give": 1, "sequence": 1, "": 4, "represent": 1, "probability": 1, "generate": 1, "real": 1, "recommendation": 1, "environment": 1}, {"discriminator": 1, "estimate": 1, "minimize": 1, "objective": 1, "function": 1, "": 7, "e": 2, "data": 1, "log": 2, "g": 1, "1": 1}, {"5": 1, "however": 1, "evaluate": 2, "complete": 1, "sequence": 3, "hence": 1, "cannot": 1, "directly": 1, "partially": 1, "generate": 1, "particular": 1, "time": 2, "step": 1, "inspire": 1, "36": 1, "utilize": 1, "montecarlo": 1, "tree": 1, "search": 1, "algorithm": 1, "rollout": 1, "policy": 1, "construct": 1, "u": 1, "get": 1, "generation": 1, "score": 1}, {"time": 1, "sequence": 2, "generation": 1, "score": 1, "qd": 2, "0t": 5, "define": 1, "": 11, "pn": 1, "1": 1, "n": 6, "u": 3, "n1": 1, "d0t": 2, "c": 2, "6": 1, "tt": 1, "set": 1, "sample": 1, "interaction": 1}, {"give": 1, "observations": 1, "offline": 1, "data": 2, "u": 1, "generate": 1, "click": 1, "reward": 1, "reflect": 1, "intrinsic": 1, "patternsp": 1, "real": 1, "distribution": 1}, {"therefore": 1, "u": 1, "maximize": 1, "sequence": 2, "generation": 1, "objective": 1, "esu0": 1, "": 7, "a0": 2, "c0": 1, "r0": 2, "g": 1, "uc0": 1, "su0": 1, "qd": 1, "00": 1, "expect": 1, "discriminator": 1, "score": 1, "generate": 1, "initial": 1, "state": 1}, {"u": 1, "may": 1, "generate": 2, "click": 1, "reward": 1, "exactly": 1, "offline": 2, "data": 3, "similarity": 1, "still": 1, "informative": 1, "signal": 1, "evaluate": 1, "sequence": 1, "generation": 1, "quality": 1}, {"set": 1, "qd": 1, "0t": 1, "": 2, "1": 1, "time": 1, "offline": 3, "data": 4, "extend": 1, "objective": 1, "include": 1, "become": 1, "likelihood": 1, "function": 1}, {"follow": 1, "36": 1, "base": 1, "eq": 2, "1": 1, "2": 1, "gradient": 1, "us": 1, "objective": 1, "derive": 1, "h": 1, "xt": 1, "e": 1, "gdata": 1, "qd": 1, "0t": 1, "u": 3, "log": 2, "pu": 2, "ct": 2, "sut": 2, "": 7, "p": 1, "rt": 1, "7": 1, "t0": 1, "denote": 2, "parameters": 1}, {"base": 1, "assumption": 1, "even": 1, "u": 1, "already": 1, "capture": 1, "users": 1, "true": 1, "behavior": 1, "pattern": 1, "still": 1, "depend": 1, "provide": 1, "appropriate": 1, "recommendations": 1, "generate": 1, "click": 1, "reward": 1, "discriminator": 1, "treat": 1, "authentic": 1}, {"hence": 1, "u": 1, "couple": 1, "adversarial": 1, "train": 1}, {"encourage": 1, "provide": 1, "need": 1, "recommendations": 1, "include": 1, "qd": 1, "0t": 1, "": 1, "sequence": 1, "generation": 1, "reward": 1, "time": 1, "well": 1}, {"qd": 2, "0t": 3, "": 5, "evaluate": 2, "overall": 1, "generation": 2, "quality": 2, "ignore": 1, "sequence": 2, "generations": 1, "whole": 1, "require": 1, "maximize": 1, "cumulative": 1, "psequence": 1, "reward": 1, "e": 1, "gdata": 1, "generate": 1, "t0": 1}, {"directly": 1, "": 3, "pt": 1, "observations": 1, "interaction": 1, "sequence": 1, "approximate": 1, "q": 1, "0": 1, "t0": 1, "0t": 1, "calculate": 1, "gradients": 1}, {"put": 1, "together": 1, "gradient": 1, "derive": 1, "sequence": 1, "generations": 1, "estimate": 1, "xt": 2, "": 8, "0": 1, "e": 1, "gdata": 1, "qd": 1, "0t": 1, "log": 1, "ct": 1, "sit": 1}, {"8": 1, "0": 1, "t0": 1, "": 8, "5": 1, "base": 1, "assumption": 1, "click": 2, "items": 2, "influence": 1, "user": 1, "behaviors": 1, "u": 1, "generate": 1, "reward": 2, "use": 1, "ct": 2, "sit": 2, "estimation": 2, "ie": 1, "promote": 1, "recommendation": 1, "time": 1, "practice": 1, "add": 1, "discount": 1, "factor": 1, "1": 1, "calculate": 1, "cumulative": 1, "reduce": 1, "variance": 1, "2": 1}, {"52": 1, "": 2, "policy": 2, "learn": 2, "adversarial": 1, "train": 1, "encourage": 1, "irecgan": 1, "generate": 2, "click": 2, "reward": 2, "similar": 1, "pattern": 1, "offline": 2, "data": 3, "assume": 1, "relate": 1, "items": 1, "use": 1, "well": 1}, {"give": 1, "data": 2, "0t": 1, "": 11, "a0": 1, "c0": 1, "r0": 1, "ct": 1, "rt": 4, "include": 1, "offline": 1, "generate": 1, "objective": 1, "agent": 1, "maximize": 1, "expect": 1, "pt": 1, "cumulative": 1, "reward": 1, "e": 1, "gdata": 1, "t0": 1}, {"generate": 3, "data": 1, "due": 1, "difference": 1, "distributions": 1, "offline": 1, "sequence": 1, "reward": 1, "rt": 1, "calculate": 1, "eq": 1, "2": 1, "might": 1, "bias": 1}, {"reduce": 1, "bias": 1, "utilize": 1, "sequence": 1, "generation": 1, "score": 1, "eq": 1, "6": 1, "rescale": 1, "generate": 2, "reward": 2, "rts": 1, "": 2, "qd": 1, "0t": 1, "rt": 1, "treat": 1, "data": 1}, {"gradient": 1, "objective": 1, "thus": 1, "estimate": 1, "xt": 2, "": 10, "0": 2, "e": 1, "gdata": 1, "rt": 5, "log": 1, "ct": 1, "sit": 1, "qd": 1, "0t": 1, "9": 1, "t0": 1, "approximation": 1, "discount": 1, "factor": 1}, {"overall": 1, "user": 1, "behavior": 1, "model": 1, "u": 1, "update": 2, "sequence": 2, "generation": 2, "objective": 1, "define": 1, "eq": 1, "7": 1, "offline": 1, "generate": 1, "data": 1, "agent": 1, "target": 1, "reward": 1}, {"hence": 1, "overall": 1, "reward": 2, "time": 1, "qd": 1, "0t": 1, "1": 1, "": 2, "r": 2, "rt": 1, "weight": 1, "cumulative": 1, "target": 1}, {"overall": 1, "gradient": 1, "thus": 1, "xt": 2, "": 24, "0": 2, "e": 1, "gdata": 1, "rta": 2, "log": 2, "ct": 1, "sit": 1, "qd": 1, "0t": 1, "1": 2, "r": 1, "rt": 1, "10": 1, "t0": 1, "6": 1, "theoretical": 1, "analysis": 1, "one": 1, "iteration": 1, "policy": 2, "learn": 2, "irecgan": 2, "first": 1, "train": 1, "discriminator": 2, "offline": 1, "data": 2, "follow": 1, "pdata": 3, "generate": 2, "unknown": 1, "distribution": 1, "g": 1, "u": 1, "give": 1, "sequence": 1, "proposition": 1, "9": 1, "optimal": 1, "pg": 1}, {"sequence": 2, "generation": 2, "u": 1, "contribute": 1, "irecgan": 1}, {"u": 1, "update": 1, "gradient": 1, "eq": 1, "7": 1, "maximize": 1, "sequence": 1, "generation": 1, "objective": 1}, {"time": 1, "expect": 1, "sequence": 1, "generation": 1, "reward": 1, "generate": 1, "data": 1, "e0t": 2, "g": 2, "qd": 1, "0t": 2, "": 3, "d0t": 1}, {"": 10, "pt": 2, "expect": 1, "value": 1, "0t": 3, "e": 2, "g": 3, "vg": 1, "t0": 2, "qd": 1, "e0t": 1, "d0t": 1}, {"": 6, "give": 1, "optimal": 1, "sequence": 1, "generation": 1, "value": 1, "write": 1, "h": 1, "xt": 1, "pdata": 1, "0t": 2, "e": 1, "g": 2, "vg": 1, "e0t": 1}, {"11": 2, "t0": 1, "pdata": 1, "0t": 4, "": 3, "pg": 1, "maximize": 1, "term": 1, "summation": 1, "eq": 1, "objective": 1, "generator": 1, "time": 1, "gin": 1}, {"accord": 1, "9": 1, "optimal": 1, "solution": 1, "term": 1, "pg": 1, "0t": 2, "s0": 2, "": 3, "pdata": 1}, {"mean": 1, "maximize": 1, "sequence": 2, "generation": 1, "value": 1, "help": 1, "generate": 1, "distribution": 1, "data": 1}, {"besides": 1, "global": 1, "optimal": 1, "eq": 1, "11": 1, "also": 1, "encourage": 1, "reward": 1, "pg": 2, "0t": 5, "": 4, "pdata": 1, "even": 1, "less": 1, "likely": 1, "generate": 1}, {"prevent": 1, "irecgan": 1, "recommend": 1, "items": 1, "consider": 1, "users": 1, "immediate": 1, "preferences": 1}, {"value": 2, "estimation": 1, "agent": 1, "also": 1, "update": 1, "maximize": 1, "expect": 1, "target": 1, "reward": 1, "va": 1, "": 1}, {"achieve": 1, "use": 1, "discriminator": 1, "rescale": 1, "estimation": 1, "va": 3, "generate": 2, "sequence": 1, "also": 1, "combine": 1, "offline": 1, "data": 2, "evaluate": 1, "policy": 1, "": 10, "xt": 2, "pdata": 2, "0t": 3, "e": 1, "1": 1, "e0t": 2, "g": 1, "rt": 4, "2": 1, "12": 1, "t0": 2, "pg": 1, "reward": 2, "u": 1, "time": 1, "true": 1}, {"1": 3, "2": 2, "represent": 1, "ratio": 1, "generate": 1, "data": 2, "offline": 1, "model": 1, "train": 1, "require": 1, "": 2}, {"simplify": 1, "p": 2, "0t": 3, "": 2}, {"result": 1, "three": 1, "source": 1, "bias": 1, "value": 1, "estimation": 1, "": 12, "rt": 2, "1": 3, "pa": 2, "0t": 4, "pg": 1, "6": 1, "2": 1, "pdata": 1}, {"base": 1, "different": 1, "source": 1, "bias": 1, "expect": 1, "value": 1, "estimation": 1, "eq": 1, "12": 1, "e": 1, "": 40, "va": 2, "1": 3, "x": 5, "e0t": 5, "g": 1, "t0": 5, "p": 1, "pa": 1, "0t": 4, "rt": 4, "2": 6, "data": 2, "pg": 1, "pdata": 1, "wt": 3, "2112": 1}, {"": 1, "1": 1, "come": 1, "bias": 1, "user": 1, "behavior": 1, "model": 1, "u": 1}, {"adversarial": 1, "train": 1, "help": 1, "improve": 1, "u": 1, "capture": 1, "real": 1, "data": 1, "pattern": 1, "decrease": 1, "": 2, "2": 1}, {"adjust": 1, "sample": 1, "ratio": 1, "1": 1, "reduce": 1, "wt": 2, "": 2, "small": 1}, {"sequence": 1, "generation": 1, "reward": 1, "p": 1, "": 3, "0t": 1, "agent": 1, "encourage": 1, "distribution": 1, "g": 1, "close": 1, "data": 1}, {"2": 2, "": 4, "1": 1, "pg": 1, "a0t": 1, "bias": 1, "also": 1, "reduce": 1}, {"show": 1, "method": 1, "bias": 1, "control": 1, "effect": 1}, {"7": 1, "": 6, "pg": 1, "0t": 2, "pdata": 1, "experiment": 1, "theoretical": 1, "analysis": 1, "find": 1, "reduce": 1, "model": 1, "bias": 1, "improve": 2, "value": 1, "estimation": 1, "therefore": 1, "policy": 1, "learn": 1}, {"section": 1, "conduct": 1, "empirical": 1, "evaluations": 1, "realworld": 1, "synthetic": 1, "datasets": 1, "demonstrate": 1, "solution": 1, "effectively": 1, "model": 1, "pattern": 1, "data": 1, "better": 1, "recommendations": 1, "compare": 1, "stateoftheart": 1, "solutions": 1}, {"71": 1, "": 2, "simulate": 1, "online": 2, "test": 1, "subject": 1, "difficulty": 1, "deploy": 1, "recommender": 1, "system": 1, "real": 1, "users": 1, "evaluation": 1, "use": 1, "simulationbased": 1, "study": 1, "first": 1, "investigate": 1, "effectiveness": 1, "approach": 1, "follow": 1, "37": 1, "3": 1}, {"simulate": 2, "environment": 2, "synthesize": 1, "mdp": 1, "online": 1, "recommendation": 1}, {"state": 1, "n": 1, "items": 1, "recommendation": 1, "randomly": 1, "initialize": 1, "transition": 1, "probability": 1, "matrix": 1, "p": 1, "": 3, "saj": 1, "si": 1}, {"state": 1, "si": 1, "": 3, "item": 1, "aj": 1, "reward": 1, "raj": 1, "asi": 1, "uniformly": 1, "sample": 1, "range": 1, "0": 1, "1": 1}, {"interaction": 1, "give": 1, "recommendation": 1, "list": 1, "include": 1, "k": 1, "items": 1, "select": 1, "whole": 1, "item": 2, "set": 1, "agent": 1, "simulator": 1, "first": 1, "sample": 1, "proportional": 1, "groundtruth": 1, "reward": 1, "current": 1, "state": 1, "si": 1, "click": 1, "candidate": 1}, {"denote": 1, "sample": 1, "item": 2, "aj": 1, "": 4, "bernoulli": 1, "experiment": 1, "perform": 1, "raj": 1, "success": 1, "probability": 2, "simulator": 1, "move": 1, "next": 1, "state": 2, "accord": 1, "transition": 1, "psaj": 1, "si": 1}, {"special": 1, "state": 1, "s0": 1, "use": 1, "initialize": 1, "sessions": 1, "stop": 1, "bernoulli": 1, "experiment": 1, "fail": 1}, {"immediate": 1, "reward": 1, "1": 1, "session": 1, "continue": 1, "next": 1, "step": 1, "otherwise": 1, "0": 1}, {"experiment": 1, "n": 1, "k": 1, "set": 1, "10": 2, "50": 1, "respectively": 1}, {"offline": 2, "data": 1, "generation": 1, "generate": 1, "recommendation": 1, "log": 1, "denote": 1, "doff": 1, "simulator": 1}, {"bias": 1, "variance": 1, "doff": 2, "especially": 1, "control": 1, "change": 1, "log": 1, "policy": 1, "size": 1, "": 1}, {"adopt": 1, "three": 1, "different": 1, "log": 1, "policies": 1, "1": 1, "uniformly": 1, "random": 2, "policy": 3, "": 3, "2": 1, "maximum": 1, "reward": 2, "max": 1, "3": 1, "mix": 2}, {"specifically": 1, "max": 1, "recommend": 1, "top": 2, "k": 2, "items": 2, "highest": 2, "groundtruth": 3, "reward": 3, "current": 1, "simulator": 1, "state": 2, "step": 1, "mix": 1, "randomly": 1, "select": 1, "either": 1, "2050": 1, "give": 1}, {"meanwhile": 1, "vary": 1, "size": 1, "data": 1, "doff": 1, "200": 1, "10000": 1}, {"baselines": 2, "compare": 1, "irecgan": 2, "follow": 1, "1": 1, "lstm": 1, "user": 2, "behavior": 2, "model": 3, "train": 3, "offline": 2, "data": 2, "2": 1, "pg": 1, "agent": 1, "policy": 1, "gradient": 1, "3": 1, "lstmd": 1, "update": 1, "adversarial": 1}, {"experiment": 1, "settings": 1, "hyperparameters": 1, "model": 1, "set": 4, "follow": 1, "item": 1, "embed": 1, "dimension": 1, "50": 1, "discount": 1, "factor": 2, "": 1, "value": 1, "calculation": 1, "09": 1, "scale": 1, "r": 1, "p": 1, "3": 1, "1": 1}, {"use": 1, "2layer": 1, "lstm": 1, "units": 1, "512dimension": 1, "hide": 1, "state": 1}, {"ratio": 1, "generate": 1, "train": 2, "sample": 1, "offline": 1, "data": 1, "epoch": 1, "set": 1, "110": 1}, {"use": 1, "rnn": 1, "base": 1, "discriminator": 1, "experiment": 1, "detail": 1, "provide": 1, "appendix": 1}, {"online": 2, "evaluation": 2, "train": 1, "model": 1, "baselines": 1, "doff": 1, "": 1, "deploy": 1, "learn": 1, "policy": 1, "interact": 1, "simulator": 1}, {"calculate": 1, "coverager": 1, "measure": 1, "proportion": 1, "true": 1, "top": 2, "r": 1, "relevant": 1, "items": 2, "rank": 1, "k": 1, "recommend": 1, "model": 1, "across": 1, "time": 1, "step": 1, "detail": 1, "appendix": 1}, {"result": 1, "coverager": 1, "different": 1, "configurations": 1, "offline": 1, "data": 1, "generation": 1, "report": 1, "figure": 1, "2": 1}, {"random": 3, "": 71, "coverager": 6, "algorithms": 1, "relatively": 1, "low": 1, "r": 5, "large": 1, "difference": 1, "overall": 1, "performance": 1, "behavior": 1, "agent": 1, "7": 5, "05": 3, "04": 3, "03": 3, "1": 4, "2": 5, "3": 4, "4": 4, "5": 4, "6": 4, "8": 4, "9": 4, "10": 7, "reward": 9, "lstm": 4, "lstmd": 4, "065": 1, "36": 4, "35": 2, "34": 4, "33": 2, "32": 4, "31": 2, "30": 4, "29": 2, "pg": 4, "irecgan": 4, "060": 1, "055": 1, "050": 1, "045": 1, "040": 1, "200": 1, "42": 2, "40": 2, "38": 2, "06": 3, "b": 1, "000": 3, "07": 1, "c": 1, "max": 1, "mix": 1, "figure": 1, "online": 1, "evaluation": 1, "result": 1, "cumulative": 1}, {"040": 1, "": 20, "035": 2, "coverage10": 2, "coverage1": 2, "030": 2, "025": 2, "020": 2, "lstmoffline": 2, "pgonline": 2, "015": 2, "0": 2, "5": 2, "10": 2, "iteration": 2, "pgonlineoffline": 2, "irecganonlineoffline": 2, "15": 2, "20": 2, "figure": 1, "3": 1, "online": 1, "learn": 1, "result": 1, "model": 1, "large": 1}, {"suggest": 1, "difficulty": 1, "recognize": 1, "high": 1, "reward": 1, "items": 1, "random": 1, "": 1, "every": 1, "item": 1, "equal": 1, "chance": 1, "observe": 1, "ie": 1, "full": 1, "exploration": 1, "especially": 1, "small": 1, "size": 1, "offline": 1, "data": 1}, {"however": 1, "max": 1, "mix": 2, "": 2, "high": 1, "reward": 2, "items": 3, "sufficiently": 1, "learn": 1, "user": 1, "behavior": 1, "model": 3, "lstm": 1, "lstmd": 1, "fail": 1, "capture": 1, "overall": 1, "prefer": 1, "agent": 1, "pg": 1, "irecgan": 2, "stable": 1, "change": 1, "r": 1, "show": 1, "advantage": 1, "especially": 1, "require": 1, "differentiate": 1, "top": 1, "relevant": 1, "moderate": 1}, {"close": 1, "coverager": 1, "lstm": 1, "r": 1, "small": 1, "better": 1, "capture": 1, "users": 1, "overall": 1, "preferences": 1, "user": 1, "behavior": 1, "model": 1, "fail": 1, "seriously": 1}, {"reward": 2, "sufficiently": 1, "learn": 2, "fig": 1, "2a": 1, "mechanism": 1, "strengthen": 1, "influence": 1, "truly": 1, "lstmd": 1, "outperform": 1, "lstm": 1, "r": 1, "small": 1, "may": 1, "also": 1, "underestimate": 1, "bias": 1}, {"however": 1, "feasible": 1, "estimate": 1, "reward": 1, "generation": 1, "fig": 1, "2bcd": 1, "lstmd": 1, "irecgan": 1, "outperform": 1, "baselines": 1, "coverager": 1, "help": 1, "generate": 1, "sample": 1, "via": 1, "adversarial": 1, "train": 1}, {"average": 1, "cumulative": 1, "reward": 1, "also": 1, "report": 1, "rightmost": 1, "bar": 1, "figure": 1, "2": 1}, {"calculate": 1, "generate": 1, "1000": 1, "sequence": 1, "environment": 1, "take": 1, "average": 1, "cumulative": 1, "reward": 1}, {"irecgan": 1, "larger": 1, "average": 1, "cumulative": 1, "reward": 1, "methods": 1, "configurations": 1, "except": 1, "random": 1, "10000": 1, "offline": 1, "sequence": 1}, {"random": 1, "10": 1, "000": 1, "irecgan": 1, "outperform": 1, "pg": 1, "lstmd": 1}, {"low": 1, "cumulative": 1, "reward": 2, "pg": 1, "random": 2, "indicate": 1, "transition": 1, "probabilities": 1, "condition": 1, "high": 1, "items": 1, "may": 1, "sufficiently": 1, "learn": 1, "offline": 1, "policy": 1}, {"online": 2, "learn": 2, "evaluate": 1, "model": 1, "effectiveness": 1, "practical": 1, "set": 1, "execute": 1, "offline": 1, "alternately": 1}, {"specifically": 1, "separate": 1, "learn": 2, "two": 1, "stag": 1, "first": 1, "agents": 1, "directly": 1, "interact": 1, "simulator": 1, "update": 1, "policies": 1, "allow": 1, "generate": 2, "200": 1, "sequence": 1, "stage": 2, "turn": 1, "offline": 2, "reuse": 1, "data": 1}, {"iterate": 1, "two": 1, "stag": 1, "record": 1, "performance": 1, "online": 1, "learn": 1, "stage": 1}, {"compare": 1, "follow": 1, "baselines": 1, "1": 1, "pgonline": 1, "online": 2, "learn": 4, "2": 1, "pgonlineoffline": 1, "reuse": 1, "generate": 1, "data": 1, "via": 1, "policy": 1, "gradient": 1, "offline": 2, "8": 1, "": 1, "3": 1, "lstmoffline": 1}, {"train": 1, "model": 1, "scratch": 1, "report": 1, "performance": 1, "coverage1": 1, "coverage10": 1, "20": 1, "iterations": 1, "figure": 1, "3": 1}, {"observe": 1, "lstmoffline": 1, "perform": 1, "worse": 1, "rl": 1, "methods": 1, "offline": 1, "learn": 1, "especially": 1, "later": 1, "stage": 1, "due": 1, "lack": 1, "exploration": 1}, {"pgonline": 1, "improve": 1, "slowly": 1, "reuse": 1, "generate": 1, "data": 1}, {"compare": 1, "pgonlineoffline": 1, "irecgan": 1, "better": 1, "convergence": 1, "coverage": 1, "reduce": 1, "value": 1, "estimation": 1, "bias": 1}, {"also": 1, "find": 1, "coverage10": 1, "harder": 1, "improve": 1}, {"key": 1, "reason": 1, "model": 1, "identify": 1, "items": 1, "high": 1, "reward": 1, "tend": 1, "recommend": 1, "often": 1}, {"give": 1, "less": 2, "relevant": 1, "items": 1, "chance": 1, "explore": 1, "similar": 1, "online": 1, "evaluation": 1, "experiment": 1, "max": 1, "mix": 1, "": 1}, {"modelbased": 1, "rl": 1, "train": 2, "alleviate": 2, "bias": 1, "certain": 1, "extent": 1, "generate": 1, "sequence": 1, "cannot": 1, "totally": 1}, {"remind": 1, "us": 1, "focus": 1, "exploreexploit": 1, "tradeoff": 1, "modelbased": 1, "rl": 1, "future": 1, "work": 1}, {"72": 1, "": 2, "realworld": 2, "data": 1, "offline": 2, "test": 1, "use": 1, "largescale": 1, "recommendation": 1, "dataset": 1, "cikm": 1, "cup": 1, "2016": 1, "evaluate": 1, "effectiveness": 1, "propose": 1, "solution": 1, "reranking": 1}, {"sessions": 1, "length": 1, "1": 1, "longer": 1, "40": 1, "items": 1, "never": 1, "click": 1, "filter": 1}, {"select": 2, "top": 1, "40000": 1, "popular": 1, "items": 1, "recommendation": 1, "candidate": 1, "set": 1, "randomly": 1, "6528417181720": 1, "sessions": 1, "trainingvalidationtesting": 1}, {"average": 1, "length": 1, "sessions": 1, "281280277": 1, "respectively": 1, "ratio": 1, "click": 1, "lead": 1, "purchase": 1, "231246245": 1}, {"follow": 1, "model": 1, "set": 1, "simulationbased": 1, "study": 1, "experiment": 1}, {"understand": 1, "effect": 1, "different": 2, "data": 2, "separation": 2, "strategies": 2, "rl": 1, "model": 1, "train": 1, "test": 1, "also": 1, "provide": 1, "comparison": 1, "performances": 1, "appendix": 1}, {"baselines": 2, "addition": 1, "compare": 1, "simulationbased": 1, "study": 1, "also": 1, "include": 1, "follow": 1, "stateoftheart": 1, "solutions": 1, "recommendation": 1, "1": 1}, {"pgis": 1, "agent": 1, "model": 1, "estimate": 1, "importance": 1, "sample": 1, "offline": 1, "data": 1, "reduce": 1, "bias": 1, "2": 1}, {"ac": 1, "lstm": 1, "model": 2, "whose": 1, "set": 1, "agent": 1, "train": 1, "actorcritic": 1, "algorithm": 1, "16": 1, "reduce": 1, "variance": 1, "3": 1}, {"pgu": 1, "agent": 1, "model": 1, "train": 2, "use": 1, "offline": 1, "generate": 1, "data": 1, "without": 1, "adversarial": 1, "4": 1}, {"acu": 1, "ac": 1, "model": 1, "train": 2, "offline": 1, "generate": 1, "data": 1, "without": 1, "adversarial": 1}, {"evaluation": 1, "metrics": 1, "model": 1, "apply": 1, "rerank": 1, "give": 1, "recommendation": 1, "list": 1, "step": 1, "test": 1, "sessions": 1, "offline": 1, "data": 1}, {"use": 1, "precisionk": 1, "p1": 1, "p10": 1, "compare": 1, "different": 1, "model": 1, "recommendation": 1, "performance": 1, "define": 1, "click": 1, "items": 1, "relevant": 1}, {"log": 2, "recommendation": 1, "list": 1, "order": 1, "cannot": 1, "assess": 1, "policys": 1, "performance": 1}, {"table": 1, "1": 1, "rerank": 1, "evaluation": 1, "realworld": 1, "dataset": 1, "random": 1, "split": 1}, {"model": 1, "p10": 1, "": 11, "p1": 1, "lstm": 1, "3289050": 1, "820065": 1, "lstmd": 1, "3342040": 1, "855063": 1, "pg": 1, "3328071": 1, "625014": 1, "pgis": 1, "2813045": 1, "461073": 1, "ac": 1, "3193017": 1, "654019": 1, "pgu": 1, "3412052": 1, "644056": 1, "acu": 1, "3243022": 1, "663": 1, "029": 1, "irecgan": 1, "3506048": 1, "679044": 1, "result": 2, "offline": 1, "rerank": 1, "evaluation": 1, "report": 1, "table": 1, "1": 1}, {"help": 1, "adversarial": 1, "train": 1, "irecgan": 1, "achieve": 1, "encourage": 1, "p10": 1, "improvement": 1, "baselines": 1}, {"verify": 1, "effectiveness": 1, "modelbased": 1, "reinforcement": 1, "learn": 1, "especially": 1, "adversarial": 1, "train": 1, "strategy": 1, "utilize": 1, "offline": 1, "data": 1, "reduce": 1, "bias": 1}, {"specifically": 1, "pgis": 1, "perform": 1, "well": 1, "pg": 1, "partially": 1, "high": 1, "variance": 1, "introduce": 1, "importance": 1, "sample": 1}, {"pgu": 1, "able": 1, "fit": 1, "give": 1, "data": 3, "accurately": 1, "pg": 1, "learn": 1, "generate": 1, "since": 1, "many": 1, "items": 1, "recommendation": 1, "collect": 1, "limit": 1}, {"however": 1, "pgu": 1, "perform": 1, "worse": 1, "irecgan": 1, "bias": 1, "user": 1, "behavior": 1, "model": 1}, {"help": 1, "discriminator": 1, "irecgan": 1, "reduce": 1, "bias": 1, "user": 1, "behavior": 1, "model": 1, "improve": 1, "value": 1, "estimation": 1, "policy": 1, "learn": 1}, {"also": 1, "reflect": 1, "improve": 1, "user": 2, "behavior": 2, "model": 2, "lstmd": 1, "outperform": 1, "lstm": 1, "give": 1}, {"8": 1, "": 2, "conclusion": 1, "work": 1, "develop": 1, "practical": 1, "solution": 2, "utilize": 1, "offline": 1, "data": 1, "build": 1, "modelbased": 1, "reinforcement": 1, "learn": 1, "recommendation": 1}, {"introduce": 1, "adversarial": 1, "train": 1, "joint": 1, "user": 1, "behavior": 1, "model": 1, "learn": 1, "policy": 1, "update": 1}, {"theoretical": 1, "analysis": 1, "show": 1, "solutions": 1, "promise": 1, "reduce": 1, "bias": 1, "empirical": 1, "evaluations": 1, "synthetic": 1, "realworld": 1, "recommendation": 1, "datasets": 1, "verify": 1, "effectiveness": 1, "solution": 1}, {"several": 1, "directions": 1, "leave": 1, "open": 1, "work": 1, "include": 1, "balance": 1, "exploreexploit": 1, "policy": 1, "learn": 2, "offline": 1, "data": 1, "incorporate": 1, "richer": 1, "structure": 1, "user": 1, "behavior": 1, "model": 1, "explore": 1, "applicability": 1, "solution": 1, "offpolicy": 1, "scenarios": 1, "conversational": 1, "systems": 1}, {"9": 1, "": 1, "reference": 1, "1": 1, "joshua": 1, "achiam": 1, "david": 1, "hold": 1, "aviv": 1, "tamar": 1, "pieter": 1, "abbeel": 1}, {"constrain": 1, "policy": 1, "optimization": 1}, {"proceed": 1, "34th": 1, "international": 1, "conference": 1, "machine": 1, "learningvolume": 1, "70": 1, "page": 1, "2231": 1}, {"jmlr": 1}, {"org": 1, "2017": 1}, {"2": 1, "minmin": 1, "chen": 1, "alex": 1, "beutel": 1, "paul": 1, "covington": 1, "sagar": 1, "jain": 1, "francois": 1, "belletti": 1, "ed": 1, "h": 1, "chi": 1}, {"topk": 1, "offpolicy": 1, "correction": 1, "reinforce": 1, "recommender": 1, "system": 1}, {"proceed": 1, "twelfth": 1, "acm": 1, "international": 1, "conference": 1, "web": 1, "search": 1, "data": 1, "mine": 1, "page": 1, "456464": 1}, {"acm": 1, "2019": 1}, {"3": 1, "xinshi": 1, "chen": 1, "shuang": 1, "li": 2, "hui": 1, "shaohua": 1, "jiang": 1, "yuan": 1, "qi": 1, "le": 1, "song": 1}, {"generative": 1, "adversarial": 1, "user": 1, "model": 1, "reinforcement": 1, "learn": 1, "base": 1, "recommendation": 1, "system": 1}, {"proceed": 1, "36th": 1, "international": 1, "conference": 1, "machine": 1, "learn": 1, "volume": 1, "97": 1, "page": 1, "10521061": 1, "2019": 1}, {"4": 1, "junyoung": 1, "chung": 1, "caglar": 1, "gulcehre": 1, "kyunghyun": 1, "cho": 1, "yoshua": 1, "bengio": 1}, {"empirical": 1, "evaluation": 1, "gate": 1, "recurrent": 1, "neural": 1, "network": 1, "sequence": 1, "model": 1}, {"arxiv": 1, "preprint": 1, "arxiv14123555": 1, "2014": 1}, {"5": 1, "marc": 1, "deisenroth": 1, "carl": 1, "e": 1, "rasmussen": 1}, {"pilco": 1, "modelbased": 1, "dataefficient": 1, "approach": 1, "policy": 1, "search": 1}, {"proceed": 1, "28th": 1, "international": 1, "conference": 1, "machine": 1, "learn": 1, "icml11": 1, "page": 1, "465472": 1, "2011": 1}, {"6": 1, "marc": 1, "peter": 1, "deisenroth": 1, "carl": 1, "edward": 1, "rasmussen": 1, "dieter": 1, "fox": 1}, {"learn": 2, "control": 1, "lowcost": 1, "manipulator": 1, "use": 1, "dataefficient": 1, "reinforcement": 1}, {"2011": 1}, {"7": 1, "marc": 1, "peter": 1, "deisenroth": 1, "gerhard": 1, "neumann": 1, "jan": 1, "peters": 1, "et": 1, "al": 1}, {"survey": 1, "policy": 1, "search": 1, "robotics": 1}, {"foundations": 1, "trend": 1, "r": 1, "robotics": 1, "2121142": 1, "2013": 1}, {"8": 1, "alexandre": 2, "gilotte": 1, "clment": 1, "calauznes": 1, "thomas": 1, "nedelec": 1, "abraham": 1, "simon": 1, "doll": 1}, {"offline": 1, "ab": 1, "test": 1, "recommender": 1, "systems": 1}, {"proceed": 1, "eleventh": 1, "acm": 1, "international": 1, "conference": 1, "web": 1, "search": 1, "data": 1, "mine": 1, "page": 1, "198206": 1}, {"acm": 1, "2018": 1}, {"9": 1, "ian": 1, "goodfellow": 1, "jean": 1, "pougetabadie": 1, "mehdi": 1, "mirza": 1, "bing": 1, "xu": 1, "david": 1, "wardefarley": 1, "sherjil": 1, "ozair": 1, "aaron": 1, "courville": 1, "yoshua": 1, "bengio": 1}, {"generative": 1, "adversarial": 1, "net": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "page": 1, "26722680": 1, "2014": 1}, {"10": 1, "shixiang": 1, "gu": 1, "timothy": 1, "lillicrap": 1, "ilya": 1, "sutskever": 1, "sergey": 1, "levine": 1}, {"continuous": 1, "deep": 1, "qlearning": 1, "modelbased": 1, "acceleration": 1}, {"international": 1, "conference": 1, "machine": 1, "learn": 1, "page": 1, "28292838": 1, "2016": 1}, {"11": 1, "xiangnan": 1, "hanwang": 1, "zhang": 1, "minyen": 1, "kan": 1, "tatseng": 1, "chua": 1}, {"fast": 1, "matrix": 1, "factorization": 1, "online": 1, "recommendation": 1, "implicit": 1, "feedback": 1}, {"proceed": 1, "39th": 1, "international": 1, "acm": 1, "sigir": 1, "conference": 1, "research": 1, "development": 1, "information": 1, "retrieval": 1, "page": 1, "549558": 1}, {"acm": 1, "2016": 1}, {"12": 1, "sepp": 1, "hochreiter": 1, "jrgen": 1, "schmidhuber": 1}, {"long": 1, "shortterm": 1, "memory": 1}, {"neural": 1, "computation": 1, "98": 1, "17351780": 1, "1997": 1}, {"13": 1, "yehuda": 1, "koren": 1, "robert": 1, "bell": 1, "chris": 1, "volinsky": 1}, {"matrix": 1, "factorization": 1, "techniques": 1, "recommender": 1, "systems": 1}, {"computer": 1, "83037": 1, "2009": 1}, {"14": 1, "reinforcement": 1, "learn": 1}, {"introduction": 1, "richard": 1, "sutton": 1, "andrew": 1, "g": 1, "barto": 1, "1998": 1}, {"15": 1, "elad": 1, "liebman": 1, "maytal": 1, "saartsechansky": 1, "peter": 1, "stone": 1}, {"djmc": 1, "reinforcementlearning": 1, "agent": 1, "music": 1, "playlist": 1, "recommendation": 1}, {"proceed": 1, "2015": 1, "international": 1, "conference": 1, "autonomous": 1, "agents": 1, "multiagent": 1, "systems": 1, "page": 1, "591599": 1}, {"international": 1, "foundation": 1, "autonomous": 1, "agents": 1, "multiagent": 1, "systems": 1, "2015": 1}, {"16": 1, "timothy": 1, "p": 1, "lillicrap": 1, "jonathan": 1, "j": 1, "hunt": 1, "alexander": 1, "pritzel": 1, "nicolas": 1, "heess": 1, "tom": 1, "erez": 1, "yuval": 1, "tassa": 1, "david": 1, "silver": 1, "daan": 1, "wierstra": 1}, {"continuous": 1, "control": 1, "deep": 1, "reinforcement": 1, "learn": 1}, {"arxiv": 1, "preprint": 1, "arxiv150902971": 1, "2015": 1}, {"17": 1, "zhongqi": 1, "lu": 1, "qiang": 1, "yang": 1}, {"partially": 1, "observable": 1, "markov": 1, "decision": 1, "process": 1, "recommender": 1, "systems": 1}, {"arxiv": 1, "preprint": 1, "arxiv160807793": 1, "2016": 1}, {"10": 1, "": 1, "18": 1, "david": 1, "meger": 1, "juan": 1, "camilo": 1, "gamboa": 1, "higuera": 1, "anqi": 1, "xu": 1, "philippe": 1, "giguere": 1, "gregory": 1, "dudek": 1}, {"learn": 1, "legged": 1, "swim": 1, "gaits": 1, "experience": 1}, {"2015": 1, "ieee": 1, "international": 1, "conference": 1, "robotics": 1, "automation": 1, "icra": 1, "page": 1, "23322338": 1}, {"ieee": 1, "2015": 1}, {"19": 1, "volodymyr": 1, "mnih": 1, "koray": 1, "kavukcuoglu": 1, "david": 1, "silver": 1, "alex": 1, "grave": 1, "ioannis": 1, "antonoglou": 1, "daan": 1, "wierstra": 1, "martin": 1, "riedmiller": 1}, {"play": 1, "atari": 1, "deep": 1, "reinforcement": 1, "learn": 1}, {"arxiv": 1, "preprint": 1, "arxiv13125602": 1, "2013": 1}, {"20": 1, "volodymyr": 1, "mnih": 1, "koray": 1, "kavukcuoglu": 1, "david": 1, "silver": 1, "andrei": 1, "rusu": 1, "joel": 1, "veness": 1, "marc": 1, "g": 1, "bellemare": 1, "alex": 1, "grave": 1, "martin": 1, "riedmiller": 1, "andreas": 1, "k": 1, "fidjeland": 1, "georg": 1, "ostrovski": 1, "et": 1, "al": 1}, {"humanlevel": 1, "control": 1, "deep": 1, "reinforcement": 1, "learn": 1}, {"nature": 1, "5187540529": 1, "2015": 1}, {"21": 1, "jun": 1, "morimoto": 1, "christopher": 1, "g": 1, "atkeson": 1}, {"minimax": 1, "differential": 1, "dynamic": 1, "program": 1, "application": 1, "robust": 1, "biped": 1, "walk": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "page": 1, "15631570": 1, "2003": 1}, {"22": 1, "rmi": 1, "munos": 1, "tom": 1, "stepleton": 1, "anna": 1, "harutyunyan": 1, "marc": 1, "bellemare": 1}, {"safe": 1, "efficient": 1, "offpolicy": 1, "reinforcement": 1, "learn": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "page": 1, "10541062": 1, "2016": 1}, {"23": 1, "junhyuk": 1, "oh": 1, "satinder": 1, "singh": 1, "honglak": 1, "lee": 1}, {"value": 1, "prediction": 1, "network": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "page": 1, "61186128": 1, "2017": 1}, {"24": 1, "baolin": 1, "peng": 1, "xiujun": 1, "li": 1, "jianfeng": 1, "gao": 1, "jingjing": 1, "liu": 1, "kamfai": 1, "wong": 1, "shangyu": 1, "su": 1}, {"deep": 1, "dynaq": 1, "integrate": 1, "plan": 1, "taskcompletion": 1, "dialogue": 1, "policy": 1, "learn": 1}, {"arxiv": 1, "preprint": 1, "arxiv180106176": 1, "2018": 1}, {"25": 1, "doina": 1, "precup": 1}, {"eligibility": 1, "trace": 1, "offpolicy": 1, "policy": 1, "evaluation": 1}, {"computer": 1, "science": 1, "department": 1, "faculty": 1, "publication": 1, "series": 1, "page": 1, "80": 1, "2000": 1}, {"26": 1, "doina": 1, "precup": 1, "richard": 1, "sutton": 1, "sanjoy": 1, "dasgupta": 1}, {"offpolicy": 1, "temporaldifference": 1, "learn": 1, "function": 1, "approximation": 1}, {"icml": 1, "page": 1, "417424": 1, "2001": 1}, {"27": 1, "john": 1, "schulman": 1, "sergey": 1, "levine": 1, "pieter": 1, "abbeel": 1, "michael": 1, "jordan": 1, "philipp": 1, "moritz": 1}, {"trust": 1, "region": 1, "policy": 1, "optimization": 1}, {"international": 1, "conference": 1, "machine": 1, "learn": 1, "page": 1, "18891897": 1, "2015": 1}, {"28": 1, "guy": 1, "shani": 1, "david": 1, "heckerman": 1, "ronen": 1, "brafman": 1}, {"mdpbased": 1, "recommender": 1, "system": 1}, {"journal": 1, "machine": 1, "learn": 1, "research": 1, "6sep12651295": 1, "2005": 1}, {"29": 1, "richard": 1, "sutton": 1}, {"integrate": 1, "architectures": 1, "learn": 1, "plan": 1, "react": 1, "base": 1, "approximate": 1, "dynamic": 1, "program": 1}, {"machine": 1, "learn": 1, "proceed": 1, "1990": 1, "page": 1, "216224": 1}, {"elsevier": 1, "1990": 1}, {"30": 1, "richard": 1, "sutton": 1, "david": 1, "mcallester": 1, "satinder": 1, "p": 1, "singh": 1, "yishay": 1, "mansour": 1}, {"policy": 1, "gradient": 1, "methods": 1, "reinforcement": 1, "learn": 1, "function": 1, "approximation": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "page": 1, "10571063": 1, "2000": 1}, {"31": 1, "adith": 1, "swaminathan": 1, "thorsten": 1, "joachims": 1}, {"batch": 1, "learn": 1, "log": 1, "bandit": 1, "feedback": 1, "counterfactual": 1, "risk": 1, "minimization": 1}, {"journal": 1, "machine": 1, "learn": 1, "research": 1, "161": 1, "17311755": 1, "2015": 1}, {"32": 1, "adith": 1, "swaminathan": 1, "thorsten": 1, "joachims": 1}, {"selfnormalized": 1, "estimator": 1, "counterfactual": 1, "learn": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "page": 1, "32313239": 1, "2015": 1}, {"33": 1, "philip": 1, "thomas": 1, "emma": 1, "brunskill": 1}, {"dataefficient": 1, "offpolicy": 1, "policy": 1, "evaluation": 1, "reinforcement": 1, "learn": 1}, {"international": 1, "conference": 1, "machine": 1, "learn": 1, "page": 1, "21392148": 1, "2016": 1}, {"34": 1, "ronald": 1, "j": 1, "williams": 1}, {"simple": 1, "statistical": 1, "gradientfollowing": 1, "algorithms": 1, "connectionist": 1, "reinforcement": 1, "learn": 1}, {"machine": 1, "learn": 1, "834229256": 1, "1992": 1}, {"35": 1, "qingyun": 1, "wu": 1, "hongning": 1, "wang": 1, "liangjie": 1, "hong": 1, "yue": 1, "shi": 1}, {"return": 1, "believe": 1, "optimize": 1, "longterm": 1, "user": 1, "engagement": 1, "recommender": 1, "systems": 1}, {"proceed": 1, "2017": 1, "acm": 1, "conference": 1, "information": 1, "knowledge": 1, "management": 1, "page": 1, "19271936": 1}, {"acm": 1, "2017": 1}, {"11": 1, "": 1, "36": 1, "lantao": 1, "yu": 2, "weinan": 1, "zhang": 1, "jun": 1, "wang": 1, "yong": 1}, {"seqgan": 1, "sequence": 1, "generative": 1, "adversarial": 1, "net": 1, "policy": 1, "gradient": 1}, {"thirtyfirst": 1, "aaai": 1, "conference": 1, "artificial": 1, "intelligence": 1, "2017": 1}, {"37": 1, "xiangyu": 1, "zhao": 2, "long": 1, "xia": 1, "yihong": 1, "dawei": 1, "yin": 1, "jiliang": 1, "tang": 1}, {"modelbased": 1, "reinforcement": 1, "learn": 1, "wholechain": 1, "recommendations": 1}, {"arxiv": 1, "preprint": 1, "arxiv190203987": 1, "2019": 1}, {"38": 1, "guanjie": 1, "zheng": 2, "fuzheng": 1, "zhang": 1, "zihan": 1, "yang": 1, "xiang": 1, "nicholas": 1, "jing": 1, "yuan": 1, "xing": 1, "xie": 1, "zhenhui": 1, "li": 1}, {"drn": 1, "deep": 1, "reinforcement": 1, "learn": 1, "framework": 1, "news": 1, "recommendation": 1}, {"proceed": 1, "2018": 1, "world": 2, "wide": 2, "web": 2, "conference": 1, "page": 1, "167176": 1}, {"international": 1, "world": 1, "wide": 1, "web": 1, "conferences": 1, "steer": 1, "committee": 1, "2018": 1}, {"12": 1}]
[{"guide": 1, "metapolicy": 1, "search": 1, "": 2, "russell": 1, "mendonca": 1, "abhishek": 1, "gupta": 1, "rosen": 1, "kralev": 1, "pieter": 1, "abbeel": 1, "sergey": 1, "levine": 1, "chelsea": 1, "finn": 1, "department": 1, "electrical": 1, "engineer": 1, "computer": 1, "science": 1, "university": 1, "california": 1, "berkeley": 1, "russellm": 1, "cbfinnberkeleyedu": 1, "abhigupta": 1, "pabbeel": 1, "svlevineeecsberkeleyedu": 1, "rdkralevgmailcom": 1, "abstract": 1, "reinforcement": 1, "learn": 2, "rl": 1, "algorithms": 1, "demonstrate": 1, "promise": 1, "result": 1, "complex": 1, "task": 1, "yet": 1, "often": 1, "require": 1, "impractical": 1, "number": 1, "sample": 1, "since": 1, "scratch": 1}, {"metarl": 1, "aim": 1, "address": 1, "challenge": 1, "leverage": 1, "experience": 1, "previous": 1, "task": 2, "quickly": 1, "solve": 1, "new": 1}, {"however": 1, "practice": 1, "algorithms": 1, "generally": 1, "also": 1, "require": 1, "large": 1, "amount": 1, "onpolicy": 1, "experience": 1, "metatraining": 1, "process": 1, "make": 1, "impractical": 1, "use": 1, "many": 1, "problems": 1}, {"end": 1, "propose": 1, "learn": 2, "reinforcement": 1, "procedure": 1, "federate": 1, "way": 1, "individual": 2, "offpolicy": 1, "learners": 1, "solve": 1, "metatraining": 1, "task": 1, "consolidate": 1, "solutions": 1, "single": 1, "metalearner": 1}, {"since": 1, "central": 1, "metalearner": 1, "learn": 1, "imitate": 1, "solutions": 1, "individual": 1, "task": 2, "accommodate": 1, "either": 1, "standard": 1, "metarl": 1, "problem": 1, "set": 2, "hybrid": 1, "provide": 1, "example": 1, "demonstrations": 1}, {"former": 1, "result": 1, "approach": 1, "leverage": 1, "policies": 1, "learn": 1, "previous": 1, "task": 1, "without": 1, "significant": 1, "amount": 1, "onpolicy": 1, "data": 1, "metatraining": 1, "whereas": 1, "latter": 1, "particularly": 1, "useful": 1, "case": 1, "demonstrations": 1, "easy": 1, "person": 1, "provide": 1}, {"across": 1, "number": 1, "continuous": 1, "control": 1, "metarl": 2, "problems": 1, "demonstrate": 1, "significant": 1, "improvements": 1, "sample": 1, "efficiency": 1, "comparison": 1, "prior": 1, "work": 1, "well": 1, "ability": 1, "scale": 1, "domains": 1, "visual": 1, "observations": 1}, {"1": 1, "": 2, "introduction": 1, "metalearning": 1, "promise": 1, "approach": 1, "use": 1, "previous": 1, "experience": 1, "across": 1, "breadth": 1, "task": 2, "significantly": 1, "accelerate": 1, "learn": 1, "new": 1}, {"metareinforcement": 1, "learn": 2, "consider": 1, "problem": 1, "specifically": 1, "context": 1, "new": 1, "behaviors": 1, "trial": 1, "error": 1, "interactions": 1, "environment": 1, "build": 1, "previous": 1, "experience": 1}, {"build": 2, "effective": 1, "metarl": 1, "algorithms": 1, "critical": 1, "towards": 1, "agents": 1, "flexible": 1, "agent": 1, "able": 1, "manipulate": 1, "new": 3, "object": 2, "ways": 1, "without": 1, "learn": 1, "scratch": 1, "goal": 1}, {"able": 1, "reuse": 1, "prior": 1, "experience": 1, "way": 1, "arguably": 1, "fundamental": 1, "aspect": 1, "intelligence": 1}, {"enable": 1, "agents": 1, "adapt": 1, "via": 1, "metarl": 1, "particularly": 1, "useful": 1, "acquire": 1, "behaviors": 1, "realworld": 1, "situations": 1, "diverse": 1, "dynamic": 1, "environments": 1}, {"however": 1, "despite": 1, "recent": 1, "advance": 1, "7": 2, "8": 2, "17": 1, "current": 1, "metarl": 1, "methods": 1, "often": 1, "limit": 1, "simpler": 1, "domains": 1, "relatively": 1, "lowdimensional": 1, "continuous": 1, "control": 1, "task": 1, "44": 1, "navigation": 1, "discrete": 1, "action": 1, "command": 1, "24": 1}, {"optimization": 1, "stability": 1, "sample": 1, "complexity": 1, "major": 1, "challenge": 1, "metatraining": 1, "phase": 1, "methods": 1, "recent": 1, "techniques": 1, "require": 2, "upto": 1, "250": 1, "million": 1, "transition": 1, "metalearning": 1, "tabular": 1, "mdps": 1, "7": 1, "typically": 1, "fraction": 1, "second": 1, "solve": 1, "isolation": 1}, {"make": 1, "follow": 1, "observation": 1, "work": 1, "goal": 1, "metareinforcement": 1, "learn": 3, "acquire": 2, "fast": 1, "efficient": 1, "reinforcement": 2, "procedures": 2, "need": 1, "directly": 1}, {"instead": 1, "use": 1, "significantly": 1, "stable": 1, "efficient": 1, "algorithm": 1, "provide": 1, "supervision": 1, "metalevel": 1}, {"work": 1, "show": 1, "practical": 1, "choice": 1, "use": 1, "supervise": 1, "imitation": 1, "learn": 1}, {"metareinforcement": 1, "learn": 1, "algorithm": 1, "33rd": 1, "conference": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "neurips": 1, "2019": 1, "vancouver": 1, "canada": 1}, {"receive": 1, "direct": 1, "supervision": 1, "metatraining": 1, "form": 1, "expert": 1, "action": 1, "still": 1, "optimize": 1, "ability": 1, "quickly": 1, "learn": 1, "task": 1, "via": 1, "reinforcement": 1}, {"crucially": 1, "expert": 1, "policies": 1, "produce": 1, "automatically": 1, "standard": 1, "reinforcement": 1, "learn": 1, "methods": 1, "additional": 1, "assumptions": 1, "supervision": 1, "actually": 1, "need": 1}, {"also": 1, "acquire": 1, "use": 2, "efficient": 1, "offpolicy": 1, "reinforcement": 1, "learn": 2, "algorithms": 1, "otherwise": 1, "challenge": 1, "metareinforcement": 1}, {"available": 1, "incorporate": 1, "humanprovided": 1, "demonstrations": 2, "enable": 1, "even": 1, "efficient": 1, "metatraining": 1, "particularly": 1, "domains": 1, "easy": 1, "collect": 1}, {"metatest": 1, "time": 1, "face": 1, "new": 2, "task": 1, "method": 1, "solve": 1, "problem": 1, "conventional": 1, "metareinforcement": 1, "learn": 1, "acquire": 1, "skill": 1, "use": 1, "reward": 1, "signal": 1}, {"main": 1, "contribution": 1, "metarl": 1, "method": 1, "learn": 2, "fast": 1, "reinforcement": 1, "via": 1, "supervise": 1, "imitation": 1}, {"optimize": 1, "set": 1, "parameters": 1, "one": 1, "gradient": 1, "step": 1, "lead": 1, "policy": 1, "match": 1, "experts": 1, "action": 1}, {"since": 1, "supervise": 1, "imitation": 1, "stable": 1, "efficient": 1, "approach": 1, "gracefully": 1, "scale": 1, "visual": 1, "control": 1, "domains": 1, "highdimensional": 1, "convolutional": 1, "network": 1}, {"use": 1, "demonstrations": 1, "metatraining": 1, "less": 1, "challenge": 1, "exploration": 1, "metaoptimization": 1, "make": 1, "possible": 1, "effectively": 1, "learn": 2, "sparse": 1, "reward": 1, "environments": 1}, {"combination": 2, "imitation": 2, "rl": 2, "explore": 1, "30": 1, "20": 1, "metalearning": 1, "context": 1, "consider": 1, "previously": 1}, {"show": 1, "experiment": 1, "combination": 1, "fact": 1, "extremely": 1, "powerful": 1, "compare": 1, "metarl": 2, "method": 1, "metalearn": 1, "comparable": 1, "adaptation": 1, "skills": 1, "10x": 1, "fewer": 1, "interaction": 1, "episodes": 1, "make": 1, "much": 1, "viable": 1, "realworld": 1, "learn": 1}, {"experiment": 1, "also": 1, "show": 1, "method": 1, "adapt": 2, "convolutional": 1, "neural": 1, "network": 1, "policies": 2, "new": 1, "goals": 1, "trialanderror": 1, "gradient": 1, "descent": 1, "step": 1, "sparsereward": 1, "manipulation": 1, "task": 1, "handful": 1, "trials": 1}, {"believe": 1, "significant": 1, "step": 1, "towards": 1, "make": 1, "metarl": 1, "practical": 1, "use": 1, "complex": 1, "realworld": 1, "environments": 1}, {"2": 1, "": 2, "relate": 1, "work": 3, "build": 1, "upon": 1, "prior": 1, "metalearning": 1, "39": 1, "1": 1, "47": 1, "goal": 1, "learn": 2, "efficiently": 1}, {"focus": 1, "particular": 1, "case": 1, "metareinforcement": 1, "learn": 1, "39": 1, "7": 1, "48": 1, "8": 1, "24": 1, "14": 1}, {"prior": 1, "methods": 1, "learn": 3, "reinforcement": 1, "learners": 1, "represent": 1, "recurrent": 1, "recursive": 1, "neural": 1, "network": 1, "7": 1, "48": 1, "24": 1, "41": 1, "33": 1, "use": 3, "gradient": 1, "descent": 1, "initialization": 1, "8": 1, "14": 1, "36": 1, "critic": 1, "provide": 1, "gradients": 1, "policy": 1, "44": 1, "17": 1, "planner": 1, "adaptable": 1, "model": 1, "5": 1, "38": 1}, {"contrast": 1, "approach": 1, "aim": 1, "leverage": 1, "supervise": 1, "learn": 1, "metaoptimization": 1, "rather": 1, "rely": 1, "highvariance": 1, "algorithms": 1, "policy": 1, "gradient": 1}, {"decouple": 1, "problem": 2, "obtain": 1, "expert": 1, "trajectories": 1, "task": 1, "learn": 1, "fast": 1, "rl": 1, "procedure": 1}, {"allow": 1, "us": 1, "obtain": 1, "expert": 1, "trajectories": 1, "use": 1, "efficient": 1, "offpolicy": 1, "rl": 1, "algorithms": 1}, {"recent": 1, "work": 1, "use": 1, "amortize": 1, "probabilistic": 1, "inference": 1, "34": 1, "achieve": 1, "dataefficient": 1, "metatraining": 1, "however": 1, "contextual": 1, "methods": 1, "cannot": 1, "continually": 1, "adapt": 1, "distribution": 1, "test": 1, "task": 1}, {"ability": 1, "method": 1, "utilize": 1, "example": 1, "demonstrations": 1, "available": 1, "enable": 1, "much": 1, "better": 1, "performance": 1, "challenge": 1, "sparse": 1, "reward": 1, "task": 1}, {"approach": 1, "also": 1, "relate": 1, "fewshot": 1, "imitation": 1, "6": 1, "11": 1, "leverage": 1, "supervise": 1, "learn": 1, "metaoptimization": 1}, {"however": 1, "contrast": 1, "methods": 1, "lean": 1, "automatic": 1, "reinforcement": 1, "learner": 1, "learn": 1, "use": 1, "reward": 1, "require": 1, "demonstrations": 1, "new": 1, "task": 1}, {"algorithm": 1, "perform": 1, "metalearning": 1, "first": 1, "individually": 1, "solve": 1, "task": 1, "local": 1, "learners": 1, "consolidate": 1, "central": 1, "metalearner": 1}, {"resemble": 1, "methods": 1, "like": 1, "guide": 1, "policy": 1, "search": 1, "also": 1, "use": 1, "local": 1, "learners": 1, "37": 1, "29": 1, "46": 1, "28": 1, "12": 1}, {"however": 1, "prior": 1, "methods": 1, "aim": 2, "learn": 1, "single": 2, "policy": 1, "solve": 1, "task": 3, "approach": 1, "instead": 1, "metalearn": 1, "learner": 1, "adapt": 2, "train": 2, "distribution": 1, "generalize": 1, "new": 1, "see": 1}, {"prior": 1, "methods": 1, "also": 1, "seek": 1, "use": 1, "demonstrations": 1, "make": 1, "standard": 1, "reinforcement": 1, "learn": 1, "efficient": 1, "singletask": 1, "set": 1, "30": 1, "20": 1, "21": 1, "45": 1, "4": 1, "42": 1, "16": 1, "43": 1, "32": 1, "26": 1, "19": 1, "40": 1}, {"methods": 1, "aim": 1, "learn": 1, "policy": 1, "demonstrations": 2, "reward": 1, "use": 1, "make": 1, "rl": 1, "problem": 1, "easier": 1}, {"approach": 1, "instead": 1, "aim": 1, "leverage": 1, "demonstrations": 2, "learn": 3, "efficiently": 1, "reinforcement": 1, "new": 2, "task": 2, "without": 1, "trialanderror": 1}, {"version": 1, "algorithm": 2, "data": 1, "aggregate": 1, "across": 1, "iterations": 1, "extension": 1, "dagger": 1, "35": 1, "metalearning": 1, "set": 1, "allow": 1, "us": 1, "provide": 1, "theoretical": 1, "guarantee": 1, "performance": 1}, {"2": 1, "": 2, "figure": 1, "1": 1, "overview": 1, "guide": 1, "metapolicy": 1, "search": 1, "algorithm": 1, "learn": 4, "policy": 1, "capable": 1, "fast": 1, "adaptation": 1, "new": 1, "task": 1, "via": 1, "reinforcement": 2, "use": 1, "inner": 1, "loop": 1, "optimization": 1, "supervise": 1, "metaoptimization": 1}, {"algorithm": 1, "either": 1, "train": 1, "pertask": 1, "experts": 1, "assume": 1, "provide": 1, "human": 1, "demonstrations": 1, "use": 1, "metaoptimization": 1}, {"importantly": 1, "face": 1, "new": 2, "task": 2, "simply": 1, "perform": 1, "standard": 1, "reinforcement": 1, "learn": 1, "via": 1, "policy": 2, "gradient": 1, "quickly": 1, "adapt": 1, "metatraining": 1}, {"3": 1, "": 2, "preliminaries": 1, "section": 1, "introduce": 1, "metarl": 1, "problem": 1, "overview": 1, "modelagnostic": 1, "metalearning": 1, "maml": 1, "8": 1, "build": 1, "work": 1}, {"assume": 1, "distribution": 1, "task": 3, "": 1, "p": 3, "metatraining": 2, "draw": 1, "metatesting": 1, "consist": 1, "learn": 2, "heldout": 1, "sample": 1, "trialanderror": 1, "leverage": 1}, {"formally": 1, "task": 1, "": 12, "rst": 2, "qs1": 2, "qst1": 2, "st": 2, "consist": 1, "reward": 1, "function": 1, "r": 1, "initial": 1, "state": 1, "distribution": 1, "unknown": 1, "dynamics": 1}, {"state": 1, "space": 2, "action": 1, "horizon": 1, "h": 1, "share": 1, "across": 1, "task": 1}, {"metalearning": 1, "methods": 1, "learn": 2, "use": 1, "experience": 1, "metatraining": 1, "task": 2, "evaluate": 1, "ability": 1, "new": 1, "metatest": 1}, {"maml": 1, "particular": 1, "perform": 1, "metalearning": 1, "optimize": 1, "deep": 1, "network": 1, "initial": 1, "parameter": 1, "set": 1, "one": 1, "step": 1, "gradient": 1, "descent": 1, "small": 1, "dataset": 1, "lead": 1, "good": 1, "generalization": 1}, {"metatraining": 1, "learn": 1, "parameters": 1, "finetuned": 1, "data": 1, "new": 1, "task": 1}, {"concretely": 1, "consider": 1, "supervise": 1, "learn": 1, "problem": 1, "loss": 1, "function": 1, "denote": 3, "l": 1, "": 1, "model": 1, "parameters": 1, "label": 1, "data": 1}, {"metatraining": 1, "task": 2, "sample": 1, "along": 1, "data": 1, "randomly": 1, "partition": 1, "two": 1, "set": 1, "dtr": 1, "dval": 1, "": 1}, {"maml": 1, "optimize": 1, "set": 1, "model": 1, "parameters": 1, "": 2, "one": 1, "gradient": 1, "step": 1, "dtr": 1, "produce": 1, "good": 1, "performance": 1, "dval": 1}, {"thus": 1, "use": 1, "denote": 1, "update": 1, "parameters": 1, "maml": 1, "objective": 1, "follow": 1, "x": 2, "min": 2, "l": 2, "": 7, "dttr": 1, "dtval": 2, "lt": 1}, {"": 7, "step": 1, "size": 1, "set": 1, "hyperparameter": 1, "learn": 1}, {"move": 1, "forward": 1, "refer": 1, "outer": 1, "objective": 1, "metaobjective": 1}, {"subsequently": 1, "metatest": 1, "time": 1, "k": 1, "examples": 1, "new": 2, "heldout": 1, "task": 2, "ttest": 2, "present": 1, "run": 1, "gradient": 1, "descent": 1, "start": 1, "": 6, "infer": 1, "model": 1, "parameters": 1, "l": 1, "dttrtest": 1}, {"maml": 1, "algorithm": 1, "also": 1, "apply": 1, "metareinforcement": 1, "learn": 1, "set": 1, "dataset": 1, "dti": 1, "consist": 1, "trajectories": 1, "form": 1, "s1": 1, "": 9, "a1": 1, "ah1": 1, "sh": 1, "inner": 1, "outer": 1, "loss": 1, "function": 1, "correspond": 1, "negative": 1, "expect": 1, "reward": 1, "h": 1, "x": 2, "1": 2, "ri": 1, "st": 1}, {"1": 1, "lrl": 1, "": 10, "dti": 3, "ri": 1, "st": 2, "est": 1, "qti": 1, "h": 1, "t1": 1, "policy": 1, "gradients": 1, "49": 1, "use": 1, "estimate": 1, "gradient": 1, "loss": 1, "function": 1}, {"thus": 1, "algorithm": 1, "proceed": 1, "follow": 1, "task": 1, "ti": 1, "": 5, "first": 1, "collect": 2, "sample": 2, "dttri": 2, "policy": 3, "compute": 1, "update": 3, "parameters": 3, "use": 1, "gradient": 2, "evaluate": 1, "new": 1, "dtvali": 1, "via": 1, "finally": 1, "initial": 1, "take": 1, "step": 1, "metaobjective": 1}, {"next": 1, "section": 1, "introduce": 1, "new": 1, "approach": 1, "metarl": 1, "incorporate": 1, "stable": 1, "metaoptimization": 1, "procedure": 1, "still": 1, "converge": 1, "solution": 1, "regularity": 1, "assumptions": 1, "naturally": 1, "leverage": 1, "demonstrations": 1, "policies": 1, "learn": 1, "previous": 1, "task": 1, "desire": 1}, {"4": 1, "": 2, "guide": 1, "metapolicy": 1, "search": 1, "exist": 1, "metarl": 1, "algorithms": 1, "generally": 1, "perform": 1, "metalearning": 1, "scratch": 1, "onpolicy": 1, "methods": 1}, {"typically": 1, "require": 1, "large": 1, "number": 1, "sample": 1, "metatraining": 1}, {"instead": 1, "formulate": 1, "3": 1, "": 1, "metatraining": 2, "datadriven": 1, "process": 1, "agent": 1, "previously": 1, "learn": 2, "variety": 1, "task": 2, "standard": 1, "multitask": 1, "reinforcement": 1, "techniques": 1, "must": 1, "use": 1, "data": 1, "collect": 1}, {"use": 1, "experience": 1, "policies": 1, "meaningful": 1, "ways": 1, "metatraining": 1}, {"goal": 1, "develop": 1, "approach": 1, "use": 1, "previously": 1, "learn": 1, "skills": 1, "guide": 1, "metalearning": 1, "process": 1}, {"still": 1, "require": 2, "onpolicy": 1, "data": 1, "inner": 1, "loop": 1, "sample": 1, "considerably": 1, "less": 1, "would": 1, "need": 1, "without": 1, "use": 1, "prior": 1, "experience": 1}, {"surprisingly": 1, "show": 1, "experiment": 1, "separate": 1, "metatraining": 2, "two": 1, "phase": 4, "way": 1, "": 2, "individually": 1, "solve": 2, "task": 2, "second": 1, "use": 2, "metalearning": 1, "actually": 2, "require": 2, "less": 2, "total": 1, "experience": 2, "overall": 1, "individual": 1, "highlyefficient": 1, "offpolicy": 1, "reinforcement": 1, "learn": 1, "methods": 1, "take": 1, "together": 1, "single": 1, "metarl": 1, "train": 1}, {"also": 1, "improve": 1, "sample": 1, "efficiency": 1, "metatraining": 1, "even": 1, "incorporate": 1, "explicit": 1, "demonstrations": 1}, {"rest": 1, "section": 1, "describe": 1, "approach": 1, "analyze": 1, "theoretical": 1, "properties": 1, "discuss": 1, "practical": 1, "implementation": 1, "multiple": 1, "real": 1, "world": 1, "scenarios": 1}, {"41": 1, "guide": 1, "metapolicy": 1, "search": 1, "algorithm": 2, "first": 1, "phase": 1, "task": 2, "learn": 2, "policies": 1, "metatraining": 1}, {"policies": 1, "solve": 1, "metatraining": 1, "task": 2, "accelerate": 1, "learn": 1, "future": 1, "metatest": 1}, {"section": 1, "43": 1, "describe": 1, "policies": 1, "train": 1}, {"instead": 1, "learn": 2, "policies": 1, "explicitly": 1, "reinforcement": 1, "also": 1, "obtain": 1, "expert": 1, "demonstrations": 1, "human": 1, "demonstrator": 1, "use": 1, "equivalently": 1, "algorithm": 1}, {"second": 1, "phase": 1, "metalearning": 1, "learn": 2, "reinforcement": 1, "use": 1, "policies": 1, "supervision": 1, "metalevel": 1}, {"particular": 1, "train": 1, "set": 1, "initial": 1, "parameters": 1, "": 1, "one": 1, "step": 1, "gradient": 1, "descent": 1, "produce": 1, "policy": 1, "match": 1, "policies": 1, "learn": 1, "first": 1, "phase": 1}, {"denote": 1, "optimal": 1, "nearoptimal": 1, "policies": 1, "learn": 1, "tasklearning": 1, "phase": 1, "metatraining": 1, "task": 1, "ti": 1, "": 1}, {"refer": 1, "individual": 1, "policies": 1, "experts": 1, "first": 1, "phase": 1, "represent": 1, "optimal": 1, "nearoptimal": 1, "solutions": 1, "task": 1}, {"goal": 1, "metalearning": 1, "phase": 1, "optimize": 1, "metaobjective": 1, "maml": 1, "algorithm": 1, "lrl": 1, "": 2, "di": 1, "denote": 1, "parameters": 1, "policy": 1, "adapt": 1, "task": 1, "ti": 1, "via": 1, "gradient": 1, "descent": 1}, {"inner": 1, "policy": 1, "optimization": 1, "remain": 1, "policygradient": 1, "maml": 1, "algorithm": 1, "however": 1, "optimize": 1, "metaobjective": 1, "leverage": 1, "policies": 1, "learn": 1, "first": 1, "phase": 1}, {"particular": 1, "base": 1, "outer": 1, "objective": 1, "supervise": 1, "imitation": 1, "behavior": 1, "p": 1, "clone": 1, "bc": 1, "expert": 1, "action": 1}, {"behavioral": 1, "clone": 1, "loss": 1, "function": 1, "use": 1, "lbc": 1, "": 7, "di": 1, "st": 2, "log": 1}, {"gradients": 2, "supervise": 1, "learn": 2, "lower": 1, "variance": 1, "hence": 1, "stable": 1, "reinforcement": 1, "27": 1}, {"specific": 1, "implementation": 1, "second": 1, "phase": 1, "proceed": 1, "follow": 1, "first": 1, "roll": 1, "policies": 1, "collect": 1, "dataset": 1, "expert": 1, "trajectories": 1, "di": 1, "metatraining": 1, "task": 1, "ti": 1, "": 1}, {"use": 1, "initial": 1, "dataset": 1, "update": 1, "policy": 1, "accord": 1, "follow": 1, "metaobjective": 1, "x": 2, "": 10, "min": 1, "editr": 1, "lbc": 1, "lrl": 1, "ditr": 1, "dival": 1}, {"2": 1, "": 3, "ti": 1, "dival": 1, "di": 1, "discuss": 1, "objective": 1, "efficiently": 1, "optimize": 1, "section": 1, "43": 1}, {"result": 1, "optimization": 1, "set": 1, "initial": 1, "policy": 1, "parameters": 1, "": 2, "adapt": 1, "variety": 1, "task": 1, "produce": 1, "way": 1, "come": 1, "close": 1, "expert": 1, "policys": 1, "action": 1}, {"note": 1, "far": 1, "actually": 1, "require": 1, "query": 1, "expert": 2, "beyond": 1, "access": 1, "initial": 1, "rollouts": 1, "hence": 1, "first": 1, "step": 1, "method": 1, "applicable": 1, "problem": 1, "domains": 1, "demonstrations": 1, "available": 1, "place": 1, "learn": 1, "policies": 1}, {"however": 1, "policies": 1, "metatraining": 1, "task": 1, "continue": 1, "improve": 1}, {"particular": 1, "supervise": 1, "learn": 1, "provide": 1, "stable": 1, "lowvariance": 1, "gradients": 1, "behavior": 1, "clone": 1, "objectives": 1, "prone": 1, "compound": 1, "errors": 1}, {"single": 1, "task": 1, "imitation": 1, "learn": 2, "set": 1, "issue": 1, "address": 1, "collect": 1, "additional": 1, "data": 1, "policy": 2, "label": 1, "visit": 1, "state": 1, "optimal": 1, "action": 1, "expert": 1, "dagger": 1, "35": 1}, {"extend": 1, "idea": 1, "metalearning": 1, "set": 1, "alternate": 1, "data": 1, "aggregation": 1, "dataset": 1, "metapolicy": 1, "optimization": 1, "eq": 1}, {"2": 1}, {"data": 4, "aggregation": 1, "entail": 1, "1": 1, "adapt": 2, "current": 2, "policy": 1, "parameters": 1, "": 10, "metatraining": 2, "task": 2, "produce": 3, "2": 1, "roll": 1, "policies": 1, "state": 1, "st": 3, "3": 1, "query": 1, "experts": 1, "supervise": 2, "finally": 1, "4": 1, "aggregate": 1, "exist": 1, "algorithm": 1, "summarize": 1, "alg": 1}, {"1": 1, "analyze": 1, "section": 1, "42": 1}, {"provide": 1, "new": 1, "task": 1, "metatest": 1, "time": 1, "initialize": 1, "": 1, "run": 1, "policy": 1, "gradient": 1, "algorithm": 1}, {"4": 2, "": 7, "algorithm": 2, "1": 2, "gmps": 1, "guide": 1, "metapolicy": 1, "search": 1, "2": 2, "optimization": 1, "meta": 1, "objective": 1, "require": 1, "set": 1, "metatraining": 2, "task": 2, "ti": 2, "use": 1, "rl": 1, "acquire": 1, "initialize": 2, "di": 1, "rollouts": 1, "3": 1, "randomly": 1, "do": 1, "optimize": 1, "metaobjective": 1, "eq": 1}, {"2": 1, "wrt": 1}, {"": 1, "use": 1, "5": 1, "alg": 1}, {"2": 2, "aggregate": 3, "data": 2, "6": 1, "metatraining": 2, "task": 5, "ti": 6, "7": 1, "collect": 2, "ditr": 4, "k": 2, "rollouts": 3, "": 31, "8": 1, "compute": 1, "taskadapted": 1, "parameters": 2, "gradient": 1, "descent": 1, "lrl": 2, "result": 1, "9": 1, "st": 3, "10": 1, "di": 3, "11": 1, "end": 2, "12": 1, "require": 3, "set": 1, "dataset": 1, "initial": 1, "1": 1, "do": 1, "sample": 2, "minibatch": 1, "3": 1, "s1": 1, "a1": 1, "sh": 1, "4": 1, "init": 1, "5": 1, "n": 1, "1nbc": 1, "evaluate": 1, "accord": 1, "eq": 1}, {"3": 1, "6": 1, "importance": 1, "weight": 1, "": 16, "aat": 1, "st": 1, "init": 1, "7": 1, "compute": 1, "adapt": 1, "parameters": 1, "gradient": 1, "descent": 1, "lrl": 1, "ditr": 1, "8": 1, "sample": 1, "expert": 1, "trajectories": 1, "dival": 2, "di": 1, "9": 1, "update": 1, "lbc": 1}, {"10": 1, "end": 2, "11": 1, "": 1, "algorithm": 1, "call": 1, "guide": 1, "metapolicy": 1, "search": 1, "gmps": 1, "appeal": 1, "properties": 1, "arise": 1, "decompose": 1, "metalearning": 2, "problem": 1, "explicitly": 1, "task": 1, "learn": 1, "phase": 2}, {"decomposition": 1, "enable": 1, "use": 1, "previously": 1, "learn": 1, "policies": 1, "humanprovided": 1, "demonstrations": 1}, {"find": 1, "also": 1, "lead": 1, "increase": 1, "stability": 1, "train": 1}, {"lastly": 1, "decomposition": 1, "make": 1, "easy": 1, "leverage": 1, "privilege": 1, "information": 3, "may": 1, "available": 1, "metatraining": 1, "shape": 1, "reward": 1, "task": 1, "lowlevel": 1, "state": 1, "position": 1, "object": 1, "23": 1}, {"particular": 1, "privilege": 1, "information": 2, "provide": 1, "initial": 1, "policies": 1, "learn": 1, "hide": 1, "metapolicy": 2, "apply": 1, "test": 1, "settings": 1, "available": 1}, {"technique": 1, "make": 1, "straightforward": 1, "learn": 4, "visionbased": 1, "policies": 1, "example": 1, "bulk": 1, "do": 1, "without": 1, "vision": 1, "visual": 1, "feature": 1, "supervise": 1, "second": 1, "phase": 1}, {"method": 1, "also": 1, "inherit": 1, "appeal": 1, "properties": 1, "policy": 1, "gradient": 1, "maml": 1, "ability": 1, "continue": 1, "learn": 1, "experience": 1, "collect": 1, "contrast": 1, "recurrent": 1, "neural": 1, "network": 1, "cannot": 1, "easily": 1, "finetuned": 1, "new": 1, "task": 1}, {"42": 1, "": 2, "convergence": 1, "analysis": 1, "derive": 1, "metarl": 1, "algorithm": 3, "leverage": 1, "supervise": 1, "learn": 1, "increase": 1, "stability": 1, "natural": 1, "question": 1, "propose": 1, "converge": 1, "answer": 1, "original": 1, "less": 1, "stable": 1, "maml": 1}, {"prove": 1, "gmps": 1, "data": 1, "aggregation": 1, "describe": 1, "indeed": 1, "obtain": 1, "nearoptimal": 2, "cumulative": 1, "reward": 1, "supply": 1, "experts": 1}, {"proof": 1, "follow": 1, "similar": 1, "technique": 1, "prior": 1, "work": 1, "analyze": 1, "convergence": 1, "imitation": 1, "algorithms": 1, "aggregation": 1, "35": 1, "18": 1, "extend": 1, "result": 1, "metalearning": 1, "set": 1}, {"specifically": 1, "prove": 1, "follow": 1, "theorem": 2, "task": 1, "distribution": 1, "p": 1, "horizon": 1, "h": 1, "41": 1, "gmps": 1, "assume": 1, "rewardtogo": 1, "bound": 2, "": 18, "train": 1, "error": 1, "ph": 2, "show": 1, "eipt": 2, "e": 2, "ri": 3, "t1": 2, "st": 2, "ei": 1, "oh": 1, "pertask": 1, "expert": 1, "policies": 1}, {"proof": 1, "theorem": 1, "require": 1, "us": 1, "assume": 1, "inner": 1, "policy": 1, "update": 1, "eq": 1}, {"2": 1, "bring": 1, "learn": 1, "policy": 1, "within": 1, "bound": 1, "error": 1, "expert": 1, "amount": 1, "assumption": 1, "universality": 1, "gradientbased": 1, "metalearning": 1, "10": 1}, {"theorem": 1, "amount": 1, "say": 1, "gmps": 1, "achieve": 1, "expect": 1, "reward": 3, "within": 1, "bound": 1, "error": 2, "optimal": 1, "ie": 1, "individual": 1, "experts": 1, "linear": 1, "h": 1, "": 2}, {"analysis": 1, "hold": 1, "gmps": 1, "iteration": 1, "generate": 1, "sample": 1, "adapt": 1, "current": 1, "metatrained": 1, "policy": 1, "train": 1, "task": 1}, {"however": 1, "find": 1, "practice": 1, "initial": 1, "iteration": 1, "data": 1, "simply": 1, "sample": 1, "pertask": 1, "experts": 1, "": 1, "quite": 1, "stable": 1, "effective": 1, "hence": 1, "use": 1, "experimental": 1, "evaluation": 1}, {"full": 1, "proof": 1, "theorem": 1, "41": 1, "see": 1, "appendix": 1}, {"5": 1, "": 1, "43": 1, "algorithm": 2, "implementation": 1, "next": 1, "describe": 1, "full": 1, "metarl": 1, "detail": 1}, {"expert": 1, "policy": 1, "optimization": 1}, {"first": 1, "phase": 1, "gmps": 1, "entail": 1, "learn": 1, "policies": 1, "metatraining": 1, "task": 1}, {"simplest": 1, "approach": 1, "learn": 1, "separate": 1, "policy": 1, "task": 1, "scratch": 1}, {"already": 1, "improve": 1, "standard": 1, "metarl": 1, "since": 1, "employ": 1, "efficient": 1, "offpolicy": 1, "reinforcement": 1, "learn": 1, "algorithms": 1}, {"improve": 1, "efficiency": 1, "approach": 1, "employ": 1, "contextual": 1, "policy": 1, "represent": 1, "experts": 1, "simultaneously": 1, "use": 1, "data": 1, "task": 1}, {"express": 1, "policy": 1, "": 4, "st": 1, "represent": 1, "task": 1, "context": 1}, {"crucially": 1, "context": 2, "need": 1, "know": 1, "metatraining": 1, "": 1, "end": 1, "result": 1, "algorithm": 1, "second": 1, "phase": 1, "still": 1, "use": 1, "raw": 1, "task": 1, "reward": 1, "without": 1, "knowledge": 1, "metatest": 1, "time": 1}, {"experiment": 1, "employ": 1, "approach": 1, "together": 1, "softactor": 1, "critic": 1, "sac": 1, "15": 1, "efficient": 1, "offpolicy": 1, "rl": 1, "method": 1}, {"train": 1, "experts": 1, "also": 1, "incorporate": 1, "extra": 1, "information": 1, "metatraining": 1, "unavailable": 1, "metatest": 1, "time": 1, "knowledge": 1, "state": 1, "better": 1, "shape": 1, "reward": 1, "available": 1}, {"former": 1, "explore": 1, "singletask": 1, "rl": 1, "settings": 2, "23": 1, "31": 1, "latter": 1, "study": 1, "onpolicy": 1, "metarl": 1, "14": 1}, {"metaoptimization": 1, "algorithm": 1}, {"order": 1, "efficiently": 1, "optimize": 1, "metaobjective": 1, "eq": 1}, {"2": 1, "adopt": 1, "approach": 1, "similar": 1, "maml": 1}, {"metaiteration": 1, "task": 2, "ti": 2, "": 6, "first": 1, "draw": 1, "sample": 1, "dttri": 2, "policy": 2, "compute": 1, "update": 2, "parameters": 1, "use": 1, "optimize": 1, "lbc": 1, "average": 1, "minibatch": 1}, {"require": 1, "sample": 1, "": 2, "efficient": 1, "learn": 1, "minimize": 1, "number": 1, "metaiterations": 1}, {"note": 1, "take": 1, "multiple": 1, "gradient": 1, "step": 1, "behavior": 1, "clone": 1, "metaobjective": 1, "metaiteration": 1, "since": 1, "objective": 1, "require": 1, "onpolicy": 1, "sample": 1}, {"however": 1, "first": 1, "gradient": 1, "step": 1, "metaobjective": 1, "modify": 1, "preupdate": 1, "parameters": 2, "": 4, "need": 1, "recompute": 1, "adapt": 1, "start": 1, "would": 1, "like": 1, "without": 1, "collect": 1, "new": 1, "data": 1}, {"achieve": 1, "use": 1, "importanceweighted": 1, "policy": 2, "gradient": 1, "importance": 1, "weight": 1, "": 4, "st": 2, "init": 2, "denote": 1, "parameters": 1, "start": 1, "metaiteration": 1}, {"start": 1, "metaiteration": 1, "sample": 1, "trajectories": 1, "": 4, "current": 1, "policy": 1, "parameters": 1, "denote": 1, "init": 1}, {"take": 1, "many": 1, "offpolicy": 1, "gradient": 1, "step": 1, "": 1}, {"offpolicy": 1, "gradient": 1, "step": 1, "involve": 1, "recomputing": 1, "update": 1, "parameters": 1, "usingimportance": 1, "sample": 1, "": 15, "e": 1, "init": 2, "log": 1, "ai": 2, "3": 1, "estimate": 1, "advantage": 1, "function": 1}, {"offpolicy": 1, "gradient": 1, "step": 1, "compute": 1, "apply": 1, "use": 2, "update": 1, "parameters": 1, "behavioral": 1, "clone": 1, "objective": 1, "define": 1, "previously": 1, "": 7, "lbc": 1, "dival": 1}, {"optimization": 1, "algorithm": 1, "summarize": 1, "alg": 1}, {"2": 1}, {"5": 1, "": 2, "experimental": 1, "evaluation": 1, "evaluate": 1, "gmps": 1, "separately": 1, "metareinforcement": 1, "algorithm": 1, "learn": 1, "fast": 1, "rl": 1, "procedures": 1, "multitask": 1, "demonstration": 1, "data": 1}, {"consider": 1, "follow": 1, "question": 1, "metarl": 2, "algorithm": 1, "1": 1, "gmps": 1, "metalearn": 1, "efficiently": 1, "prior": 1, "methods": 1}, {"learn": 3, "demonstrations": 1, "2": 1, "use": 1, "imitation": 1, "outer": 1, "loop": 1, "optimization": 1, "enable": 1, "us": 1, "overcome": 1, "challenge": 1, "exploration": 1, "sparse": 1, "reward": 1, "3": 1, "effectively": 1, "metalearn": 1, "cnn": 1, "policies": 1, "quickly": 1, "adapt": 1, "visionbased": 1, "task": 1}, {"answer": 1, "question": 1, "consider": 1, "multiple": 1, "continuous": 1, "control": 1, "domains": 1, "show": 1, "fig": 1}, {"2": 1}, {"51": 1, "experimental": 1, "setup": 1, "sawyer": 1, "manipulation": 1, "task": 1}, {"task": 1, "involve": 1, "7dof": 1, "sawyer": 1, "arm": 1, "perform": 1, "3d": 1, "position": 1, "control": 1, "parallel": 1, "jaw": 1, "gripper": 1, "four": 1, "dof": 1, "total": 1, "include": 1, "openclose": 1}, {"sawyer": 1, "environments": 1, "include": 1, "": 2, "push": 2, "full": 1, "state": 1, "task": 1, "involve": 1, "block": 1, "fix": 1, "initial": 1, "position": 1, "target": 1, "location": 1, "sample": 1, "20": 1, "cm": 2, "10": 1, "region": 1}, {"target": 1, "location": 1, "within": 1, "region": 1, "observe": 1, "must": 1, "implicitly": 1, "infer": 1, "trialanderror": 1}, {"full": 1, "state": 1, "observations": 1, "include": 1, "3d": 1, "position": 1, "end": 1, "effector": 1, "block": 1}, {"": 1, "push": 1, "vision": 1, "except": 1, "policy": 1, "receive": 1, "image": 1, "instead": 1, "block": 1, "position": 1}, {"": 1, "door": 2, "open": 2, "task": 1, "distribution": 1, "involve": 1, "target": 1, "angle": 1, "sample": 1, "uniformly": 1, "0": 1, "60": 1, "degrees": 1}, {"target": 1, "angle": 1, "present": 1, "observations": 1, "must": 1, "implicitly": 1, "6": 1, "": 1, "figure": 2, "2": 1, "illustration": 1, "push": 3, "leave": 1, "door": 1, "open": 1, "center": 1, "legged": 1, "locomotion": 2, "right": 1, "use": 1, "experiment": 1, "goal": 1, "regions": 1, "specify": 1, "green": 1, "3": 1, "metatraining": 1, "efficiency": 1, "full": 1, "state": 1, "dense": 1, "ing": 1}, {"reward": 1, "locomotion": 1}, {"methods": 1, "reach": 1, "similar": 1, "asymptotic": 1, "performance": 1, "gmps": 1, "require": 1, "significantly": 1, "fewer": 1, "sample": 1}, {"infer": 1, "trialanderror": 1}, {"full": 1, "state": 2, "observations": 1, "include": 1, "3d": 1, "end": 1, "effector": 1, "position": 2, "arm": 1, "gripper": 1, "current": 1, "angle": 1, "door": 1}, {"quadrupedal": 1, "legged": 1, "locomotion": 1}, {"environment": 2, "use": 1, "ant": 1, "openai": 1, "gym": 1, "3": 1}, {"ask": 1, "distribution": 1, "comprise": 1, "goal": 1, "position": 1, "sample": 1, "uniformly": 1, "edge": 1, "circle": 1, "radius": 1, "2": 1, "0": 1, "90": 1, "degrees": 1}, {"consider": 1, "dense": 1, "reward": 2, "evaluate": 2, "gmps": 2, "metarl": 1, "algorithm": 1, "challenge": 1, "sparse": 1, "set": 1, "demonstrations": 1}, {"detail": 1, "reward": 1, "function": 1, "environments": 1, "network": 1, "architectures": 1, "hyperparameters": 1, "sweep": 1, "appendix": 1}, {"videos": 1, "result": 1, "available": 1, "online": 1, "1": 1, "": 1}, {"52": 1, "": 2, "metareinforcement": 1, "learn": 1, "first": 1, "evaluate": 1, "sample": 2, "efficiency": 1, "gmps": 1, "metarl": 1, "algorithm": 1, "measure": 1, "performance": 1, "function": 1, "total": 1, "number": 1, "use": 1, "metatraining": 1}, {"compare": 1, "recent": 1, "inference": 1, "base": 1, "offpolicy": 1, "method": 1, "pearl": 1, "34": 1, "policy": 1, "gradient": 1, "version": 1, "modelagnostic": 1, "metalearning": 1, "maml": 1, "8": 1, "use": 1, "reinforce": 1, "inner": 1, "loop": 2, "trpo": 1, "outer": 1}, {"also": 1, "compare": 1, "rl2": 1, "7": 1, "single": 1, "policy": 1, "train": 1, "across": 1, "metatraining": 1, "task": 1, "refer": 1, "comparison": 1, "multitask": 1}, {"metatraining": 1, "time": 2, "metatest": 1, "assume": 1, "access": 1, "task": 1, "context": 1, "ie": 1}, {"information": 1, "completely": 1, "specify": 1, "task": 1, "target": 1, "location": 1, "push": 1, "locomotion": 1, "experiment": 1}, {"train": 1, "policy": 1, "condition": 1, "target": 1, "position": 1, "soft": 1, "actorcritic": 1, "sac": 1, "obtain": 1, "expert": 1, "trajectories": 1, "use": 1, "gmps": 1}, {"sample": 1, "use": 1, "train": 1, "expert": 1, "policy": 1, "sac": 1, "include": 1, "evaluation": 1}, {"metatest": 1, "time": 1, "adapt": 1, "new": 1, "validation": 1, "task": 2, "access": 1, "reward": 1, "necessitate": 1, "metalearning": 1, "without": 1, "provide": 1, "contexts": 1, "policy": 1}, {"metalearning": 1, "curve": 1, "fig": 1}, {"3": 1, "see": 1, "similar": 1, "performance": 1, "compare": 1, "pearl": 1, "4x": 1, "improvement": 2, "sawyer": 1, "object": 1, "push": 1, "12x": 1, "legged": 1, "locomotion": 1, "maml": 1, "term": 1, "number": 1, "sample": 1, "require": 1}, {"also": 1, "see": 1, "gmps": 1, "perform": 1, "substantially": 1, "better": 1, "pearl": 1, "evaluate": 1, "figure": 1, "4": 1, "testtime": 1, "extrapolation": 1, "dense": 1, "reward": 1, "ant": 1, "locomotion": 1}, {"test": 2, "task": 2, "train": 1, "involve": 1, "navigate": 1, "red": 1, "goals": 1, "indicate": 1, "right": 1}, {"ing": 1, "distribution": 1, "legged": 1, "locomotion": 1, "gmps": 1, "get": 1, "better": 1, "average": 1, "return": 1, "across": 1, "task": 1, "leave": 1}, {"fig": 1}, {"4": 1}, {"pearl": 1, "cannot": 1, "generate": 1, "useful": 1, "contexts": 1, "distribution": 1, "task": 1, "gmps": 1, "use": 1, "policy": 1, "gradient": 1, "adapt": 1, "enable": 1, "continuously": 1, "make": 1, "progress": 1}, {"hence": 1, "combination": 1, "1": 1, "offpolicy": 2, "rl": 1, "algorithm": 1, "sac": 1, "obtain": 1, "pertask": 1, "experts": 1, "2": 1, "ability": 1, "take": 1, "multiple": 1, "supervise": 1, "gradient": 1, "step": 1, "wrt": 1}, {"experts": 1, "outer": 1, "loop": 1, "enable": 1, "us": 1, "obtain": 1, "significant": 1, "overall": 1, "sample": 1, "efficiency": 1, "gain": 1, "compare": 1, "onpolicy": 1, "metarl": 1, "algorithm": 1, "maml": 1, "also": 1, "show": 1, "much": 1, "better": 1, "extrapolation": 1, "dataefficient": 1, "contextual": 1, "methods": 1, "like": 1, "pearl": 1}, {"sample": 2, "efficiency": 1, "gain": 1, "important": 1, "since": 1, "bring": 1, "us": 1, "significantly": 1, "closer": 1, "robust": 1, "metareinforcement": 1, "learn": 1, "algorithm": 1, "run": 1, "physical": 1, "robots": 1, "practical": 1, "time": 1, "scale": 1, "complexity": 1}, {"1": 1, "": 3, "website": 1, "httpssitesgooglecomberkeleyeduguidedmetapolicysearchhome": 1, "7": 1, "figure": 1, "5": 1, "metatraining": 1, "comparisons": 1, "sparse": 2, "reward": 2, "door": 1, "open": 1, "leave": 1, "ant": 1, "locomotion": 1, "middle": 1, "vision": 1, "pusher": 1, "right": 1}, {"method": 1, "able": 1, "learn": 1, "sparse": 1, "reward": 1, "available": 1, "adaptation": 1, "whereas": 1, "prior": 1, "methods": 1, "struggle": 1}, {"visionbased": 1, "task": 1, "find": 1, "gmps": 1, "able": 1, "effectively": 1, "leverage": 1, "demonstrations": 1, "quickly": 1, "stably": 1, "learn": 1, "adapt": 1}, {"53": 1, "": 2, "metalearning": 1, "demonstrations": 2, "challenge": 1, "task": 1, "involve": 1, "sparse": 1, "reward": 1, "image": 1, "observations": 1, "access": 1, "greatly": 1, "help": 1, "learn": 1, "reinforcement": 1, "learners": 1}, {"gmps": 1, "allow": 1, "us": 1, "incorporate": 1, "supervision": 1, "demonstrations": 1, "much": 1, "easily": 1, "prior": 1, "methods": 1}, {"compare": 1, "pearl": 1, "maml": 1, "multitask": 1, "previous": 1, "section": 1}, {"evaluate": 1, "task": 3, "require": 1, "exploration": 2, "sparsereward": 1, "additionally": 1, "compare": 1, "model": 1, "agnostic": 1, "structure": 1, "noise": 1, "maesn": 1, "14": 1, "design": 1, "sparse": 1, "reward": 1, "mind": 1}, {"finally": 1, "compare": 1, "single": 1, "policy": 1, "train": 1, "imitation": 2, "learn": 1, "across": 1, "metatraining": 1, "task": 2, "use": 1, "provide": 1, "demonstrations": 1, "refer": 1, "comparison": 1, "multitask": 1, "adaptation": 1, "new": 1, "validation": 1, "via": 1, "finetuning": 1}, {"experiment": 1, "position": 1, "goal": 2, "location": 1, "provide": 1, "input": 1, "metalearning": 1, "algorithm": 1, "must": 1, "discover": 1, "strategy": 1, "infer": 1, "reward": 1}, {"sparse": 1, "reward": 1, "task": 1}, {"one": 1, "potential": 1, "benefit": 1, "learn": 2, "demonstrations": 2, "exploration": 1, "challenge": 1, "substantially": 1, "reduce": 1, "metaoptimizer": 1, "since": 1, "provide": 1, "detail": 1, "guidance": 1, "task": 1, "perform": 1}, {"hypothesize": 1, "typical": 1, "metarl": 1, "lack": 1, "easily": 1, "avail": 1, "figure": 1, "6": 1, "comparison": 1, "gmps": 1, "finetuning": 1, "policy": 1, "able": 1, "reward": 2, "signal": 1, "sparse": 1, "pretrained": 1, "multitask": 1, "imitation": 1, "heldout": 1, "validation": 1, "task": 2, "make": 1, "metaoptimization": 1, "sparsereward": 1, "door": 1, "open": 1, "right": 1, "vision": 1, "pusher": 1, "leave": 1}, {"metalearning": 1, "structure": 1, "across": 1, "task": 1, "gmps": 1, "achieve": 1, "faster": 1, "challenge": 1, "use": 1, "demonstra": 1, "learn": 1}, {"error": 1, "bar": 1, "across": 1, "different": 1, "seed": 1}, {"tions": 1, "make": 1, "optimization": 1, "significantly": 1, "easier": 1}, {"test": 1, "hypothesis": 1, "experiment": 1, "learn": 2, "reinforcement": 1, "sparse": 2, "reward": 1, "signal": 1, "two": 1, "different": 1, "domains": 1, "door": 1, "open": 1, "legged": 1, "locomotion": 1, "describe": 1, "section": 1, "51": 1}, {"see": 1, "fig": 1}, {"5": 1, "unlike": 1, "metarl": 1, "methods": 1, "maesn": 1, "pearl": 1, "maml": 1, "find": 2, "gmps": 1, "able": 1, "successfully": 1, "good": 1, "solution": 1, "sparse": 1, "reward": 1, "settings": 1, "learn": 1, "explore": 1}, {"benefit": 1, "largely": 1, "due": 1, "fact": 1, "tackle": 1, "exploration": 1, "problem": 1, "better": 1, "demonstrations": 1, "require": 1, "metareinforcement": 1, "learn": 1, "scratch": 1}, {"also": 1, "find": 1, "gmps": 1, "adapt": 1, "validation": 1, "task": 1, "successfully": 1, "policy": 1, "pretrained": 1, "multitask": 1, "imitation": 1, "fig": 1}, {"6": 1}, {"policy": 1, "pretrained": 1, "imitation": 1, "learn": 1, "effectively": 1, "transfer": 1, "new": 1, "validation": 1, "task": 1, "via": 1, "finetuning": 1, "since": 1, "train": 1, "adaptability": 1}, {"vision": 1, "base": 1, "task": 1}, {"deep": 1, "rl": 1, "methods": 1, "potential": 1, "acquire": 1, "policies": 1, "produce": 1, "action": 1, "base": 1, "simply": 1, "visual": 1, "input": 1, "22": 1, "25": 1, "9": 1}, {"however": 1, "vision": 1, "base": 1, "policies": 1, "quickly": 1, "adapt": 1, "new": 1, "task": 1, "use": 1, "metareinforcement": 1, "learn": 1, "prove": 1, "challenge": 1, "difficulty": 1, "optimize": 1, "metaobjective": 1, "extremely": 1, "high": 1, "variance": 1, "policy": 1, "gradient": 1, "algorithms": 1}, {"hand": 1, "visual": 1, "imitation": 1, "learn": 3, "algorithms": 2, "rl": 2, "leverage": 1, "supervise": 2, "far": 1, "successful": 1, "23": 1, "2": 1, "13": 1, "50due": 1, "stability": 1, "compare": 1}, {"evaluate": 1, "gmps": 1, "visual": 2, "observations": 1, "assumption": 1, "access": 1, "demonstrations": 1, "metatraining": 1, "task": 1}, {"give": 1, "demonstrations": 1, "directly": 1, "train": 1, "visionbased": 1, "policies": 1, "use": 1, "gmps": 1, "rl": 1, "inner": 1, "loop": 2, "imitation": 1, "outer": 1}, {"best": 1, "leverage": 1, "add": 1, "stability": 1, "provide": 1, "imitation": 1, "learn": 1, "metaoptimize": 1, "entire": 1, "policy": 1, "fully": 2, "connect": 2, "convolutional": 1, "layer": 2, "adapt": 1, "inner": 1, "loop": 1}, {"enable": 1, "us": 1, "get": 1, "benefit": 1, "fast": 1, "adaptation": 1, "retain": 1, "stability": 1, "metaimitation": 1}, {"8": 1, "": 1, "see": 1, "fig": 1}, {"5": 1, "learn": 1, "vision": 1, "base": 1, "policies": 1, "gmps": 1, "stable": 1, "achieve": 1, "higher": 1, "reward": 1, "use": 1, "metalearning": 1, "algorithms": 1, "maml": 1}, {"additionally": 1, "find": 1, "gmps": 1, "maml": 1, "able": 1, "achieve": 1, "better": 1, "performance": 1, "single": 1, "policy": 1, "train": 2, "reinforcement": 1, "learn": 1, "across": 1, "task": 1}, {"fig": 1}, {"6": 1, "see": 1, "gmps": 1, "outperform": 1, "multitask": 1, "imitation": 1, "adaptation": 1, "validation": 1, "task": 1, "sparse": 1, "reward": 1, "case": 1}, {"6": 1, "": 2, "discussion": 1, "future": 1, "work": 2, "present": 1, "metarl": 1, "algorithm": 1, "learn": 1, "efficient": 1, "rl": 1, "procedures": 1, "via": 1, "supervise": 1, "imitation": 1}, {"enable": 1, "substantially": 1, "efficient": 1, "metatraining": 1, "phase": 1, "incorporate": 1, "expertprovided": 1, "demonstrations": 1, "drastically": 1, "accelerate": 1, "acquisition": 1, "reinforcement": 1, "learn": 1, "procedures": 1, "priors": 1}, {"believe": 1, "method": 1, "address": 1, "major": 1, "limitation": 1, "metareinforcement": 2, "learn": 3, "although": 1, "algorithms": 1, "effectively": 1, "acquire": 1, "adaptation": 1, "procedures": 1, "new": 1, "task": 1, "metatest": 1, "time": 1, "sample": 2, "extremely": 1, "expensive": 1, "term": 1, "count": 1, "metatraining": 1, "limit": 1, "applicability": 1, "realworld": 1, "problems": 1}, {"accelerate": 1, "metatraining": 2, "via": 1, "demonstrations": 1, "enable": 1, "sampleefficient": 1, "learn": 1, "time": 2, "metatest": 1}, {"give": 1, "efficiency": 1, "stability": 1, "supervise": 1, "imitation": 1, "expect": 1, "method": 1, "readily": 1, "applicable": 1, "domains": 1, "highdimensional": 1, "observations": 1, "image": 1}, {"give": 1, "number": 1, "sample": 1, "need": 1, "experiment": 1, "approach": 1, "likely": 1, "efficient": 1, "enough": 1, "practical": 1, "run": 1, "physical": 1, "robotic": 1, "systems": 1}, {"investigate": 1, "applications": 1, "approach": 1, "realworld": 1, "reinforcement": 1, "learn": 1, "excite": 1, "direction": 1, "future": 1, "work": 1}, {"7": 1, "": 2, "acknowledgements": 1, "author": 1, "would": 1, "like": 1, "thank": 1, "tianhe": 1, "yu": 1, "contributions": 1, "early": 1, "version": 1, "paper": 1}, {"work": 1, "support": 1, "intel": 1, "jp": 1, "morgan": 1, "national": 1, "science": 1, "foundation": 1, "graduate": 1, "research": 1, "fellowship": 1, "abhishek": 1, "gupta": 1}, {"reference": 1, "1": 1, "yoshua": 1, "bengio": 2, "samy": 1, "jocelyn": 1, "cloutier": 1}, {"learn": 2, "synaptic": 1, "rule": 1}, {"2": 1, "mariusz": 1, "bojarski": 1, "davide": 1, "del": 1, "testa": 1, "daniel": 1, "dworakowski": 1, "bernhard": 1, "firner": 1, "beat": 1, "flepp": 1, "prasoon": 1, "goyal": 1, "lawrence": 1, "jackel": 1, "mathew": 1, "monfort": 1, "urs": 1, "muller": 1, "jiakai": 1, "zhang": 2, "xin": 1, "jake": 1, "zhao": 1, "karol": 1, "zieba": 1}, {"end": 2, "learn": 1, "selfdriving": 1, "cars": 1}, {"corr": 1, "abs160407316": 1, "2016": 1}, {"3": 1, "greg": 1, "brockman": 1, "vicki": 1, "cheung": 1, "ludwig": 1, "pettersson": 1, "jonas": 1, "schneider": 1, "john": 1, "schulman": 1, "jie": 1, "tang": 1, "wojciech": 1, "zaremba": 1}, {"openai": 1, "gym": 1}, {"arxiv": 1, "preprint": 1, "arxiv160601540": 1, "2017": 1}, {"4": 1, "tim": 1, "brys": 1, "anna": 1, "harutyunyan": 1, "halit": 1, "bener": 1, "suay": 1, "sonia": 1, "chernova": 1, "matthew": 1, "e": 1, "taylor": 1, "ann": 1}, {"reinforcement": 1, "learn": 1, "demonstration": 1, "shape": 1}, {"ijcai": 1, "2015": 1}, {"5": 1, "ignasi": 1, "clavera": 1, "anusha": 1, "nagabandi": 1, "ronald": 1, "fear": 1, "pieter": 1, "abbeel": 1, "sergey": 1, "levine": 1, "chelsea": 1, "finn": 1}, {"learn": 1, "adapt": 1, "metalearning": 1, "modelbased": 1, "control": 1}, {"arxiv": 1, "preprint": 1, "arxiv180311347": 1, "2018": 1}, {"6": 1, "yan": 1, "duan": 1, "marcin": 1, "andrychowicz": 1, "bradly": 1, "c": 1, "stadie": 1, "jonathan": 1, "ho": 1, "jonas": 1, "schneider": 1, "ilya": 1, "sutskever": 1, "pieter": 1, "abbeel": 1, "wojciech": 1, "zaremba": 1}, {"oneshot": 1, "imitation": 1, "learn": 1}, {"nip": 1, "page": 1, "10871098": 1, "2017": 1}, {"7": 1, "yan": 1, "duan": 1, "john": 1, "schulman": 1, "xi": 1, "chen": 1, "peter": 1, "l": 1, "bartlett": 1, "ilya": 1, "sutskever": 1, "pieter": 1, "abbeel": 1}, {"rl2": 1, "": 1, "fast": 1, "reinforcement": 2, "learn": 2, "via": 1, "slow": 1}, {"arxiv": 1, "preprint": 1, "arxiv161102779": 1, "2016": 1}, {"8": 1, "chelsea": 1, "finn": 1, "pieter": 1, "abbeel": 1, "sergey": 1, "levine": 1}, {"modelagnostic": 1, "metalearning": 1, "fast": 1, "adaptation": 1, "deep": 1, "network": 1}, {"international": 1, "conference": 1, "machine": 1, "learn": 1, "2017": 1}, {"9": 1, "chelsea": 1, "finn": 1, "sergey": 1, "levine": 1}, {"deep": 1, "visual": 1, "foresight": 1, "plan": 1, "robot": 1, "motion": 1}, {"2017": 1, "ieee": 1, "international": 1, "conference": 1, "robotics": 1, "automation": 1, "icra": 1, "page": 1, "27862793": 1}, {"ieee": 1, "2017": 1}, {"9": 1, "": 1, "10": 1, "chelsea": 1, "finn": 1, "sergey": 1, "levine": 1}, {"metalearning": 1, "universality": 1, "deep": 1, "representations": 1, "gradient": 1, "descent": 1, "approximate": 1, "learn": 1, "algorithm": 1}, {"international": 1, "conference": 1, "learn": 1, "representations": 1, "iclr": 1, "2018": 1}, {"11": 1, "chelsea": 1, "finn": 1, "tianhe": 1, "yu": 1, "tianhao": 1, "zhang": 1, "pieter": 1, "abbeel": 1, "sergey": 1, "levine": 1}, {"oneshot": 1, "visual": 1, "imitation": 1, "learn": 1, "via": 1, "metalearning": 1}, {"arxiv": 1, "preprint": 1, "arxiv170904905": 1, "2017": 1}, {"12": 1, "dibya": 1, "ghosh": 1, "avi": 1, "singh": 1, "aravind": 1, "rajeswaran": 1, "vikash": 1, "kumar": 1, "sergey": 1, "levine": 1}, {"divideandconquer": 1, "reinforcement": 1, "learn": 1}, {"international": 1, "conference": 1, "learn": 1, "representations": 1, "iclr": 1, "2018": 1}, {"13": 1, "alessandro": 1, "giusti": 1, "jrme": 1, "guzzi": 1, "dan": 1, "c": 1, "ciresan": 1, "fanglin": 1, "juan": 1, "p": 1, "rodrguez": 1, "flavio": 1, "fontana": 1, "matthias": 1, "faessler": 1, "christian": 1, "forster": 1, "jrgen": 1, "schmidhuber": 1, "gianni": 1, "di": 1, "caro": 1, "et": 1, "al": 1}, {"machine": 1, "learn": 1, "approach": 1, "visual": 1, "perception": 1, "forest": 1, "trail": 1, "mobile": 1, "robots": 1}, {"ieee": 1, "robotics": 1, "automation": 1, "letter": 1, "12661667": 1, "2016": 1}, {"14": 1, "abhishek": 1, "gupta": 1, "russell": 1, "mendonca": 1, "yuxuan": 1, "liu": 1, "pieter": 1, "abbeel": 1, "sergey": 1, "levine": 1}, {"metareinforcement": 1, "learn": 1, "structure": 1, "exploration": 1, "strategies": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "page": 1, "53075316": 1, "2018": 1}, {"15": 1, "tuomas": 1, "haarnoja": 1, "aurick": 1, "zhou": 1, "pieter": 1, "abbeel": 1, "sergey": 1, "levine": 1}, {"soft": 1, "actorcritic": 1, "offpolicy": 1, "maximum": 1, "entropy": 1, "deep": 1, "reinforcement": 1, "learn": 1, "stochastic": 1, "actor": 1}, {"arxiv": 1, "preprint": 1, "arxiv180101290": 1, "2018": 1}, {"16": 1, "todd": 1, "hester": 1, "matej": 1, "vecerik": 1, "olivier": 1, "pietquin": 1, "marc": 1, "lanctot": 1, "tom": 1, "schaul": 1, "bilal": 1, "piot": 1, "dan": 1, "horgan": 1, "john": 1, "quan": 1, "andrew": 1, "sendonaris": 1, "gabriel": 1, "dulacarnold": 1, "et": 1, "al": 1}, {"deep": 1, "qlearning": 1, "demonstrations": 1}, {"aaai": 1, "2018": 1}, {"17": 1, "rein": 1, "houthooft": 1, "richard": 1, "chen": 1, "phillip": 1, "isola": 1, "bradly": 1, "c": 1, "stadie": 1, "filip": 1, "wolski": 1, "jonathan": 1, "ho": 1, "pieter": 1, "abbeel": 1}, {"evolve": 1, "policy": 1, "gradients": 1}, {"arxiv": 1, "preprint": 1, "arxiv180204821": 1, "2018": 1}, {"18": 1, "gregory": 1, "kahn": 1, "tianhao": 1, "zhang": 1, "sergey": 1, "levine": 1, "pieter": 1, "abbeel": 1}, {"plato": 1, "policy": 1, "learn": 1, "use": 1, "adaptive": 1, "trajectory": 1, "optimization": 1}, {"corr": 1, "abs160300622": 1, "2016": 1}, {"19": 1, "jens": 1, "kober": 1, "j": 1, "andrew": 1, "bagnell": 1, "jan": 1, "peters": 1}, {"reinforcement": 1, "learn": 1, "robotics": 1, "survey": 1}, {"international": 1, "journal": 1, "robotics": 1, "research": 1, "2013": 1}, {"20": 1, "jens": 1, "kober": 1, "jan": 1, "r": 1, "peters": 1}, {"policy": 1, "search": 1, "motor": 1, "primitives": 1, "robotics": 1}, {"neural": 1, "information": 1, "process": 1, "systems": 1, "nip": 1, "2009": 1}, {"21": 1, "petar": 1, "kormushev": 1, "sylvain": 1, "calinon": 1, "darwin": 1, "g": 1, "caldwell": 1}, {"robot": 1, "motor": 1, "skill": 1, "coordination": 1, "embased": 1, "reinforcement": 1, "learn": 1}, {"international": 1, "conference": 1, "intelligent": 1, "robots": 1, "systems": 1, "iros": 1, "2010": 1}, {"22": 1, "sascha": 1, "lange": 1, "martin": 1, "riedmiller": 1, "arne": 1, "voigtlnder": 1}, {"autonomous": 1, "reinforcement": 1, "learn": 1, "raw": 1, "visual": 1, "input": 1, "data": 1, "real": 1, "world": 1, "application": 1}, {"2012": 1, "international": 1, "joint": 1, "conference": 1, "neural": 1, "network": 1, "ijcnn": 1, "page": 1, "18": 1}, {"ieee": 1, "2012": 1}, {"23": 1, "sergey": 1, "levine": 1, "chelsea": 1, "finn": 1, "trevor": 1, "darrell": 1, "pieter": 1, "abbeel": 1}, {"endtoend": 1, "train": 1, "deep": 1, "visuomotor": 1, "policies": 1}, {"journal": 1, "machine": 1, "learn": 1, "research": 1, "jmlr": 1, "1739140": 1, "2016": 1}, {"24": 1, "nikhil": 1, "mishra": 1, "mostafa": 1, "rohaninejad": 1, "xi": 1, "chen": 1, "pieter": 1, "abbeel": 1}, {"simple": 1, "neural": 1, "attentive": 1, "metalearner": 1}, {"international": 1, "conference": 1, "learn": 1, "representations": 1, "iclr": 1, "2018": 1}, {"25": 1, "volodymyr": 1, "mnih": 1, "koray": 1, "kavukcuoglu": 1, "david": 1, "silver": 1, "andrei": 1, "rusu": 1, "joel": 1, "veness": 1, "marc": 1, "g": 1, "bellemare": 1, "alex": 1, "grave": 1, "martin": 1, "riedmiller": 1, "andreas": 1, "k": 1, "fidjeland": 1, "georg": 1, "ostrovski": 1, "et": 1, "al": 1}, {"humanlevel": 1, "control": 1, "deep": 1, "reinforcement": 1, "learn": 1}, {"nature": 1, "5187540529533": 1, "2015": 1}, {"26": 1, "ashvin": 1, "nair": 1, "bob": 1, "mcgrew": 1, "marcin": 1, "andrychowicz": 1, "wojciech": 1, "zaremba": 1, "pieter": 1, "abbeel": 1}, {"overcome": 1, "exploration": 1, "reinforcement": 1, "learn": 1, "demonstrations": 1}, {"international": 1, "conference": 1, "robotics": 1, "automation": 1, "icra": 1, "2018": 1}, {"27": 1, "mohammad": 1, "norouzi": 1, "samy": 1, "bengio": 1, "zhifeng": 1, "chen": 1, "navdeep": 1, "jaitly": 1, "mike": 1, "schuster": 1, "yonghui": 1, "wu": 1, "dale": 1, "schuurmans": 1}, {"reward": 1, "augment": 1, "maximum": 1, "likelihood": 1, "neural": 1, "structure": 1, "prediction": 1}, {"corr": 1, "abs160900150": 1, "2016": 1}, {"10": 1, "": 1, "28": 1, "shayegan": 1, "omidshafiei": 1, "jason": 1, "pazis": 1, "christopher": 1, "amato": 1, "jonathan": 1, "p": 1, "john": 1, "vian": 1}, {"deep": 1, "decentralize": 1, "multitask": 1, "multiagent": 1, "rl": 1, "partial": 1, "observability": 1}, {"international": 1, "conference": 1, "machine": 1, "learn": 1, "icml": 1, "2017": 1}, {"29": 1, "emilio": 1, "parisotto": 1, "jimmy": 1, "lei": 1, "ba": 1, "ruslan": 1, "salakhutdinov": 1}, {"actormimic": 1, "deep": 1, "multitask": 1, "transfer": 1, "reinforcement": 1, "learn": 1}, {"international": 1, "conference": 1, "learn": 1, "representations": 1, "iclr": 1, "2016": 1}, {"30": 1, "jan": 1, "peters": 1, "stefan": 1, "schaal": 1}, {"policy": 1, "gradient": 1, "methods": 1, "robotics": 1}, {"international": 1, "conference": 1, "intelligent": 1, "robots": 1, "systems": 1, "iros": 1, "2006": 1}, {"31": 1, "lerrel": 1, "pinto": 1, "marcin": 1, "andrychowicz": 1, "peter": 1, "welinder": 1, "wojciech": 1, "zaremba": 1, "pieter": 1, "abbeel": 1}, {"asymmetric": 1, "actor": 1, "critic": 1, "imagebased": 1, "robot": 1, "learn": 1}, {"arxiv": 1, "preprint": 1, "arxiv171006542": 1, "2017": 1}, {"32": 1, "aravind": 1, "rajeswaran": 1, "vikash": 1, "kumar": 1, "abhishek": 1, "gupta": 1, "john": 1, "schulman": 1, "emanuel": 1, "todorov": 1, "sergey": 1, "levine": 1}, {"learn": 2, "complex": 1, "dexterous": 1, "manipulation": 1, "deep": 1, "reinforcement": 1, "demonstrations": 1}, {"robotics": 1, "science": 1, "systems": 1, "2018": 1}, {"33": 1, "kate": 1, "rakelly": 1, "aurick": 1, "zhou": 1, "deirdre": 1, "quillen": 1, "chelsea": 1, "finn": 1, "sergey": 1, "levine": 1}, {"efficient": 1, "offpolicy": 1, "metareinforcement": 1, "learn": 1, "via": 1, "probabilistic": 1, "context": 1, "variables": 1}, {"arxiv": 1, "preprint": 1, "arxiv190308254": 1, "2019": 1}, {"34": 1, "kate": 1, "rakelly": 1, "aurick": 1, "zhou": 1, "deirdre": 1, "quillen": 1, "chelsea": 1, "finn": 1, "sergey": 1, "levine": 1}, {"efficient": 1, "offpolicy": 1, "metareinforcement": 1, "learn": 1, "via": 1, "probabilistic": 1, "context": 1, "variables": 1}, {"international": 1, "conference": 1, "machine": 1, "learn": 1, "icml": 1, "2019": 1}, {"35": 1, "stphane": 1, "ross": 1, "geoffrey": 1, "gordon": 1, "draw": 1, "bagnell": 1}, {"reduction": 1, "imitation": 1, "learn": 2, "structure": 1, "prediction": 1, "noregret": 1, "online": 1}, {"international": 1, "conference": 1, "artificial": 1, "intelligence": 1, "statistics": 1, "2011": 1}, {"36": 1, "jonas": 1, "rothfuss": 1, "dennis": 1, "lee": 1, "ignasi": 1, "clavera": 1, "tamim": 1, "asfour": 1, "pieter": 1, "abbeel": 1}, {"promp": 1, "proximal": 1, "metapolicy": 1, "search": 1}, {"corr": 1, "abs181006784": 1, "2018": 1}, {"37": 1, "andrei": 1, "rusu": 1, "sergio": 1, "gomez": 1, "colmenarejo": 1, "caglar": 1, "gulcehre": 1, "guillaume": 1, "desjardins": 1, "jam": 1, "kirkpatrick": 1, "razvan": 1, "pascanu": 1, "volodymyr": 1, "mnih": 1, "koray": 1, "kavukcuoglu": 1, "raia": 1, "hadsell": 1}, {"policy": 1, "distillation": 1}, {"international": 1, "conference": 1, "learn": 1, "representations": 1, "iclr": 1, "2016": 1}, {"38": 1, "steindr": 1, "smundsson": 1, "katja": 1, "hofmann": 1, "marc": 1, "peter": 1, "deisenroth": 1}, {"meta": 1, "reinforcement": 1, "learn": 1, "latent": 1, "variable": 1, "gaussian": 1, "process": 1}, {"corr": 1, "abs180307551": 1, "2018": 1}, {"39": 1, "jrgen": 1, "schmidhuber": 1}, {"evolutionary": 1, "principles": 1, "selfreferential": 1, "learn": 3, "metameta": 1, "hook": 1}, {"phd": 1, "thesis": 1, "technische": 1, "universitt": 1, "mnchen": 1, "1987": 1}, {"40": 1, "david": 1, "silver": 1, "aja": 1, "huang": 1, "chris": 1, "j": 1, "maddison": 1, "arthur": 1, "guez": 1, "laurent": 1, "sifre": 1, "george": 1, "van": 1, "den": 1, "driessche": 1, "julian": 1, "schrittwieser": 1, "ioannis": 1, "antonoglou": 1, "veda": 1, "panneershelvam": 1, "marc": 1, "lanctot": 1, "et": 1, "al": 1}, {"master": 1, "game": 1, "go": 1, "deep": 1, "neural": 1, "network": 1, "tree": 1, "search": 1}, {"nature": 1, "2016": 1}, {"41": 1, "bradly": 1, "c": 1, "stadie": 1, "ge": 1, "yang": 1, "rein": 1, "houthooft": 1, "xi": 1, "chen": 1, "yan": 1, "duan": 1, "yuhuai": 1, "wu": 1, "pieter": 1, "abbeel": 1, "ilya": 1, "sutskever": 1}, {"considerations": 1, "learn": 2, "explore": 1, "via": 1, "metareinforcement": 1}, {"arxiv": 1, "preprint": 1, "arxiv180301118": 1, "2018": 1}, {"42": 1, "kaushik": 1, "subramanian": 1, "charles": 1, "l": 2, "isbell": 1, "jr": 1, "andrea": 1, "thomaz": 1}, {"exploration": 1, "demonstration": 1, "interactive": 1, "reinforcement": 1, "learn": 1}, {"international": 1, "conference": 1, "autonomous": 1, "agents": 1, "": 1, "multiagent": 1, "systems": 1, "2016": 1}, {"43": 1, "wen": 1, "sun": 1, "j": 1, "andrew": 1, "bagnell": 1, "byron": 1, "boot": 1}, {"truncate": 1, "horizon": 1, "policy": 1, "search": 1, "combine": 1, "reinforcement": 1, "learn": 2, "": 1, "imitation": 1}, {"international": 1, "conference": 1, "learn": 1, "representations": 1, "iclr": 1, "2018": 1}, {"44": 1, "flood": 1, "sing": 1, "li": 1, "zhang": 1, "tao": 1, "xiang": 1, "timothy": 1, "hospedales": 1, "yongxin": 1, "yang": 1}, {"learn": 3, "metacritic": 1, "network": 1, "sample": 1, "efficient": 1}, {"arxiv": 1, "preprint": 1, "arxiv170609529": 1, "2017": 1}, {"11": 1, "": 1, "45": 1, "matthew": 1, "e": 1, "taylor": 1, "halit": 1, "bener": 1, "suay": 1, "sonia": 1, "chernova": 1}, {"integrate": 1, "reinforcement": 1, "learn": 1, "human": 1, "demonstrations": 1, "vary": 1, "ability": 1}, {"international": 1, "conference": 1, "autonomous": 1, "agents": 1, "multiagent": 1, "systems": 1, "2011": 1}, {"46": 1, "yee": 1, "teh": 1, "victor": 1, "bapst": 1, "wojciech": 1, "czarnecki": 1, "john": 1, "quan": 1, "jam": 1, "kirkpatrick": 1, "raia": 1, "hadsell": 1, "nicolas": 1, "heess": 1, "razvan": 1, "pascanu": 1}, {"distral": 1, "robust": 1, "multitask": 1, "reinforcement": 1, "learn": 1}, {"neural": 1, "information": 1, "process": 1, "systems": 1, "nip": 1, "2017": 1}, {"47": 1, "sebastian": 1, "thrun": 1, "lorien": 1, "pratt": 1}, {"learn": 2}, {"springer": 1, "science": 1, "": 1, "business": 1, "media": 1, "2012": 1}, {"48": 1, "jane": 1, "x": 1, "wang": 1, "zeb": 1, "kurthnelson": 1, "dhruva": 1, "tirumala": 1, "hubert": 1, "soyer": 1, "joel": 1, "z": 1, "leibo": 1, "remi": 1, "munos": 1, "charles": 1, "blundell": 1, "dharshan": 1, "kumaran": 1, "matt": 1, "botvinick": 1}, {"learn": 2, "reinforcement": 1}, {"arxiv": 1, "preprint": 1, "arxiv161105763": 1, "2016": 1}, {"49": 1, "ronald": 1, "j": 1, "williams": 1}, {"simple": 1, "statistical": 1, "gradientfollowing": 1, "algorithms": 1, "connectionist": 1, "reinforcement": 1, "learn": 1}, {"reinforcement": 1, "learn": 1}, {"springer": 1, "1992": 1}, {"50": 1, "jiakai": 1, "zhang": 1, "kyunghyun": 1, "cho": 1}, {"queryefficient": 1, "imitation": 1, "learn": 1, "endtoend": 1, "simulate": 1, "drive": 1}, {"thirtyfirst": 1, "aaai": 1, "conference": 1, "artificial": 1, "intelligence": 1, "2017": 1}, {"12": 1}]
[{"generalize": 1, "offpolicy": 2, "actorcritic": 1, "": 2, "shangtong": 1, "zhang": 1, "wendelin": 1, "boehmer": 1, "shimon": 1, "whiteson": 1, "department": 1, "computer": 1, "science": 1, "university": 1, "oxford": 1, "shangtongzhang": 1, "wendelinboehmer": 1, "shimonwhitesoncsoxacuk": 1, "abstract": 1, "propose": 1, "new": 1, "objective": 2, "counterfactual": 1, "unify": 1, "exist": 1, "objectives": 1, "policy": 1, "gradient": 1, "algorithms": 1, "continue": 1, "reinforcement": 1, "learn": 1, "rl": 1, "set": 1}, {"compare": 1, "commonly": 1, "use": 1, "excursion": 1, "objective": 2, "mislead": 1, "performance": 2, "target": 1, "policy": 1, "deploy": 1, "new": 1, "better": 1, "predict": 1}, {"prove": 1, "generalize": 2, "offpolicy": 2, "policy": 3, "gradient": 3, "theorem": 1, "compute": 1, "counterfactual": 1, "objective": 1, "use": 1, "emphatic": 1, "approach": 1, "get": 1, "unbiased": 1, "sample": 1, "yield": 1, "actorcritic": 1, "geoffpac": 1, "algorithm": 1}, {"demonstrate": 1, "merit": 1, "geoffpac": 1, "exist": 1, "algorithms": 2, "mujoco": 1, "robot": 1, "simulation": 1, "task": 1, "first": 1, "empirical": 1, "success": 1, "emphatic": 1, "prevail": 1, "deep": 1, "rl": 1, "benchmarks": 1}, {"1": 1, "": 2, "introduction": 1, "reinforcement": 1, "learn": 1, "rl": 1, "algorithms": 1, "base": 1, "policy": 1, "gradient": 1, "theorem": 1, "sutton": 1, "et": 2, "al": 2, "2000": 1, "marbach": 1, "tsitsiklis": 1, "2001": 1, "recently": 1, "enjoy": 1, "great": 1, "success": 1, "various": 1, "domains": 1, "eg": 1, "achieve": 1, "humanlevel": 1, "performance": 1, "atari": 1, "game": 1, "mnih": 1, "2016": 1}, {"original": 1, "policy": 1, "gradient": 1, "theorem": 1, "onpolicy": 2, "use": 1, "optimize": 1, "objective": 1}, {"however": 1, "many": 1, "case": 1, "would": 1, "prefer": 1, "learn": 1, "offpolicy": 1, "improve": 1, "data": 1, "efficiency": 1, "lin": 1, "1992": 1, "exploration": 1, "osband": 1, "et": 1, "al": 1, "2018": 1}, {"end": 1, "offpolicy": 1, "policy": 1, "gradient": 1, "oppg": 1, "theorem": 1, "degris": 1, "et": 7, "al": 7, "2012": 1, "maei": 1, "2018": 3, "imani": 1, "develop": 1, "widely": 1, "use": 1, "silver": 1, "2014": 1, "lillicrap": 1, "2015": 1, "wang": 1, "2016": 1, "gu": 1, "2017": 2, "ciosek": 1, "whiteson": 1, "espeholt": 1}, {"ideally": 1, "offpolicy": 2, "algorithm": 1, "optimize": 1, "analogue": 1, "onpolicy": 1, "objective": 1}, {"continue": 1, "rl": 1, "set": 1, "analogue": 1, "would": 1, "performance": 1, "target": 1, "policy": 1, "expectation": 1, "wrt": 1}, {"stationary": 1, "distribution": 1, "target": 1, "policy": 1, "refer": 1, "alternative": 1, "life": 1, "objective": 1, "ghiassian": 1, "et": 1, "al": 1, "2018": 1}, {"objective": 1, "correspond": 1, "performance": 1, "target": 1, "policy": 1, "deploy": 1}, {"previously": 1, "oppg": 1, "optimize": 1, "different": 1, "objective": 1, "performance": 1, "target": 1, "policy": 1, "expectation": 1, "wrt": 1}, {"stationary": 1, "distribution": 1, "behavior": 1, "policy": 1}, {"objective": 2, "refer": 1, "excursion": 2, "ghiassian": 1, "et": 2, "al": 2, "2018": 1, "correspond": 1, "set": 1, "sutton": 1, "2016": 1}, {"unfortunately": 1, "excursion": 1, "objective": 1, "mislead": 1, "performance": 1, "target": 1, "policy": 1, "deploy": 1, "illustrate": 1, "section": 1, "3": 1}, {"infeasible": 1, "optimize": 1, "alternative": 1, "life": 1, "objective": 1, "directly": 1, "offpolicy": 1, "continue": 1, "set": 1}, {"instead": 1, "propose": 1, "optimize": 1, "counterfactual": 1, "objective": 2, "approximate": 1, "alternative": 1, "life": 1}, {"excursion": 2, "set": 1, "agent": 1, "stationary": 1, "distribution": 1, "behavior": 1, "policy": 2, "consider": 1, "hypothetical": 1, "follow": 1, "target": 1}, {"return": 1, "hypothetical": 1, "excursion": 1, "indicator": 1, "performance": 1, "target": 1, "policy": 1}, {"excursion": 1, "objective": 1, "measure": 1, "return": 1, "wrt": 1}, {"stationary": 1, "distribution": 1, "behavior": 2, "policy": 2, "use": 1, "sample": 1, "generate": 1, "execute": 1}, {"contrast": 1, "evaluate": 1, "alternative": 1, "life": 1, "objective": 1, "require": 1, "sample": 1, "stationary": 1, "distribution": 1, "target": 1, "policy": 1, "agent": 1, "access": 1}, {"counterfactual": 2, "objective": 2, "use": 1, "new": 1, "parameter": 1, "": 1, "control": 1, "33rd": 1, "conference": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "neurips": 1, "2019": 1, "vancouver": 1, "canada": 1}, {"akin": 1, "gelada": 1, "bellemare": 1, "2019": 1}, {"": 2, "0": 1, "counterfactual": 1, "objective": 2, "use": 1, "stationary": 1, "distribution": 1, "behavior": 1, "policy": 2, "measure": 1, "performance": 1, "target": 1, "recover": 1, "excursion": 1}, {"": 2, "1": 1, "counterfactual": 1, "objective": 2, "fully": 1, "decouple": 1, "behavior": 1, "policy": 3, "use": 1, "stationary": 1, "distribution": 1, "target": 2, "measure": 1, "performance": 1, "recover": 1, "alternative": 1, "life": 1}, {"excursion": 2, "objective": 1, "never": 1, "actually": 1, "execute": 1, "agent": 1, "always": 1, "follow": 1, "behavior": 1, "policy": 1}, {"make": 1, "two": 1, "contributions": 1, "paper": 1}, {"first": 1, "prove": 1, "generalize": 1, "offpolicy": 1, "policy": 2, "gradient": 2, "goppg": 1, "theorem": 1, "give": 1, "counterfactual": 1, "objective": 1}, {"second": 1, "use": 1, "emphatic": 1, "approach": 1, "sutton": 1, "et": 1, "al": 1, "2016": 1, "compute": 1, "unbiased": 1, "sample": 1, "policy": 1, "gradient": 1, "develop": 1, "generalize": 1, "offpolicy": 1, "actorcritic": 1, "geoffpac": 1, "algorithm": 1}, {"evaluate": 1, "geoffpac": 1, "empirically": 1, "challenge": 1, "robot": 1, "simulation": 1, "task": 1, "neural": 1, "network": 1, "function": 1, "approximators": 1}, {"geoffpac": 1, "outperform": 1, "actorcritic": 1, "algorithms": 1, "propose": 1, "degris": 1, "et": 1, "al": 1}, {"2012": 1, "imani": 1, "et": 1, "al": 1}, {"2018": 1, "best": 1, "knowledge": 1, "geoffpac": 1, "first": 1, "empirical": 1, "success": 1, "emphatic": 1, "algorithms": 1, "prevail": 1, "deep": 1, "rl": 1, "benchmarks": 1}, {"2": 1, "": 3, "background": 1, "use": 1, "timeindexed": 1, "capital": 1, "letter": 1, "eg": 1, "xt": 1, "denote": 1, "random": 1, "variable": 1}, {"use": 1, "bold": 2, "capital": 1, "letter": 2, "eg": 2, "x": 2, "denote": 2, "matrix": 1, "lowercase": 1, "column": 1, "vector": 1}, {"x": 1, "": 3, "r": 1, "scalar": 1, "function": 1, "define": 1, "finite": 1, "set": 1, "use": 1, "correspond": 1, "bold": 1, "lowercase": 1}, {"letter": 1, "denote": 1, "vector": 1, "form": 1, "ie": 1, "x": 1, "": 3, "xs1": 1}, {"": 1}, {"": 1}, {"": 2, "xss": 1}, {"use": 1, "denote": 2, "identity": 1, "matrix": 1, "1": 1, "allone": 1, "column": 1, "vector": 1}, {"consider": 1, "infinite": 1, "horizon": 1, "mdp": 1, "puterman": 1, "2014": 1, "consist": 1, "finite": 2, "state": 1, "space": 2, "action": 1, "bound": 1, "reward": 1, "function": 1, "r": 2, "": 7, "transition": 1, "kernel": 1, "p": 1, "0": 1, "1": 1}, {"consider": 1, "transitionbased": 1, "discount": 1, "function": 1, "white": 1, "2017": 1, "": 5, "0": 1, "1": 1, "unify": 1, "continue": 1, "task": 2, "episodic": 1}, {"time": 1, "step": 1, "agent": 1, "state": 1, "st": 1, "take": 1, "action": 1, "accord": 1, "policy": 1, "": 4, "0": 1, "1": 1}, {"agent": 1, "proceed": 1, "new": 1, "state": 1, "st1": 1, "accord": 1, "p": 2, "get": 1, "reward": 1, "rt1": 1, "satisfy": 1, "ert1": 1, "": 4, "rst": 1}, {"return": 1, "": 2, "time": 1}, {"": 1}, {"": 2, "1": 1}, {"step": 1, "gt": 1, "": 4, "i0": 1, "i1": 3, "rt1i": 1, "1": 1}, {"j0": 1, "stj": 1, "": 4, "atj": 1, "stj1": 1}, {"use": 1, "v": 2, "denote": 1, "value": 1, "function": 1, "": 3, "define": 1, "e": 1, "gt": 1, "st": 1}, {"like": 1, "white": 1, "": 1}, {"2017": 1, "assume": 1, "v": 1, "exist": 1, "use": 1, "q": 1, "": 4, "e": 1, "gt": 1, "st": 1, "denote": 1, "stateactionp": 1, "value": 1, "function": 1}, {"use": 1, "p": 1, "denote": 1, "transition": 1, "matrix": 1, "induce": 1, "": 2, "ie": 1}, {"p": 1, "s0": 1, "": 2, "asps0": 1}, {"assume": 1, "chain": 1, "induce": 1, "": 2, "ergodic": 1, "use": 1, "denote": 1}, {"unique": 1, "stationary": 1, "distribution": 1}, {"define": 1, "": 2, "diagd": 1}, {"offpolicy": 1, "set": 1, "agent": 1, "aim": 1, "learn": 1, "target": 1, "policy": 2, "": 2, "follow": 1, "behavior": 1}, {"use": 1, "assumption": 1, "coverage": 1, "sutton": 1, "barto": 1, "2018": 1, "ie": 1, "": 3, "0": 2}, {"assume": 1, "chain": 1, "induce": 1, "": 2, "ergodic": 1, "use": 1, "denote": 1}, {"": 1}, {"": 1}, {"stationary": 1, "distribution": 1}, {"similarly": 1, "": 2, "diagd": 1}, {"define": 1, "": 6, "st": 1}, {"": 4, "st1": 1, "at1": 1, "st": 1}, {"typically": 1, "two": 1, "kinds": 1, "task": 1, "rl": 1, "prediction": 1, "control": 1}, {"prediction": 3, "interest": 1, "find": 1, "value": 1, "function": 1, "v": 1, "give": 1, "policy": 1, "": 1, "temporal": 1, "difference": 1, "td": 1, "learn": 1, "sutton": 1, "1988": 1, "perhaps": 1, "powerful": 1, "algorithm": 1}, {"td": 1, "enjoy": 1, "convergence": 1, "guarantee": 1, "offpolicy": 1, "tabular": 1, "settings": 1}, {"td": 1, "also": 1, "combine": 1, "linear": 1, "function": 1, "approximation": 1}, {"update": 1, "rule": 1, "onpolicy": 1, "linear": 1, "td": 1, "": 1}, {"w": 3, "": 9, "step": 1, "size": 1, "rt1": 1, "v": 3, "st1": 1, "st": 2, "incremental": 1, "update": 1}, {"use": 1, "v": 2, "denote": 1, "estimate": 1, "parameterized": 1, "w": 1, "tsitsiklis": 1, "van": 1, "roy": 1, "1997": 1, "prove": 1, "convergence": 1, "onpolicy": 1, "linear": 1, "td": 1}, {"offpolicy": 1, "linear": 1, "td": 1, "update": 1, "weight": 1, "": 1}, {"divergence": 1, "offpolicy": 1, "linear": 1, "td": 1, "well": 1, "document": 1, "tsitsiklis": 1, "van": 1, "roy": 1, "1997": 1}, {"approach": 1, "issue": 1, "gradient": 1, "td": 1, "methods": 1, "sutton": 1, "et": 1, "al": 1}, {"2009": 1, "propose": 1}, {"instead": 1, "bootstrapping": 1, "prediction": 1, "successor": 1, "state": 1, "like": 1, "td": 2, "gradient": 2, "methods": 1, "compute": 1, "project": 1, "bellman": 1, "error": 1, "directly": 1}, {"gradient": 2, "td": 1, "methods": 2, "true": 1, "stochastic": 1, "enjoy": 1, "convergence": 1, "guarantee": 1}, {"however": 1, "usually": 1, "twotimescale": 1, "involve": 1, "two": 2, "set": 1, "parameters": 1, "learn": 1, "rat": 1, "make": 1, "hard": 1, "use": 1, "practice": 1, "sutton": 1, "et": 1, "al": 1, "2016": 1}, {"approach": 1, "issue": 1, "emphatic": 1, "td": 1, "etd": 1, "sutton": 1, "et": 1, "al": 1}, {"2016": 1, "propose": 1}, {"2": 1, "": 4, "etd": 1, "introduce": 1, "interest": 1, "function": 1, "0": 1, "specify": 1, "users": 1, "preferences": 1, "different": 1, "state": 1}, {"function": 1, "approximation": 1, "typically": 1, "cannot": 1, "get": 1, "accurate": 1, "predictions": 1, "state": 1, "must": 1, "thus": 1, "trade": 1}, {"state": 1, "usually": 1, "weight": 2, "offpolicy": 1, "set": 1, "eg": 1, "gradient": 1, "td": 1, "methods": 1, "interest": 1, "function": 1, "explicitly": 1, "sis": 1, "objective": 1}, {"consequently": 1, "weight": 1, "update": 1, "time": 1, "via": 1, "mt": 1, "": 1, "emphasis": 1, "accumulate": 1, "previous": 1, "interest": 1, "certain": 1, "way": 1}, {"simplest": 1, "form": 1, "etd": 1, "": 1}, {"mt": 1, "": 4, "ist": 1, "t1": 1, "mt1": 1}, {"update": 1, "weight": 1, "mt": 1, "": 1}, {"practice": 1, "usually": 1, "set": 1, "": 1, "1": 1}, {"inspire": 1, "etd": 1, "hallak": 1, "mannor": 1, "2017": 1, "propose": 1, "weight": 1, "via": 1, "cst": 1, "": 2, "consistent": 1}, {"offpolicy": 1, "td": 1, "coptd": 1, "algorithm": 1, "cs": 1, "": 1, "dd": 1, "density": 1, "ratio": 1, "also": 1, "know": 1, "covariate": 1, "shift": 1, "gelada": 1, "bellemare": 1, "2019": 1}, {"learn": 1, "c": 1, "via": 1, "stochastic": 1, "approximation": 1, "hallak": 1, "mannor": 1, "2017": 1, "propose": 1, "cop": 1, "operator": 1}, {"however": 1, "cop": 1, "operator": 1, "unique": 1, "fix": 1, "point": 1}, {"extra": 1, "normalization": 1, "projection": 1, "use": 1, "ensure": 1, "convergence": 1, "hallak": 1, "mannor": 1, "2017": 1, "tabular": 1, "set": 1}, {"address": 1, "limitation": 1, "gelada": 1, "bellemare": 1, "2019": 1, "propose": 1, "discount": 1, "cop": 1, "operator": 1}, {"": 1}, {"gelada": 1, "bellemare": 1, "2019": 1, "define": 1, "new": 1, "transition": 1, "matrix": 1, "p": 2, "": 5, "1": 2, "1dt": 1, "0": 1, "constant": 1}, {"follow": 1, "matrix": 1, "agent": 1, "either": 1, "proceed": 1, "next": 1, "state": 1, "accord": 1, "p": 1, "wp": 1}, {"": 1, "get": 1, "reset": 1, "wp": 1}, {"1": 1, "": 2}, {"gelada": 1, "bellemare": 1, "2019": 1, "show": 1, "chain": 1, "p": 1, "ergodic": 1}, {"denote": 1, "stationary": 1, "distribution": 1, "prove": 1, "": 4, "1": 2, "pt": 1}, {"": 3, "1": 1}, {"cs": 1, "": 28, "1": 5, "gelada": 1, "bellemare": 1, "2019": 1, "prove": 1, "c": 5, "d1": 1, "p": 1, "2": 1, "yield": 1, "follow": 1, "learn": 1, "rule": 1, "estimate": 2, "tabular": 1, "set": 1, "cst1": 3, "cst": 1, "3": 1, "step": 1, "size": 1}, {"semigradient": 1, "use": 1, "c": 1, "parameterized": 1, "function": 1, "gelada": 1, "bellemare": 1, "2019": 1}, {"small": 1, "": 3, "depend": 1, "difference": 1, "gelada": 1, "bellemare": 1, "2019": 1, "prove": 1, "multistep": 1, "contraction": 1, "linear": 1, "function": 1, "approximation": 1}, {"large": 1, "": 3, "nonlinear": 1, "function": 1, "approximation": 1, "provide": 1, "extra": 1, "normalization": 1, "loss": 1, "sake": 1, "constraint": 1, "dt": 1, "c": 1, "1t": 1, "1": 1}, {"gelada": 1, "bellemare": 1, "2019": 1, "use": 1, "cst": 1, "": 1, "weight": 1, "update": 1, "discount": 1, "coptd": 1}, {"demonstrate": 1, "empirical": 1, "success": 1, "atari": 1, "game": 1, "bellemare": 1, "et": 1, "al": 1, "2013": 1, "pixel": 1, "input": 1}, {"control": 2, "paper": 1, "focus": 1, "policybased": 1}, {"onpolicy": 1, "continue": 1, "set": 1, "seek": 1, "optimize": 1, "average": 1, "value": 1, "objective": 1, "silver": 1, "2015": 1, "": 1}, {"p": 1, "j": 1, "": 1, "sisv": 1}, {"4": 1, "optimize": 2, "average": 2, "value": 1, "objective": 2, "equivalent": 1, "reward": 1, "puterman": 1, "2014": 1, "": 1, "constant": 1, "see": 1, "white": 1, "2017": 1}, {"general": 1, "average": 2, "value": 1, "objective": 2, "interpret": 1, "generalization": 1, "reward": 1, "adopt": 1, "transitionbased": 1, "discount": 1, "nonconstant": 1, "interest": 1, "function": 1}, {"offpolicy": 1, "continue": 1, "set": 1, "imani": 1, "et": 1, "al": 1}, {"2018": 1, "propose": 1, "optimize": 1, "excursion": 1, "objective": 1, "": 1}, {"p": 1, "j": 2, "": 2, "sisv": 1, "5": 1, "instead": 1, "alternative": 1, "life": 1, "objective": 1}, {"key": 1, "difference": 1, "j": 2, "trade": 1, "different": 1, "state": 1}, {"function": 1, "approximation": 1, "usually": 1, "possible": 1, "maximize": 1, "v": 1, "state": 1, "first": 1, "tradeoff": 1, "need": 1, "make": 1}, {"moreover": 1, "visit": 2, "one": 1, "state": 2, "imply": 1, "another": 1, "less": 1, "second": 1, "tradeoff": 1, "need": 1, "make": 1}, {"j": 2, "achieve": 1, "kinds": 1, "tradeoff": 1, "accord": 1, "respectively": 1}, {"however": 1, "j": 2, "": 4, "correctly": 1, "reflect": 1, "deploytime": 1, "performance": 1, "behavior": 1, "policy": 1, "longer": 1, "matter": 1, "deploy": 1, "offpolicy": 1, "learn": 1, "continue": 1, "task": 1}, {"objectives": 1, "usually": 1, "set": 1, "1": 1}, {"assume": 1, "": 2, "parameterized": 1}, {"rest": 1, "paper": 1, "gradients": 1, "take": 1, "wrt": 1}, {"": 5, "unless": 1, "otherwise": 1, "specify": 1, "consider": 1, "gradient": 1, "3": 1, "b": 1, "c": 1, "figure": 1, "1": 1, "twocircle": 1, "mdp": 1}, {"reward": 1, "0": 1, "unless": 1, "specify": 1, "edge": 1, "b": 2, "probability": 1, "transition": 1, "target": 1, "policy": 1, "": 2, "train": 1, "c": 1, "influence": 1, "2": 1, "final": 1, "solution": 1, "find": 1, "geoffpac": 1}, {"one": 1, "component": 1, "": 1, "sake": 1, "clarity": 1}, {"clear": 1, "compute": 1, "policy": 1, "gradient": 1, "j": 1, "offpolicy": 1, "continue": 1, "set": 1, "directly": 1}, {"j": 2, "": 5, "compute": 1, "policy": 1, "gradient": 1, "p": 2, "sis": 1, "q": 1, "aas": 1, "asq": 1}, {"6": 1, "degris": 1, "et": 1, "al": 1}, {"2012": 1, "prove": 1, "offpolicy": 2, "policy": 1, "gradient": 1, "oppg": 1, "theorem": 1, "ignore": 1, "term": 1, "aq": 1, "without": 1, "introduce": 1, "bias": 1, "tabular": 1, "policy1": 1, "": 12, "1": 1, "yield": 1, "actor": 1, "critic": 1, "offpac": 1, "update": 1, "t1": 1, "q": 1, "st": 4, "log": 1, "7": 1, "step": 1, "size": 1, "sample": 2}, {"policy": 1, "use": 1, "general": 1, "function": 1, "approximator": 1, "imani": 1, "et": 1, "al": 1}, {"2018": 1, "propose": 1, "new": 1, "oppg": 1, "theorem": 1}, {"define": 1, "1": 1, "": 1}, {"1": 2, "": 1}, {"1": 5, "ft": 2, "": 10, "ist": 2, "t1": 1, "ft1": 1, "mt": 1}, {"1": 2, "zt": 1, "": 5, "mt": 1, "q": 1, "st": 2, "log": 1}, {"1": 2, "": 2, "0": 2, "constant": 1, "use": 1, "optimize": 1, "biasvariance": 1, "tradeoff": 1, "f1": 1}, {"imani": 1, "1": 1, "et": 1, "al": 1}, {"2018": 1, "prove": 1, "zt": 2, "unbiased": 1, "sample": 1, "j": 2, "limit": 1, "sense": 1, "1": 3, "": 5, "fix": 1, "ie": 1, "limt": 1, "e": 1}, {"base": 1, "imani": 1, "et": 1, "al": 1}, {"2018": 1, "propose": 1, "actorcritic": 1, "1": 1, "emphatic": 1, "weight": 1, "ace": 1, "algorithm": 1, "update": 1, "": 4, "t1": 1, "zt": 1}, {"ace": 1, "emphatic": 1, "1": 1, "approach": 1, "mt": 1, "emphasis": 1, "reweigh": 1, "update": 1}, {"3": 1, "": 3, "counterfactual": 2, "objective": 2, "introduce": 1}, {"p": 1, "j": 1, "": 3, "sisv": 1, "8": 1, "userdefined": 1, "interest": 1, "function": 1}, {"similarly": 1, "set": 2, "1": 1, "continue": 1, "proceed": 1, "general": 1}, {"": 3, "1": 1, "j": 2, "recover": 1, "alternative": 1, "life": 1, "objective": 1}, {"": 3, "0": 1, "j": 2, "recover": 1, "excursion": 1, "objective": 1}, {"motivate": 1, "counterfactual": 1, "objective": 1, "j": 3, "": 2, "first": 1, "present": 1, "twocircle": 1, "mdp": 1, "figure": 1, "1a": 1, "highlight": 1, "difference": 1}, {"twocircle": 1, "mdp": 1, "agent": 1, "need": 1, "make": 1, "decision": 1, "state": 1}, {"behavior": 1, "policy": 1, "": 1, "proceed": 1, "b": 1, "c": 1, "randomly": 1, "equal": 1, "probability": 1}, {"continue": 1, "task": 1, "discount": 1, "factor": 1, "": 1, "always": 2, "06": 1, "interest": 1, "1": 1}, {"task": 1, "specification": 1, "white": 1, "2017": 1, "optimal": 1, "policy": 1, "alternative": 1, "life": 1, "objective": 2, "j": 1, "": 2, "equivalent": 1, "average": 1, "reward": 1, "constant": 1, "stay": 1, "outer": 1, "circle": 1}, {"however": 1, "maximize": 1, "j": 1, "": 1, "agent": 1, "prefer": 1, "inner": 1, "circle": 1}, {"see": 1, "first": 1, "note": 1, "v": 2, "b": 1, "c": 1, "hardly": 1, "change": 1, "wrt": 1}, {"": 3, "v": 2, "b": 1, "36": 1, "c": 1, "5": 1}, {"maximize": 2, "j": 1, "": 2, "target": 1, "policy": 1, "would": 1, "prefer": 1, "transition": 1, "state": 1, "c": 1, "v": 1}, {"agent": 1, "therefore": 1, "remain": 1, "inner": 1, "circle": 1}, {"twocircle": 1, "mdp": 1, "tabular": 1, "policy": 1, "1": 1, "": 1, "see": 1, "errata": 1, "degris": 1, "et": 1, "al": 1}, {"2012": 1, "also": 1, "imani": 1, "et": 1, "al": 1}, {"2018": 2, "maei": 1}, {"4": 1, "": 1, "maximize": 1, "v": 1, "represent": 1, "accurately": 1}, {"consider": 1, "episodic": 2, "task": 1, "eg": 1, "aim": 1, "maximize": 1, "v": 1, "set": 1, "discount": 1, "transition": 1, "back": 1, "0": 1, "policy": 1, "optimal": 1, "return": 1, "criterion": 1}, {"however": 1, "consider": 2, "continue": 1, "task": 1, "aim": 1, "optimize": 1, "j": 1, "": 1, "state": 1, "visitation": 1}, {"excursion": 1, "objective": 1, "j": 1, "maximize": 1, "v": 1, "regardless": 1, "state": 3, "visitation": 1, "": 2, "yield": 1, "policy": 1, "never": 1, "visit": 1, "highest": 1, "value": 1}, {"policy": 1, "suboptimal": 1, "continue": 1, "task": 1}, {"maximize": 1, "j": 1, "": 1, "agent": 1, "sacrifice": 1, "v": 1, "visit": 1, "state": 1}, {"twocircle": 1, "mdp": 1, "artifact": 1, "due": 1, "small": 1, "": 1}, {"effect": 1, "also": 1, "occur": 1, "larger": 1, "": 1, "path": 1, "longer": 1}, {"function": 1, "approximation": 1, "discrepancy": 1, "j": 2, "magnify": 1, "need": 1, "make": 1, "tradeoff": 1, "maximize": 1, "v": 1, "state": 1, "visitation": 1}, {"one": 1, "solution": 1, "problem": 1, "set": 1, "interest": 1, "function": 1, "j": 1, "clever": 1, "way": 1}, {"however": 1, "clear": 1, "achieve": 1, "without": 1, "domain": 1, "knowledge": 1}, {"imani": 1, "et": 1, "al": 1}, {"2018": 1, "simply": 1, "set": 1, "1": 1}, {"another": 1, "solution": 1, "might": 1, "optimize": 1, "j": 1, "directly": 1, "offpolicy": 1, "learn": 1, "one": 1, "could": 1, "use": 1, "importance": 1, "sample": 1, "ratios": 1, "fully": 1, "correct": 1, "precup": 1, "et": 1, "al": 1}, {"2001": 1, "propose": 1, "valuebased": 1, "methods": 1, "episodic": 1, "set": 1}, {"however": 1, "solution": 1, "infeasible": 1, "continue": 1, "set": 1, "sutton": 1, "et": 1, "al": 1, "2016": 1}, {"one": 1, "may": 1, "also": 1, "use": 1, "differential": 1, "value": 2, "function": 2, "sutton": 1, "barto": 1, "2018": 1, "replace": 1, "discount": 1, "j": 1, "": 1}, {"offpolicy": 1, "policy": 1, "gradient": 1, "differential": 1, "value": 1, "function": 1, "however": 1, "still": 1, "open": 1, "problem": 1, "aware": 1, "exist": 1, "literature": 1}, {"paper": 1, "propose": 1, "optimize": 1, "j": 1, "instead": 1}, {"wellknown": 1, "fact": 1, "policy": 1, "gradient": 1, "stationary": 1, "distribution": 1, "exist": 1, "mild": 1, "condition": 1, "eg": 1, "see": 1, "yu": 1, "20052": 1, "follow": 1, "immediately": 1, "proof": 1, "existence": 1, "lim1": 1, "": 2}, {"moreover": 1, "trivial": 1, "see": 1, "lim0": 1, "": 2, "indicate": 1, "counterfactual": 1, "objective": 3, "recover": 1, "excursion": 1, "alternative": 1, "life": 1, "smoothly": 1}, {"furthermore": 1, "show": 1, "empirically": 1, "small": 1, "": 2, "eg": 1, "06": 1, "twocircle": 1, "mdp": 1, "02": 1, "mujoco": 1, "task": 1, "enough": 1, "generate": 1, "different": 1, "solution": 1, "maximize": 1, "j": 1}, {"4": 1, "": 2, "generalize": 1, "offpolicy": 1, "policy": 1, "gradient": 1, "section": 1, "derive": 1, "estimator": 1, "j": 1, "show": 1, "proposition": 1, "1": 1, "unbiased": 1, "limit": 1, "sense": 1}, {"standard": 1, "assumptions": 1, "give": 1, "supplementary": 1, "materials": 1}, {"oppg": 1, "theorem": 1, "imani": 1, "et": 1, "al": 1, "2018": 1, "leave": 1, "us": 1, "freedom": 1, "choose": 1, "interest": 1, "function": 1, "j": 1, "": 1}, {"paper": 1, "": 1}, {"set": 1, "": 1, "iscs": 1, "best": 1, "knowledge": 1, "first": 1, "time": 1, "ap": 1, "nontrivial": 1, "interest": 1, "use": 1}, {"hence": 1, "depend": 1, "": 1, "cannot": 1, "invoke": 1, "oppg": 1, "directly": 1, "j": 1, "6": 1, "sisv": 1}, {"however": 1, "still": 1, "invoke": 1, "remain": 1, "part": 1, "oppg": 1, "x": 3, "sisv": 1, "": 5, "ms": 1, "q": 1, "aas": 1, "9": 1}, {"": 1}, {"p": 3, "": 6, "1": 1, "s0": 2, "asps0": 1}, {"compute": 1, "gradient": 1, "j": 1, "": 1}, {"": 23, "theorem": 2, "1": 3, "generalize": 1, "offpolicy": 1, "policy": 1, "gradient": 1, "x": 3, "j": 1, "ms": 1, "q": 1, "aas": 1, "sisv": 1, "sgs": 1, "z": 2, "2": 1}, {"": 1}, {"1": 1, "g": 1, "": 5, "d1": 1, "b": 2, "pt": 1, "c": 1, "p": 1, "proof": 1}, {"first": 1, "use": 1, "product": 1, "rule": 1, "calculus": 1, "plug": 1, "": 14, "scs": 1, "x": 4, "j": 1, "sisv": 2, "scsisv": 2, "2": 1}, {"": 7, "z": 1, "3": 1, "completeness": 1, "include": 1, "proof": 1, "supplementary": 1, "materials": 1}, {"5": 1, "": 5, "z": 1, "4": 1, "1": 1, "3": 1, "follow": 1, "directly": 1, "9": 1}, {"show": 1, "2": 2, "": 2, "4": 1, "take": 1, "gradients": 1, "side": 1}, {"1": 6, "c": 8, "": 29, "d1": 5, "p": 8, "solve": 1, "linear": 1, "system": 1, "lead": 1, "g": 1, "cs": 1, "gs": 1, "2": 1, "4": 1, "follow": 1, "easily": 1}, {"": 3, "use": 1, "emphatic": 1, "approach": 1, "provide": 1, "unbiased": 1, "sample": 1, "j": 1}, {"define": 1, "": 1}, {"2": 1, "": 1}, {"2": 2, "": 1}, {"2": 3, "": 10, "cst1": 1, "t1": 2, "log": 1, "at1": 1, "st1": 1, "ft": 2, "ft1": 1, "mt": 1, "1": 1}, {"function": 1, "intrinsic": 1, "interest": 3, "contrast": 1, "userdefined": 1, "extrinsic": 1, "2": 2, "sample": 1, "b": 2, "ft": 1, "accumulate": 1, "previous": 1, "translate": 1, "g": 1, "biasvariance": 1, "tradeoff": 1, "similar": 1, "sutton": 1, "et": 1, "al": 1}, {"2016": 1, "imani": 1, "et": 1, "al": 1}, {"2018": 1}, {"define": 1, "": 1}, {"1": 1, "2": 1, "": 1}, {"2": 2, "zt": 5, "": 7, "ist": 1, "v": 1, "st": 1, "mt": 1, "proceed": 1, "show": 1, "unbiased": 1, "sample": 1, "j": 1}, {"": 1}, {"lemma": 1, "1": 3, "assume": 1, "chain": 1, "induce": 1, "": 8, "ergodic": 1, "fix": 1, "limit": 1, "f": 2, "2": 1, "limt": 1, "e": 1, "ft": 1, "st": 1, "exist": 1, "pt": 1, "b": 1}, {"1": 1, "": 1, "proof": 1}, {"previous": 1, "work": 1, "sutton": 1, "et": 2, "al": 2, "2016": 1, "imani": 1, "2018": 1, "assume": 1, "limt": 1, "e": 1, "ft": 1, "st": 1, "": 1, "exist": 1}, {"2": 1, "prove": 1, "existence": 2, "limt": 2, "e": 2, "ft": 2, "st": 2, "": 2, "inspire": 1, "process": 1, "compute": 1, "1": 1, "value": 1, "assume": 1, "sutton": 1, "et": 1, "al": 1}, {"2016": 1}, {"existence": 1, "1": 1, "limt": 1, "e": 1, "ft": 1, "st": 1, "": 2, "transitiondependent": 1, "also": 2, "establish": 1, "routine3": 1, "proof": 1, "involve": 1, "similar": 1, "techniques": 1, "hallak": 1, "mannor": 1, "2017": 1}, {"detail": 1, "supplementary": 1, "materials": 1}, {"": 2}, {"proposition": 1, "1": 4, "assume": 1, "chain": 1, "induce": 1, "": 9, "ergodic": 1, "fix": 1, "2": 1, "iscs": 1, "limt": 1, "e": 1, "zt": 1, "j": 1, "proof": 1}, {"proof": 1, "involve": 1, "proposition": 1, "1": 1, "imani": 1, "et": 1, "al": 1}, {"2018": 1, "lemma": 1, "1": 1}, {"detail": 1, "provide": 1, "supplementary": 1, "materials": 1}, {"": 3, "0": 1, "generalize": 1, "offpolicy": 1, "policy": 1, "gradient": 1, "goppg": 1, "theorem": 2, "recover": 1, "oppg": 1, "imani": 1, "et": 1, "al": 1}, {"2018": 1}, {"main": 1, "contribution": 1, "goppg": 1, "lie": 1, "computation": 1, "c": 1, "ie": 1, "policy": 2, "gradient": 2, "distribution": 1, "do": 1, "previous": 1, "methods": 1}, {"2": 1, "main": 1, "contribution": 1, "proposition": 1, "1": 1, "trace": 1, "ft": 1, "": 1, "effective": 1, "way": 1, "approximate": 1, "c": 1}, {"inspire": 1, "propostion": 1, "1": 1, "propose": 1, "update": 1, "": 5, "t1": 1, "zt": 1, "step": 1, "size": 1}, {"far": 1, "discuss": 1, "policy": 2, "gradient": 1, "single": 1, "dimension": 1, "parameter": 1, "": 4, "1": 2, "2": 2, "ft": 2, "mt": 2, "scalars": 1}, {"compute": 1, "policy": 1, "gradients": 1, "whole": 1, "": 4, "parallel": 1, "1": 2, "2": 2, "ft": 2, "mt": 2, "remain": 1, "scalars": 1, "become": 1, "vectors": 1, "size": 1}, {"intrinsic": 1, "interest": 1, "function": 2, "multidimensional": 1, "random": 1, "variable": 1, "instead": 1, "deterministic": 1, "scalar": 1, "like": 1}, {"therefore": 1, "generalize": 1, "concept": 1, "interest": 1}, {"far": 1, "also": 1, "assume": 1, "access": 1, "true": 2, "density": 1, "ratio": 1, "c": 1, "value": 1, "function": 1, "v": 1, "": 1}, {"plug": 1, "estimate": 2, "c": 2, "v": 1, "": 1, "yield": 1, "generalize": 1, "offpolicy": 1, "actorcritic": 1, "geoffpac": 1, "algorithm4": 1, "density": 1, "ratio": 1, "learn": 2, "via": 1, "rule": 1, "3": 1}, {"value": 1, "estimate": 1, "v": 1, "learn": 1, "offpolicy": 2, "prediction": 1, "algorithm": 1, "eg": 1, "onestep": 1, "td": 2, "sutton": 1, "barto": 1, "2018": 2, "gradient": 1, "methods": 1, "discount": 1, "coptd": 1, "vtrace": 1, "espeholt": 1, "et": 1, "al": 1}, {"pseudocode": 1, "geoffpac": 1, "provide": 1, "supplementary": 1, "materials": 1}, {"discuss": 1, "two": 1, "potential": 1, "practical": 1, "issue": 1, "geoffpac": 1}, {"first": 1, "proposition": 1, "1": 1, "require": 1, "": 2}, {"practice": 1, "mean": 1, "": 2, "execute": 1, "long": 1, "time": 1, "satisfy": 1, "warmup": 1, "3": 1, "4": 1, "existence": 1, "follow": 1, "directly": 1, "convergence": 1, "analysis": 1, "etd": 1, "yu": 1, "2015": 1}, {"moment": 1, "convergence": 1, "analysis": 1, "geoffpac": 1, "open": 1, "problem": 1}, {"6": 1, "": 1, "train": 1}, {"second": 1, "proposition": 1, "1": 1, "provide": 1, "unbiased": 1, "sample": 1, "fix": 1, "policy": 1, "": 1}, {"": 3, "update": 1, "1": 1, "2": 1, "ft": 2, "invalidate": 1, "well": 1, "c": 1, "v": 1}, {"update": 1, "rule": 1, "learn": 2, "rate": 2, "1": 1, "2": 1, "cannot": 1, "simply": 1, "use": 1, "larger": 1, "ft": 2, "": 2, "would": 1, "c": 1, "v": 1}, {"issue": 1, "also": 1, "appear": 1, "imani": 1, "et": 1, "al": 1}, {"2018": 1}, {"principle": 1, "could": 1, "store": 1, "previous": 1, "transition": 1, "replay": 2, "buffer": 1, "lin": 1, "1992": 1, "certain": 1, "number": 1, "step": 1, "": 1, "update": 1}, {"way": 1, "1": 1, "2": 1, "satisfy": 1, "requirement": 1, "": 4, "get": 1, "uptodate": 1, "ft": 2}, {"practice": 1, "find": 1, "unnecessary": 1}, {"use": 1, "small": 1, "learn": 1, "rate": 1, "": 2, "assume": 1, "change": 1, "slowly": 1, "ignore": 1, "invalidation": 1, "effect": 1}, {"5": 1, "": 2, "experimental": 1, "result": 1, "experiment": 1, "aim": 1, "answer": 1, "follow": 1, "question": 1}, {"1": 1, "geoffpac": 1, "find": 1, "solution": 1, "onpolicy": 1, "policy": 1, "gradient": 1, "algorithms": 1, "twocircle": 1, "mdp": 1, "promise": 1}, {"2": 1, "degree": 1, "counterfactualness": 1, "": 1, "influence": 1, "solution": 1}, {"3": 1, "geoffpac": 1, "scale": 1, "challenge": 1, "task": 1, "like": 1, "robot": 1, "simulation": 1, "mujoco": 1, "neural": 1, "network": 1, "function": 1, "approximators": 1}, {"4": 1, "counterfactual": 1, "objective": 1, "geoffpac": 1, "translate": 1, "performance": 1, "improvement": 1, "offpac": 1, "ace": 1}, {"5": 1, "geoffpac": 1, "compare": 1, "downstream": 1, "applications": 1, "oppg": 1, "eg": 1, "ddpg": 1, "lillicrap": 1, "et": 2, "al": 2, "2015": 1, "td3": 1, "fujimoto": 1, "2018": 1}, {"51": 1, "": 2, "twocircle": 2, "mdp": 2, "implement": 1, "tabular": 1, "version": 1, "ace": 1, "geoffpac": 1}, {"behavior": 1, "policy": 2, "": 2, "random": 1, "monitor": 1, "probability": 1, "b": 1, "target": 1}, {"figure": 1, "1b": 1, "plot": 1, "": 1, "b": 1, "train": 1}, {"curve": 1, "average": 1, "10": 1, "run": 1, "shade": 1, "regions": 1, "indicate": 1, "standard": 1, "errors": 1}, {"set": 1, "1": 2, "": 2, "2": 1, "ace": 1, "geoffpac": 1, "unbiased": 1}, {"geoffpac": 1, "": 1, "set": 1, "09": 1}, {"ace": 1, "converge": 2, "correct": 1, "policy": 3, "maximize": 2, "j": 2, "expect": 1, "geoffpac": 1, "": 1, "want": 1, "onpolicy": 1, "train": 1}, {"figure": 1, "1c": 1, "show": 1, "manipulate": 1, "": 1, "2": 1, "influence": 1, "final": 1, "solution": 1}, {"twocircle": 1, "mdp": 1, "2": 1, "little": 1, "influence": 1, "final": 2, "solution": 2, "manipulate": 1, "": 1, "significantly": 1, "change": 1}, {"52": 1, "": 2, "robot": 1, "simulation": 1, "figure": 1, "2": 1, "comparison": 1, "among": 1, "offpac": 1, "ace": 1, "geoffpac": 1}, {"black": 1, "dash": 1, "line": 1, "random": 1, "agents": 1}, {"figure": 1, "3": 1, "comparison": 1, "among": 1, "ddpg": 2, "td3": 2, "geoffpac": 2, "evaluation": 1, "benchmarked": 1, "offpac": 1, "ace": 1, "five": 1, "mujoco": 1, "robot": 1, "simulation": 1, "task": 1, "openai": 1, "gym": 1, "brockman": 1, "et": 1, "al": 1, "2016": 1}, {"original": 1, "task": 2, "episodic": 1, "adopt": 1, "similar": 1, "techniques": 1, "white": 1, "2017": 1, "compose": 1, "continue": 1}, {"set": 1, "discount": 1, "function": 1, "": 1, "099": 1, "nontermination": 1, "transition": 2, "0": 1, "termination": 1}, {"7": 1, "": 1, "agent": 1, "teleport": 1, "back": 1, "initial": 1, "state": 1, "upon": 1, "termination": 1}, {"interest": 1, "function": 1, "always": 1, "1": 1}, {"set": 1, "comply": 1, "common": 1, "train": 1, "scheme": 1, "mujoco": 1, "task": 1, "lillicrap": 1, "et": 1, "al": 1, "2015": 1, "asadi": 1, "williams": 1, "2016": 1}, {"however": 1, "interpret": 1, "task": 2, "continue": 1}, {"consequence": 1, "j": 1, "": 2, "instead": 1, "episodic": 1, "return": 1, "proper": 1, "metric": 1, "measure": 1, "performance": 1, "policy": 1}, {"behavior": 1, "policy": 2, "": 1, "fix": 1, "uniformly": 1, "random": 1, "gelada": 1, "bellemare": 1, "2019": 1}, {"data": 1, "generate": 1, "": 1, "significantly": 1, "different": 1, "meaningful": 1, "policy": 1, "task": 1}, {"thus": 1, "set": 1, "exhibit": 1, "high": 1, "degree": 1, "offpolicyness": 1}, {"monitor": 1, "j": 1, "periodically": 1, "train": 1}, {"evaluate": 1, "j": 1, "": 2, "state": 1, "sample": 1, "accord": 1, "v": 1, "approximate": 1, "via": 1, "monte": 1, "carlo": 1, "return": 1}, {"evaluation": 1, "base": 1, "commonly": 1, "use": 1, "total": 1, "undiscounted": 1, "episodic": 1, "return": 1, "criterion": 1, "provide": 1, "supplementary": 1, "materials": 1}, {"relative": 1, "performance": 1, "two": 1, "criterion": 1, "almost": 1, "identical": 1}, {"implementation": 1, "although": 1, "emphatic": 1, "algorithms": 1, "enjoy": 1, "great": 1, "theoretical": 1, "success": 2, "yu": 1, "2015": 1, "hallak": 1, "et": 3, "al": 3, "2016": 2, "sutton": 1, "imani": 1, "2018": 1, "empirical": 1, "still": 1, "limit": 1, "simple": 2, "domains": 1, "eg": 1, "handcraft": 1, "markov": 1, "chain": 1, "cartpole": 1, "balance": 1, "linear": 1, "function": 1, "approximation": 1}, {"best": 1, "knowledge": 1, "first": 1, "time": 1, "emphatic": 1, "algorithms": 1, "evaluate": 1, "challenge": 1, "robot": 1, "simulation": 1, "task": 1, "neural": 1, "network": 1, "function": 1, "approximators": 1}, {"stabilize": 1, "train": 1, "adopt": 1, "a2c": 1, "clemente": 1, "et": 2, "al": 2, "2017": 1, "paradigm": 1, "multiple": 1, "workers": 1, "utilize": 1, "target": 1, "network": 1, "mnih": 1, "2015": 1, "replay": 1, "buffer": 1, "lin": 1, "1992": 1}, {"three": 1, "algorithms": 1, "share": 1, "architecture": 1, "parameterization": 1}, {"first": 1, "tune": 1, "hyperparameters": 1, "offpac": 1}, {"ace": 1, "geoffpac": 1, "inherit": 1, "common": 1, "hyperparameters": 1, "offpac": 1}, {"ddpg": 1, "td3": 1, "use": 1, "architecture": 1, "hyperparameters": 1, "lillicrap": 1, "et": 1, "al": 1}, {"2015": 1, "fujimoto": 1, "et": 1, "al": 1}, {"2018": 1, "respectively": 1}, {"detail": 1, "provide": 1, "supplementary": 1, "materials": 1, "implementations": 1, "publicly": 1, "available": 1, "5": 1, "": 1}, {"result": 1, "first": 1, "study": 1, "influence": 2, "1": 2, "ace": 1, "": 3, "2": 1, "geoffpac": 1, "halfcheetah": 1}, {"result": 1, "report": 1, "supplementary": 1, "materials": 1}, {"find": 1, "ace": 1, "sensitive": 1, "1": 2, "set": 1, "": 1, "0": 1, "experiment": 1}, {"geoffpac": 1, "find": 1, "1": 1, "": 4, "07": 1, "2": 1, "06": 1, "02": 1, "produce": 1, "good": 1, "empirical": 1, "result": 1, "use": 1, "combination": 1, "remain": 1, "task": 1}, {"curve": 1, "average": 1, "10": 1, "independent": 1, "run": 1, "shade": 1, "regions": 1, "indicate": 1, "standard": 1, "errors": 1}, {"figure": 1, "2": 1, "compare": 1, "geoffpac": 1, "ace": 1, "offpac": 1}, {"geoffpac": 1, "significantly": 1, "outperform": 1, "ace": 1, "offpac": 1, "3": 1, "5": 1, "task": 1}, {"performance": 1, "walker": 1, "reacher": 1, "similar": 1}, {"performance": 1, "improvement": 1, "support": 1, "claim": 1, "optimize": 2, "j": 3, "better": 1, "approximate": 1, "": 1}, {"also": 1, "report": 1, "performance": 1, "random": 1, "agent": 1, "reference": 1}, {"moreover": 1, "first": 1, "time": 1, "ace": 1, "evaluate": 1, "challenge": 1, "domains": 1, "instead": 1, "simple": 1, "markov": 1, "chain": 1}, {"figure": 1, "3": 1, "compare": 1, "geoffpac": 1, "ddpg": 1, "td3": 1}, {"geoffpac": 1, "outperform": 1, "ddpg": 1, "hopper": 1, "swimmer": 1}, {"ddpg": 1, "uniformly": 1, "random": 1, "policy": 1, "exhibit": 1, "high": 1, "instability": 1, "halfcheetah": 1, "walker": 1, "hopper": 1}, {"expect": 1, "ddpg": 1, "ignore": 1, "discrepancy": 1, "": 1}, {"train": 1, "progress": 1, "discrepancy": 1, "get": 1, "larger": 1, "finally": 1, "yield": 1, "performance": 1, "drop": 1}, {"td3": 1, "use": 1, "several": 1, "techniques": 1, "stabilize": 1, "ddpg": 2, "translate": 1, "performance": 1, "stability": 1, "improvement": 1, "figure": 1, "3": 1}, {"however": 1, "geoffpac": 1, "still": 1, "outperform": 1, "td3": 1, "hopper": 1, "swimmer": 1}, {"fair": 1, "comparison": 1, "many": 1, "design": 1, "choices": 1, "ddpg": 1, "td3": 1, "geoffpac": 2, "different": 1, "eg": 1, "one": 1, "worker": 1, "vs": 2, "multiple": 1, "workers": 1, "deterministic": 1, "stochastic": 1, "policy": 1, "network": 1, "architectures": 1, "expect": 1, "outperform": 1, "applications": 1, "oppg": 1}, {"however": 1, "comparison": 1, "suggest": 1, "goppg": 1, "shed": 1, "light": 1, "improve": 1, "applications": 1, "oppg": 1}, {"6": 1, "": 2, "relate": 1, "work": 1, "density": 1, "ratio": 1, "c": 1, "key": 1, "component": 1, "geoffpac": 1, "propose": 1, "gelada": 1, "bellemare": 1, "2019": 1}, {"however": 1, "use": 1, "density": 1, "ratio": 1, "different": 1}, {"qlearning": 1, "watkins": 1, "dayan": 1, "1992": 1, "mnih": 1, "et": 1, "al": 1, "2015": 1, "semigradient": 1, "method": 1}, {"gelada": 1, "bellemare": 1, "2019": 1, "use": 1, "density": 1, "ratio": 1, "reweigh": 1, "qlearning": 1, "semigradient": 1, "update": 1, "directly": 1}, {"result": 1, "algorithm": 1, "still": 1, "belong": 1, "semigradient": 1, "methods": 1}, {"would": 2, "use": 1, "density": 1, "ratio": 1, "reweigh": 1, "offpac": 1, "update": 1, "7": 1, "directly": 1, "actorcritic": 1, "analogue": 1, "qlearning": 1, "approach": 1, "gelada": 1, "bellemare": 1, "2019": 1}, {"reweighed": 1, "offpac": 1, "however": 1, "longer": 1, "follow": 1, "policy": 2, "gradient": 1, "objective": 1, "j": 1, "": 1, "yield": 1, "instead": 1, "semigradient": 1}, {"paper": 1, "use": 1, "density": 1, "ratio": 1, "define": 1, "new": 2, "objective": 3, "counterfactual": 1, "compute": 1, "policy": 1, "gradient": 1, "directly": 1, "theorem": 1, "1": 1}, {"result": 1, "algorithm": 1, "geoffpac": 1, "still": 1, "belong": 1, "policy": 1, "gradient": 1, "methods": 1, "5": 1, "": 3, "httpsgithubcomshangtongzhangdeeprl": 1, "8": 1, "limit": 1, "sense": 1}, {"compute": 2, "policy": 2, "gradient": 2, "counterfactual": 1, "objective": 1, "require": 1, "density": 1, "ratio": 1, "explore": 1, "gelada": 1, "bellemare": 1, "2019": 1}, {"many": 1, "applications": 1, "oppg": 1, "eg": 1, "dpg": 1, "silver": 1, "et": 3, "al": 3, "2014": 1, "ddpg": 1, "acer": 1, "wang": 1, "2016": 1, "epg": 1, "ciosek": 1, "whiteson": 1, "2017": 1, "impala": 1, "espeholt": 1, "2018": 1}, {"particularly": 1, "gu": 1, "et": 1, "al": 1}, {"2017": 1, "propose": 1, "ipg": 1, "unify": 1, "offpolicy": 1, "policy": 1, "gradients": 1}, {"ipg": 1, "mix": 2, "gradients": 1, "ie": 1, "j": 2, "": 1}, {"compute": 1, "j": 1, "": 1, "ipg": 1, "need": 1, "onpolicy": 1, "sample": 1}, {"paper": 1, "counterfactual": 2, "objective": 2, "mix": 1, "objectives": 1, "need": 1, "onpolicy": 1, "sample": 1, "compute": 1, "policy": 1, "gradient": 1}, {"mix": 1, "j": 2, "directly": 1, "ipgstyle": 1, "possibility": 1, "future": 1, "work": 1}, {"policybased": 1, "offpolicy": 1, "algorithms": 1}, {"maei": 1, "2018": 1, "provide": 1, "unbiased": 1, "estimator": 1, "limit": 1, "sense": 1, "j": 1, "": 1, "assume": 1, "value": 1, "function": 1, "linear": 1}, {"theoretical": 1, "result": 1, "provide": 1, "without": 1, "empirical": 1, "study": 1}, {"imani": 1, "et": 1, "al": 1}, {"2018": 1, "eliminate": 1, "linear": 1, "assumption": 1, "provide": 1, "thorough": 1, "empirical": 1, "study": 1}, {"therefore": 1, "conduct": 1, "comparison": 1, "imani": 1, "et": 1, "al": 1}, {"2018": 2, "instead": 1, "maei": 1}, {"another": 1, "line": 1, "work": 1, "policy": 1, "entropy": 1, "use": 1, "reward": 1, "shape": 1}, {"target": 1, "policy": 1, "derive": 1, "value": 1, "function": 1, "directly": 1, "odonoghue": 1, "et": 3, "al": 3, "2016": 1, "nachum": 1, "2017a": 1, "schulman": 1, "2017": 1}, {"line": 1, "work": 1, "include": 1, "deep": 1, "energybased": 1, "rl": 1, "haarnoja": 1, "et": 2, "al": 2, "2017": 1, "2018": 1, "value": 2, "function": 2, "learn": 2, "offpolicy": 1, "policy": 1, "derive": 1, "directly": 1, "path": 2, "consistency": 1, "nachum": 1, "2017ab": 1, "gradients": 1, "compute": 1, "satisfy": 1, "certain": 1, "consistencies": 1}, {"line": 1, "work": 1, "orthogonal": 1, "paper": 1, "compute": 1, "policy": 1, "gradients": 1, "counterfactual": 1, "objective": 1, "directly": 1, "offpolicy": 1, "manner": 1, "involve": 1, "reward": 1, "shape": 1}, {"liu": 1, "et": 1, "al": 1}, {"2018": 1, "prove": 1, "c": 1, "unique": 1, "solution": 1, "minimax": 1, "problem": 1, "involve": 1, "maximization": 1, "function": 1, "set": 1, "f": 2, "show": 1, "theoretically": 1, "sufficiently": 1, "rich": 1, "eg": 1, "neural": 1, "network": 1}, {"make": 1, "tractable": 1, "restrict": 1, "f": 1, "ball": 1, "reproduce": 1, "kernel": 1, "hilbert": 1, "space": 1, "yield": 1, "close": 1, "form": 1, "solution": 1, "maximization": 1, "step": 1}, {"sgd": 1, "use": 2, "learn": 1, "estimate": 1, "c": 1, "minimization": 1, "step": 1, "policy": 1, "evaluation": 1}, {"concurrent": 1, "work": 1, "liu": 1, "et": 1, "al": 1, "2019": 1, "approximate": 1, "c": 1, "use": 1, "offpolicy": 1, "policy": 1, "gradient": 1, "j": 1, "": 1, "empirical": 1, "success": 1, "observe": 1, "simple": 1, "domains": 1}, {"contrast": 1, "j": 3, "unify": 1, "": 2, "naturally": 1, "allow": 1, "biasvariance": 1, "tradeoff": 1, "yield": 1, "empirical": 1, "success": 1, "challenge": 1, "robot": 1, "simulation": 1, "task": 1}, {"7": 1, "": 2, "conclusions": 1, "paper": 1, "introduce": 1, "counterfactual": 1, "objective": 3, "unify": 1, "excursion": 1, "alternative": 1, "life": 1, "continue": 1, "rl": 1, "set": 1}, {"provide": 1, "generalize": 1, "offpolicy": 1, "policy": 1, "gradient": 1, "theorem": 1, "correspond": 1, "geoffpac": 1, "algorithm": 1}, {"goppg": 1, "first": 2, "example": 1, "nontrivial": 1, "interest": 1, "function": 1, "use": 1, "geoffpac": 1, "empirical": 1, "success": 1, "emphatic": 1, "algorithms": 1, "prevail": 1, "deep": 1, "rl": 1, "benchmarks": 1}, {"numerous": 1, "applications": 1, "oppg": 1, "include": 1, "ddpg": 1, "acer": 1, "ipg": 1, "epg": 1, "impala": 1}, {"expect": 1, "goppg": 1, "shed": 1, "light": 1, "improve": 1, "applications": 1}, {"theoretically": 1, "convergent": 1, "analysis": 1, "geoffpac": 1, "involve": 1, "compatible": 1, "function": 1, "assumption": 1, "sutton": 1, "et": 1, "al": 1, "2000": 1, "multitimescale": 1, "stochastic": 1, "approximation": 1, "borkar": 1, "2009": 1, "also": 1, "worth": 1, "investigation": 1}, {"acknowledgments": 1, "sz": 1, "generously": 1, "fund": 1, "engineer": 1, "physical": 1, "sciences": 1, "research": 1, "council": 1, "epsrc": 1}, {"project": 1, "receive": 1, "fund": 1, "european": 2, "research": 2, "council": 1, "unions": 1, "horizon": 1, "2020": 1, "innovation": 1, "programme": 1, "grant": 1, "agreement": 1, "number": 1, "637713": 1}, {"experiment": 1, "make": 1, "possible": 1, "generous": 1, "equipment": 1, "grant": 1, "nvidia": 1}, {"author": 1, "thank": 1, "richard": 1, "sutton": 1, "matthew": 1, "fellows": 1, "huizhen": 1, "yu": 1, "valuable": 1, "discussion": 1}, {"reference": 1, "asadi": 1, "k": 1, "williams": 1, "j": 1, "2016": 1}, {"sampleefficient": 1, "deep": 1, "reinforcement": 1, "learn": 1, "dialog": 1, "control": 1}, {"arxiv": 1, "preprint": 1, "arxiv161206000": 1}, {"bellemare": 1, "g": 1, "naddaf": 1, "veness": 1, "j": 1, "bowl": 1, "2013": 1}, {"arcade": 1, "learn": 1, "environment": 1, "evaluation": 1, "platform": 1, "general": 1, "agents": 1}, {"journal": 1, "artificial": 1, "intelligence": 1, "research": 1}, {"9": 1, "": 1, "borkar": 1, "v": 1, "2009": 1}, {"stochastic": 1, "approximation": 1, "dynamical": 1, "systems": 1, "viewpoint": 1}, {"springer": 1}, {"brockman": 1, "g": 1, "cheung": 1, "v": 1, "pettersson": 1, "l": 1, "schneider": 1, "j": 3, "schulman": 1, "tang": 1, "zaremba": 1, "w": 1, "2016": 1}, {"openai": 1, "gym": 1}, {"arxiv": 1, "preprint": 1, "arxiv160601540": 1}, {"ciosek": 1, "k": 1, "whiteson": 1, "2017": 1}, {"expect": 1, "policy": 1, "gradients": 1}, {"arxiv": 1, "preprint": 1, "arxiv170605374": 1}, {"clemente": 1, "v": 1, "castejn": 1, "h": 1, "n": 1, "chandra": 1}, {"2017": 1}, {"efficient": 1, "parallel": 1, "methods": 1, "deep": 1, "reinforcement": 1, "learn": 1}, {"arxiv": 1, "preprint": 1, "arxiv170504862": 1}, {"degris": 1, "white": 1, "sutton": 1, "r": 1, "2012": 1}, {"arxiv12054839": 1}, {"offpolicy": 1, "actorcritic": 1}, {"arxiv": 1, "preprint": 1, "": 1, "espeholt": 1, "l": 1, "soyer": 1, "h": 1, "munos": 1, "r": 1, "simonyan": 1, "k": 1, "mnih": 1, "v": 2, "ward": 1, "doron": 1, "firoiu": 1, "harley": 1, "dun": 1, "et": 1, "al": 1}, {"2018": 1}, {"impala": 1, "scalable": 1, "distribute": 1, "deeprl": 1, "importance": 1, "weight": 1, "actorlearner": 1, "architectures": 1}, {"arxiv": 1, "preprint": 1, "arxiv180201561": 1}, {"fujimoto": 1, "van": 1, "hoof": 1, "h": 1, "meger": 1, "2018": 1}, {"address": 1, "function": 1, "approximation": 1, "error": 1, "actorcritic": 1, "methods": 1}, {"arxiv": 1, "preprint": 1, "arxiv180209477": 1}, {"gelada": 1, "c": 1, "bellemare": 1, "g": 1, "2019": 1}, {"offpolicy": 1, "deep": 1, "reinforcement": 1, "learn": 1, "bootstrapping": 1, "covariate": 1, "shift": 1}, {"proceed": 1, "33rd": 1, "aaai": 1, "conference": 1, "artificial": 1, "intelligence": 1}, {"ghiassian": 1, "patterson": 1, "white": 2, "sutton": 1, "r": 1}, {"2018": 1}, {"online": 1, "offpolicy": 1, "prediction": 1}, {"gu": 1, "lillicrap": 1, "turner": 1, "r": 1, "e": 1, "ghahramani": 1, "z": 1, "schlkopf": 1, "b": 1, "levine": 1, "2017": 1}, {"interpolate": 1, "policy": 1, "gradient": 2, "merge": 1, "onpolicy": 1, "offpolicy": 1, "estimation": 1, "deep": 1, "reinforcement": 1, "learn": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1}, {"haarnoja": 1, "tang": 1, "h": 1, "abbeel": 1, "p": 1, "levine": 1, "2017": 1}, {"reinforcement": 1, "learn": 1, "deep": 1, "energybased": 1, "policies": 1}, {"proceed": 1, "34th": 1, "international": 1, "conference": 1, "machine": 1, "learningvolume": 1, "70": 1, "page": 1, "13521361": 1}, {"jmlr": 1}, {"org": 1}, {"haarnoja": 1, "zhou": 1, "abbeel": 1, "p": 1, "levine": 1, "2018": 1}, {"soft": 1, "actorcritic": 1, "offpolicy": 1, "maximum": 1, "entropy": 1, "deep": 1, "reinforcement": 1, "learn": 1, "stochastic": 1, "actor": 1}, {"arxiv": 1, "preprint": 1, "arxiv180101290": 1}, {"hallak": 1, "mannor": 1, "2017": 1}, {"consistent": 1, "online": 1, "offpolicy": 1, "evaluation": 1}, {"proceed": 1, "34th": 1, "international": 1, "conference": 1, "machine": 1, "learn": 1}, {"hallak": 1, "tamar": 1, "munos": 1, "r": 1, "mannor": 1, "2016": 1}, {"generalize": 1, "emphatic": 1, "temporal": 1, "difference": 1, "learn": 1, "biasvariance": 1, "analysis": 1}, {"proceedins": 1, "30th": 1, "aaai": 1, "conference": 1, "artificial": 1, "intelligence": 1}, {"imani": 1, "e": 2, "grave": 1, "white": 1, "2018": 1}, {"offpolicy": 1, "policy": 1, "gradient": 1, "theorem": 1, "use": 1, "emphatic": 1, "weight": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1}, {"lillicrap": 1, "p": 1, "hunt": 1, "j": 2, "pritzel": 1, "heess": 1, "n": 1, "erez": 1, "tassa": 1, "silver": 1, "wierstra": 1, "2015": 1}, {"continuous": 1, "control": 1, "deep": 1, "reinforcement": 1, "learn": 1}, {"arxiv": 1, "preprint": 1, "arxiv150902971": 1}, {"lin": 1, "lj": 1}, {"1992": 1}, {"selfimproving": 1, "reactive": 1, "agents": 1, "base": 1, "reinforcement": 1, "learn": 1, "plan": 1, "teach": 1}, {"machine": 1, "learn": 1}, {"liu": 1, "q": 1, "li": 1, "l": 1, "tang": 1, "z": 1, "zhou": 1, "2018": 1}, {"break": 1, "curse": 1, "horizon": 1, "infinitehorizon": 1, "offpolicy": 1, "estimation": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1}, {"liu": 1, "swaminathan": 1, "agarwal": 1, "brunskill": 1, "e": 1, "2019": 1}, {"offpolicy": 1, "policy": 1, "gradient": 1, "state": 1, "distribution": 1, "correction": 1}, {"arxiv": 1, "preprint": 1, "arxiv190408473": 1}, {"maei": 1, "h": 1, "r": 1, "2018": 1}, {"convergent": 1, "actorcritic": 1, "algorithms": 1, "offpolicy": 1, "train": 1, "function": 1, "approximation": 1}, {"arxiv": 1, "preprint": 1, "arxiv180207842": 1}, {"marbach": 1, "p": 1, "tsitsiklis": 1, "j": 1, "n": 1, "2001": 1}, {"simulationbased": 1, "optimization": 1, "markov": 1, "reward": 1, "process": 1}, {"ieee": 1, "transactions": 1, "automatic": 1, "control": 1}, {"10": 1, "": 1, "mnih": 1, "v": 1, "badia": 1, "p": 1, "mirza": 1, "grave": 1, "lillicrap": 1, "harley": 1, "silver": 1, "kavukcuoglu": 1, "k": 1, "2016": 1}, {"asynchronous": 1, "methods": 1, "deep": 1, "reinforcement": 1, "learn": 1}, {"proceed": 1, "33rd": 1, "international": 1, "conference": 1, "machine": 1, "learn": 1}, {"mnih": 1, "v": 1, "kavukcuoglu": 1, "k": 1, "silver": 1, "rusu": 1}, {"veness": 1, "j": 1, "bellemare": 1, "g": 2, "grave": 1, "riedmiller": 1, "fidjeland": 1, "k": 1, "ostrovski": 1, "et": 1, "al": 1}, {"2015": 1}, {"humanlevel": 1, "control": 1, "deep": 1, "reinforcement": 1, "learn": 1}, {"nature": 1}, {"nachum": 1, "norouzi": 1, "xu": 1, "k": 1, "schuurmans": 1, "2017a": 1}, {"bridge": 1, "gap": 1, "value": 1, "policy": 1, "base": 1, "reinforcement": 1, "learn": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1}, {"nachum": 1, "norouzi": 1, "xu": 1, "k": 1, "schuurmans": 1, "2017b": 1}, {"trustpcl": 1, "offpolicy": 1, "trust": 1, "region": 1, "method": 1, "continuous": 1, "control": 1}, {"arxiv": 1, "preprint": 1, "arxiv170701891": 1}, {"odonoghue": 1, "b": 1, "munos": 1, "r": 1, "kavukcuoglu": 1, "k": 1, "mnih": 1, "v": 1, "2016": 1}, {"combine": 1, "policy": 1, "gradient": 1, "qlearning": 1}, {"arxiv": 1, "preprint": 1, "arxiv161101626": 1}, {"osband": 1, "aslanides": 1, "j": 1, "cassirer": 1}, {"2018": 1}, {"randomize": 1, "prior": 1, "function": 1, "deep": 1, "reinforcement": 1, "learn": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1}, {"precup": 1, "sutton": 1, "r": 1, "dasgupta": 1, "2001": 1}, {"offpolicy": 1, "temporaldifference": 1, "learn": 1, "function": 1, "approximation": 1}, {"proceed": 1, "18th": 1, "international": 1, "conference": 1, "machine": 1, "learn": 1}, {"puterman": 1, "l": 1, "2014": 1}, {"markov": 1, "decision": 1, "process": 1, "discrete": 1, "stochastic": 1, "dynamic": 1, "program": 1}, {"john": 1, "wiley": 1, "": 1, "sons": 1}, {"schulman": 1, "j": 1, "chen": 1, "x": 1, "abbeel": 1, "p": 1, "2017": 1}, {"equivalence": 1, "policy": 1, "gradients": 1, "soft": 1, "qlearning": 1}, {"arxiv": 1, "preprint": 1, "arxiv170406440": 1}, {"silver": 1, "2015": 1}, {"policy": 1, "gradient": 1, "methods": 1}, {"silverwebteachingfilespgpdf": 1}, {"url": 1, "": 2, "httpwww0csuclacukstaffd": 1}, {"silver": 1, "lever": 1, "g": 1, "heess": 1, "n": 1, "degris": 1, "wierstra": 1, "riedmiller": 1, "2014": 1}, {"deterministic": 1, "policy": 1, "gradient": 1, "algorithms": 1}, {"proceed": 1, "31st": 1, "international": 1, "conference": 1, "machine": 1, "learn": 1}, {"sutton": 1, "r": 1, "1988": 1}, {"learn": 1, "predict": 1, "methods": 1, "temporal": 1, "differences": 1}, {"machine": 1, "learn": 1}, {"sutton": 1, "r": 1, "barto": 1, "g": 1, "2018": 1}, {"reinforcement": 1, "learn": 1, "introduction": 1, "2nd": 1, "edition": 1}, {"mit": 1, "press": 1}, {"sutton": 1, "r": 2, "maei": 1, "h": 1, "szepesvri": 1, "c": 1, "2009": 1}, {"convergent": 1, "temporaldifference": 1, "algorithm": 1, "offpolicy": 1, "learn": 1, "linear": 1, "function": 1, "approximation": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1}, {"sutton": 1, "r": 2, "mahmood": 1, "white": 1, "2016": 1}, {"emphatic": 1, "approach": 1, "problem": 1, "offpolicy": 1, "temporaldifference": 1, "learn": 1}, {"journal": 1, "machine": 1, "learn": 1, "research": 1}, {"sutton": 1, "r": 1, "mcallester": 1, "singh": 1, "p": 1, "mansour": 1}, {"2000": 1}, {"policy": 1, "gradient": 1, "methods": 1, "reinforcement": 1, "learn": 1, "function": 1, "approximation": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1}, {"tsitsiklis": 1, "j": 1, "n": 1, "van": 1, "roy": 1, "b": 1}, {"1997": 1}, {"analysis": 1, "temporaldiffference": 1, "learn": 1, "function": 1, "approximation": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1}, {"wang": 1, "z": 1, "bapst": 1, "v": 2, "heess": 1, "n": 2, "mnih": 1, "munos": 1, "r": 1, "kavukcuoglu": 1, "k": 1, "de": 1, "freitas": 1, "2016": 1}, {"sample": 1, "efficient": 1, "actorcritic": 1, "experience": 1, "replay": 1}, {"arxiv": 1, "preprint": 1, "arxiv161101224": 1}, {"watkins": 1, "c": 1, "j": 1, "dayan": 1, "p": 1, "1992": 1}, {"qlearning": 1}, {"machine": 1, "learn": 1}, {"white": 1, "2017": 1}, {"unify": 1, "task": 1, "specification": 1, "reinforcement": 1, "learn": 1}, {"proceed": 1, "34th": 1, "international": 1, "conference": 1, "machine": 1, "learn": 1}, {"yu": 1, "h": 1, "2005": 1}, {"function": 1, "approximation": 1, "approach": 1, "estimation": 1, "policy": 1, "gradient": 1, "pomdp": 1, "structure": 1, "policies": 1}, {"21st": 1, "conference": 1, "uncertainty": 1, "artificial": 1, "intelligence": 1}, {"yu": 1, "h": 1, "2015": 1}, {"convergence": 1, "emphatic": 1, "temporaldifference": 1, "learn": 1}, {"conference": 1, "learn": 1, "theory": 1}, {"11": 1}]
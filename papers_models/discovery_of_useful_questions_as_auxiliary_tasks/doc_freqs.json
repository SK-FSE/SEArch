[{"discovery": 1, "useful": 2, "question": 3, "auxiliary": 1, "task": 1, "": 9, "vivek": 1, "veeriah1": 1, "matteo": 1, "hessel2": 1, "janarthanan": 1, "rajendran1": 1, "zhongwen": 1, "xu2": 1, "junhyuk": 1, "oh2": 1, "hado": 1, "van": 1, "hasselt2": 1, "richard": 1, "lewis1": 1, "david": 1, "silver2": 1, "satinder": 1, "singh12": 1, "abstract": 1, "arguably": 1, "intelligent": 1, "agents": 2, "ought": 1, "able": 1, "discover": 1, "learn": 4, "answer": 2, "unanticipated": 1, "knowledge": 1, "skills": 1, "depart": 1, "focus": 1, "much": 1, "machine": 1, "externally": 1, "define": 1}, {"present": 1, "novel": 1, "method": 1, "reinforcement": 1, "learn": 1, "rl": 1, "agent": 1, "discover": 1, "question": 1, "formulate": 1, "general": 1, "value": 1, "function": 1, "gvfs": 1, "fairly": 1, "rich": 1, "form": 1, "knowledge": 1, "representation": 1}, {"specifically": 1, "method": 1, "use": 1, "nonmyopic": 1, "metagradients": 1, "learn": 2, "gvfquestions": 1, "answer": 1, "auxiliary": 1, "task": 2, "induce": 1, "useful": 1, "representations": 1, "main": 1, "face": 1, "rl": 1, "agent": 1}, {"demonstrate": 1, "auxiliary": 2, "task": 3, "base": 1, "discover": 1, "gvfs": 1, "sufficient": 1, "build": 1, "representations": 1, "support": 1, "main": 1, "learn": 1, "better": 1, "popular": 1, "handdesigned": 1, "literature": 1}, {"furthermore": 1, "show": 1, "context": 1, "atari": 1, "2600": 1, "videogames": 1, "auxiliary": 1, "task": 2, "metalearned": 1, "alongside": 1, "main": 1, "improve": 1, "data": 1, "efficiency": 1, "actorcritic": 1, "agent": 1}, {"increasingly": 1, "important": 1, "component": 1, "recent": 1, "approach": 1, "develop": 1, "flexible": 1, "autonomous": 1, "agents": 1, "pose": 1, "useful": 1, "question": 1, "future": 1, "agent": 1, "learn": 1, "answer": 1, "experience": 1}, {"question": 1, "take": 1, "many": 2, "form": 1, "serve": 1, "purpose": 1}, {"answer": 1, "prediction": 1, "control": 1, "question": 1, "suitable": 1, "feature": 1, "state": 2, "may": 1, "directly": 1, "form": 1, "useful": 1, "representations": 1, "singh": 1, "et": 1, "al": 1, "2004": 1}, {"alternatively": 1, "prediction": 1, "control": 1, "question": 1, "may": 1, "define": 1, "auxiliary": 1, "task": 2, "drive": 1, "representation": 1, "learn": 1, "aid": 1, "main": 1, "jaderberg": 1, "et": 1, "al": 1, "2017": 1}, {"goalconditional": 1, "question": 1, "may": 1, "also": 1, "drive": 1, "acquisition": 1, "diverse": 1, "set": 1, "skills": 1, "even": 1, "main": 1, "task": 1, "know": 1, "form": 1, "basis": 1, "policy": 1, "composition": 1, "exploration": 1, "andrychowicz": 1, "et": 6, "al": 6, "2016": 1, "veeriah": 1, "2018": 5, "eysenbach": 1, "florensa": 1, "mankowitz": 1, "riedmiller": 1}, {"paper": 1, "consider": 1, "question": 1, "form": 1, "general": 1, "value": 1, "function": 1, "gvfs": 2, "sutton": 1, "et": 1, "al": 1, "2011": 1, "purpose": 1, "use": 1, "discover": 1, "auxiliary": 1, "task": 2, "aid": 1, "learn": 2, "main": 1, "reinforcement": 1, "rl": 1}, {"choose": 1, "gvf": 1, "formulation": 1, "flexibility": 1, "accord": 1, "reward": 1, "hypothesis": 1, "sutton": 1, "": 1, "barto": 1, "2018": 1, "goal": 1, "might": 1, "formulate": 1, "term": 1, "scalar": 1, "signal": 1, "cumulant": 1, "white": 1, "2015": 1, "whose": 1, "discount": 1, "sum": 1, "must": 1, "maximize": 1}, {"additionally": 1, "gvfbased": 1, "auxiliary": 1, "task": 2, "show": 1, "previous": 1, "work": 1, "improve": 1, "sample": 1, "efficiency": 1, "reinforcement": 1, "learn": 2, "agents": 1, "engage": 1, "complex": 1, "mirowski": 1, "et": 2, "al": 2, "2017": 2, "jaderberg": 1}, {"literature": 1, "gvfbased": 1, "auxiliary": 1, "task": 1, "typically": 1, "require": 1, "agent": 1, "estimate": 1, "discount": 2, "sum": 1, "suitable": 1, "handcraft": 2, "function": 1, "state": 1, "cumulants": 1, "gvf": 1, "terminology": 1, "factor": 1}, {"show": 1, "combine": 1, "gradients": 1, "learn": 1, "auxiliary": 1, "gvfs": 1, "1": 1, "2": 1, "": 1, "university": 1, "michigan": 1, "ann": 1, "arbor": 1}, {"correspond": 1, "author": 1, "vivek": 1, "veeriah": 1, "hvveeriahumichedui": 1, "deepmind": 1, "london": 1}, {"33rd": 1, "conference": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "neurips": 1, "2019": 1, "vancouver": 1, "canada": 1}, {"update": 1, "main": 1, "task": 1, "possible": 1, "accelerate": 1, "representation": 1, "learn": 1, "improve": 1, "performance": 1}, {"fell": 1, "however": 1, "onto": 1, "algorithm": 1, "designer": 1, "design": 1, "question": 1, "useful": 1, "specific": 1, "task": 1}, {"limitation": 1, "question": 1, "equally": 1, "well": 1, "align": 1, "main": 1, "task": 1, "bellemare": 1, "et": 1, "al": 1, "2019": 1, "whether": 1, "case": 1, "may": 1, "hard": 1, "predict": 1, "advance": 1}, {"paper": 1, "make": 1, "three": 1, "contributions": 1}, {"first": 1, "propose": 1, "principled": 1, "general": 1, "method": 1, "automate": 1, "discovery": 1, "question": 1, "form": 1, "gvfs": 1, "use": 1, "auxiliary": 1, "task": 1}, {"main": 2, "idea": 1, "use": 1, "metagradient": 1, "rl": 1, "discover": 1, "question": 1, "answer": 1, "maximise": 1, "usefulness": 1, "induce": 1, "representation": 1, "task": 1}, {"remove": 1, "need": 1, "handdesign": 1, "auxiliary": 1, "task": 1, "match": 1, "environment": 1, "agent": 1}, {"second": 1, "contribution": 1, "empirically": 1, "demonstrate": 1, "success": 1, "nonmyopic": 2, "metagradient": 3, "rl": 1, "large": 1, "challenge": 1, "domains": 1, "oppose": 1, "approximate": 1, "myopic": 1, "methods": 1, "previous": 1, "work": 1, "xu": 1, "et": 2, "al": 2, "2018": 2, "zheng": 1, "calculation": 1, "prove": 1, "essential": 1, "successfully": 1, "learn": 1, "useful": 1, "question": 1, "applicable": 1, "broadly": 1, "applications": 1, "metagradients": 1}, {"finally": 1, "demonstrate": 1, "context": 1, "atari": 1, "2600": 1, "videogames": 1, "discovery": 1, "auxiliary": 1, "task": 2, "improve": 1, "data": 1, "efficiency": 1, "actorcritic": 1, "agent": 1, "metalearned": 1, "along": 1, "side": 1, "main": 1}, {"1": 1, "": 2, "background": 2, "brief": 1, "gvfs": 1, "standard": 1, "value": 2, "function": 2, "rl": 1, "define": 1, "question": 2, "answer": 2, "discount": 1, "sum": 1, "future": 1, "reward": 1, "policy": 1, "approximate": 1}, {"generalize": 2, "value": 2, "function": 4, "gvfs": 1, "standard": 1, "allow": 1, "arbitrary": 1, "cumulant": 2, "state": 1, "place": 1, "reward": 1, "specify": 1, "combination": 1, "discount": 1, "factor": 1, "policy": 1}, {"generalization": 1, "standard": 1, "value": 2, "function": 2, "allow": 1, "gvfs": 2, "express": 1, "quite": 1, "general": 1, "predictive": 1, "knowledge": 1, "notably": 1, "temporaldifference": 1, "td": 1, "methods": 1, "learn": 2, "extend": 1, "predictionsanswers": 1}, {"refer": 1, "sutton": 1, "et": 1, "al": 1}, {"2011": 1, "additional": 1, "detail": 1}, {"prior": 1, "work": 1, "auxiliary": 1, "task": 1, "rl": 1, "jaderberg": 1, "et": 1, "al": 1}, {"2017": 1, "explore": 1, "extensively": 1, "potential": 1, "rl": 1, "agents": 1, "jointly": 1, "learn": 1, "representation": 1, "use": 2, "solve": 1, "main": 1, "task": 4, "number": 1, "gvfbased": 1, "auxiliary": 2, "pixelcontrol": 1, "featurecontrol": 1, "base": 1, "control": 1, "change": 1, "pixel": 1, "intensities": 1, "feature": 1, "activations": 1, "class": 1, "also": 1, "multitask": 1, "set": 1, "hessel": 1, "et": 1, "al": 1}, {"2019a": 1}, {"recent": 1, "examples": 1, "auxiliary": 1, "task": 1, "include": 1, "depth": 1, "loop": 1, "closure": 1, "classification": 1, "mirowski": 1, "et": 3, "al": 3, "2017": 2, "observation": 1, "reconstruction": 1, "reward": 1, "prediction": 2, "inverse": 1, "dynamics": 1, "shelhamer": 1, "manygoals": 1, "learn": 1, "veeriah": 1, "2018": 1}, {"geometrical": 1, "perspective": 1, "auxiliary": 1, "task": 1, "introduce": 1, "bellemare": 1, "et": 1, "al": 1}, {"2019": 1}, {"prior": 1, "work": 1, "metalearning": 2, "recently": 1, "lot": 1, "interest": 1, "explore": 1, "learn": 2}, {"metalearner": 1, "progressively": 1, "improve": 1, "learn": 1, "process": 1, "learner": 1, "schmidhuber": 1, "et": 1, "al": 1, "1996": 1, "thrun": 1, "": 1, "pratt": 1, "1998": 1, "attempt": 1, "solve": 1, "task": 1}, {"recent": 1, "work": 1, "metalearning": 1, "include": 1, "learn": 4, "good": 1, "policy": 1, "initializations": 1, "quickly": 1, "adapt": 1, "new": 1, "task": 1, "finn": 1, "et": 13, "al": 13, "2017": 6, "alshedivat": 1, "2018": 7, "improve": 2, "fewshot": 2, "performance": 1, "mishra": 1, "duan": 1, "snell": 1, "explore": 1, "stadie": 1, "unsupervised": 1, "gupta": 2, "hsu": 1, "model": 1, "adaptation": 1, "nagabandi": 1, "optimizers": 1, "andrychowicz": 1, "2016": 2, "li": 1, "": 2, "malik": 1, "ravi": 1, "larochelle": 1, "wichrowska": 1, "chen": 1}, {"prior": 1, "work": 1, "metagradients": 1, "xu": 1, "et": 1, "al": 1}, {"2018": 1, "formalize": 1, "metagradients": 1, "form": 1, "metalearning": 1, "metalearner": 1, "train": 2, "via": 2, "gradients": 2, "effect": 1, "metaparameters": 1, "learner": 1, "also": 1}, {"contrast": 1, "much": 1, "work": 1, "metalearning": 1, "focus": 1, "multitask": 1, "learn": 1, "xu": 1, "et": 1, "al": 1}, {"2018": 1, "formalize": 1, "use": 1, "metagradients": 1, "way": 1, "applicable": 1, "also": 1, "single": 1, "task": 1, "set": 1, "although": 1, "limit": 1}, {"illustrate": 1, "approach": 1, "use": 1, "metagradients": 1, "adapt": 1, "discount": 1, "factor": 2, "": 2, "bootstrapping": 1, "reinforcement": 1, "learn": 1, "agent": 2, "substantially": 1, "improve": 1, "performance": 1, "actorcritic": 1, "many": 1, "atari": 1, "game": 1}, {"concurrently": 1, "zheng": 1, "et": 1, "al": 1}, {"2018": 1, "use": 1, "metagradients": 1, "learn": 1, "intrinsic": 2, "reward": 2, "demonstrate": 1, "maximize": 1, "sum": 1, "extrinsic": 1, "could": 1, "improve": 1, "agents": 1, "performance": 1, "number": 1, "atari": 1, "game": 1, "mujoco": 1, "task": 1}, {"xu": 1, "et": 1, "al": 1}, {"2018": 1, "discuss": 1, "possibility": 1, "compute": 1, "metagradients": 1, "nonmyopic": 1, "manner": 1, "propose": 1, "algorithm": 1, "zheng": 1, "et": 1, "al": 1}, {"2018": 1, "introduce": 1, "severe": 1, "approximation": 1, "measure": 1, "immediate": 1, "consequences": 1, "update": 1}, {"2": 1, "": 3, "figure": 1, "1": 1, "architecture": 1, "discovery": 1, "leave": 1, "main": 1, "task": 1, "answer": 2, "network": 1, "parameters": 1, "take": 1, "past": 1, "observations": 1, "input": 1, "parameterises": 1, "directly": 1, "indirectly": 1, "policy": 1, "well": 1, "gvf": 1, "question": 1}, {"right": 1, "question": 1, "network": 1, "parameters": 1, "": 1, "take": 1, "future": 1, "observations": 1, "input": 1, "parameterises": 1, "cumulants": 1, "discount": 1, "specify": 1, "gvfs": 1}, {"2": 1, "": 2, "discovery": 2, "useful": 1, "question": 2, "section": 1, "present": 1, "neural": 1, "network": 1, "architecture": 1, "principled": 1, "metagradient": 1, "algorithm": 1, "gvfbased": 1, "use": 1, "auxiliary": 1, "task": 1, "context": 1, "deep": 1, "rl": 1, "agents": 1}, {"21": 1, "": 3, "neural": 2, "network": 3, "architecture": 2, "discovery": 1, "consider": 1, "feature": 1, "two": 1, "first": 1, "leave": 1, "figure": 1, "1": 1, "take": 1, "last": 1, "observations": 1, "oti1t": 1, "input": 1, "parameterises": 1, "directly": 1, "indirectly": 1, "policy": 1, "main": 1, "reinforcement": 1, "learn": 1, "task": 1, "together": 1, "gvfpredictions": 1, "number": 1, "discover": 1, "cumulants": 1, "discount": 1}, {"use": 1, "": 1, "denote": 1, "parameters": 1, "first": 1, "network": 1}, {"second": 1, "network": 2, "refer": 1, "question": 1, "depict": 1, "right": 1, "figure": 1, "1": 1}, {"take": 1, "input": 1, "j": 1, "future": 1, "observations": 1, "ot1tj": 1, "metaparameters": 1, "": 1, "compute": 1, "value": 1, "set": 1, "cumulants": 1, "ut": 2, "correspond": 1, "discount": 1, "therefore": 1, "vectors": 1}, {"use": 2, "future": 1, "observations": 1, "ot1tj": 1, "input": 1, "question": 2, "network": 2, "require": 1, "us": 1, "wait": 1, "j": 1, "step": 1, "unfold": 1, "compute": 1, "cumulants": 1, "discount": 1, "acceptable": 1, "answer": 1, "train": 1, "neither": 1, "need": 1, "action": 1, "selection": 1}, {"discuss": 1, "section": 1, "1": 1, "gvfquestion": 1, "specify": 1, "cumulant": 1, "function": 2, "discount": 1, "policy": 1}, {"method": 1, "question": 1, "network": 1, "explicitly": 1, "parameterises": 1, "discount": 1, "cumulants": 1, "consider": 1, "onpolicy": 1, "gvfs": 1, "therefore": 1, "policy": 2, "always": 1, "implicitly": 1, "latest": 1, "maintask": 1, "": 1}, {"note": 1, "however": 1, "since": 1, "cumulant": 2, "function": 2, "future": 1, "observations": 1, "influence": 1, "action": 1, "choose": 1, "main": 1, "task": 1, "policy": 2, "discount": 1, "nonstationary": 1, "learn": 2, "question": 1, "network": 1, "parameters": 1, "also": 1, "maintask": 1, "change": 1, "progress": 1}, {"previous": 1, "work": 1, "auxiliary": 2, "task": 2, "reinforcement": 1, "learn": 1, "may": 1, "interpret": 1, "use": 2, "network": 2, "leave": 1, "cumulant": 1, "function": 1, "handcraft": 1, "metalearnable": 1, "parameters": 1, "availability": 1, "separate": 1, "question": 2, "critical": 1, "component": 1, "approach": 1, "discovery": 1, "enable": 1, "agent": 1, "discover": 1, "experience": 1, "suitable": 1, "future": 1}, {"terminology": 1, "question": 1, "answer": 1, "network": 2, "derive": 1, "work": 1, "td": 1, "sutton": 1, "": 1, "tanner": 1, "2005": 1}, {"see": 1, "makino": 1, "": 1, "takagi": 1, "2008": 1, "schlegel": 1, "et": 1, "al": 1}, {"2018": 1, "relate": 1, "work": 2, "incremental": 1, "discovery": 1, "structure": 1, "tdnetworks": 1, "gvfnetworks": 1, "however": 1, "use": 1, "metagradients": 1, "apply": 1, "relatively": 1, "simple": 1, "domains": 1}, {"22": 1, "": 3, "multistep": 1, "metagradients": 1, "abstract": 1, "form": 1, "reinforcement": 1, "learn": 1, "algorithms": 1, "describe": 1, "update": 1, "procedure": 1, "modify": 1, "step": 1, "agents": 1, "parameters": 1}, {"central": 1, "idea": 1, "metagradient": 1, "rl": 1, "parameterise": 1, "update": 1, "": 2, "metaparameters": 1}, {"may": 1, "consider": 1, "consequences": 1, "change": 1, "": 2, "parameterised": 1, "update": 1, "rule": 1, "measure": 1, "subsequent": 1, "performance": 1, "agent": 1, "term": 1, "metaloss": 1, "function": 1, "mtk": 1}, {"metaloss": 1, "may": 1, "evaluate": 1, "one": 1, "update": 2, "myopic": 1, "k": 1, "": 1, "1": 1, "nonmyopic": 1}, {"metagradient": 1, "chain": 1, "rule": 1, "mtk": 2, "": 4, "tk": 1}, {"": 6, "tk": 1, "1": 2, "implicit": 1, "equation": 1, "change": 1, "metaparameters": 1, "one": 1, "time": 2, "step": 2, "affect": 1, "immediate": 1, "update": 2, "next": 1, "future": 1}, {"make": 1, "metagradient": 2, "3": 1, "": 25, "algorithm": 1, "1": 3, "multistep": 1, "discovery": 1, "question": 1, "auxiliary": 1, "task": 1, "initialize": 1, "parameters": 2, "2": 2, "n": 1, "t0": 1, "k": 1, "l": 1, "generate": 1, "experience": 1, "use": 1, "tk1": 6, "tk": 2, "0": 2, "lrl": 2, "lans": 1, "end": 2, "pl": 1, "t1": 2, "k1": 1, "tl": 1, "challenge": 1, "compute": 1}, {"straightforward": 1, "effective": 1, "way": 1, "capture": 1, "multistep": 1, "effect": 1, "change": 1, "": 7, "build": 1, "computational": 1, "graph": 1, "consist": 1, "sequence": 1, "update": 1, "make": 1, "parameters": 1, "tk": 1, "hold": 1, "fix": 1, "end": 1, "metaloss": 1, "evaluation": 1, "mtk": 1}, {"tk": 1, "": 3, "metagradient": 1, "may": 1, "efficiently": 1, "compute": 1, "graph": 1, "backwardmode": 1, "au": 1, "todifferentiation": 1, "computational": 1, "cost": 1, "similar": 1, "forward": 1, "computation": 1, "griewank": 1, "walther": 1, "2008": 1, "require": 1, "storage": 1, "k": 1, "copy": 1, "parameters": 1, "ttk": 1, "thus": 1, "increase": 1, "memory": 1, "footprint": 1}, {"emphasize": 1, "approach": 1, "contrast": 1, "myopic": 1, "metagradient": 1, "use": 1, "previous": 1, "work": 1, "either": 1, "ignore": 1, "effect": 1, "past": 1, "first": 1, "time": 1, "step": 1, "make": 1, "severe": 1, "approximations": 1}, {"23": 1, "": 2, "multistep": 1, "metagradient": 2, "algorithm": 2, "discovery": 2, "apply": 1, "present": 1, "section": 2, "22": 1, "gvfbased": 1, "auxiliary": 1, "task": 1, "represent": 1, "neural": 1, "network": 1, "architecture": 1, "21": 1}, {"complete": 1, "pseudo": 1, "code": 1, "propose": 1, "approach": 1, "discovery": 1, "outline": 1, "algorithm": 1, "1": 1}, {"iteration": 1, "algorithm": 1, "inner": 1, "loop": 1, "apply": 1, "l": 1, "update": 1, "agent": 1, "parameters": 1, "": 1, "parameterise": 1, "maintask": 1, "policy": 1, "gvf": 1, "answer": 1, "use": 1, "separate": 1, "sample": 1, "experience": 1, "environment": 1}, {"outer": 1, "loop": 1, "apply": 1, "single": 1, "update": 2, "metaparameters": 1, "": 2, "question": 1, "network": 1, "parameterises": 1, "cumulant": 1, "discount": 1, "function": 1, "define": 1, "gvfs": 1, "base": 1, "effect": 1, "metaloss": 1, "next": 1, "make": 1, "step": 1, "explicit": 1}, {"inner": 1, "update": 2, "include": 1, "two": 1, "components": 1, "first": 1, "canonical": 1, "deep": 1, "reinforcement": 1, "learn": 1, "use": 1, "loss": 1, "denote": 1, "lrl": 1, "optimize": 1, "maintask": 1, "policy": 1, "": 1, "either": 1, "directly": 1, "policybased": 1, "algorithms": 2, "eg": 2, "williams": 1, "1992": 1, "indirectly": 1, "valuebased": 1, "watkins": 1, "1989": 1}, {"second": 1, "component": 1, "update": 1, "rule": 1, "estimate": 1, "answer": 1, "gvfbased": 1, "question": 1}, {"slight": 1, "abuse": 1, "notation": 1, "denote": 2, "innerloop": 1, "update": 1, "follow": 1, "gradient": 1, "descent": 1, "step": 1, "pseudo": 1, "losses": 1, "lrl": 2, "lans": 2, "": 6, "tk": 1, "tk1": 5, "0": 2}, {"2": 1, "": 2, "meta": 1, "loss": 1, "sum": 1, "rl": 1, "pseudo": 1, "losses": 1, "associate": 1, "main": 1, "task": 1, "update": 2, "compute": 1, "batch": 1, "generate": 1, "inner": 1, "loop": 1, "function": 1, "metaparameters": 1, "answer": 1}, {"therefore": 1, "compute": 1, "update": 1, "metaparameters": 1, "t1": 1, "": 6, "l": 1, "x": 1, "lrl": 1, "tk": 1}, {"3": 1, "": 2, "k1": 1, "metagradient": 1, "procedure": 1, "optimize": 2, "area": 1, "curve": 1, "temporal": 1, "span": 1, "define": 1, "inner": 1, "unroll": 1, "length": 1, "l": 1, "alternatively": 1, "metaloss": 1, "may": 1, "evaluate": 1, "last": 1, "batch": 1, "alone": 1, "final": 1, "performance": 1}, {"unless": 1, "specify": 1, "otherwise": 1, "use": 1, "area": 1, "curve": 1}, {"24": 1, "": 2, "actor": 1, "critic": 1, "agent": 2, "discovery": 1, "question": 1, "auxiliary": 1, "task": 1, "section": 1, "describe": 1, "concrete": 1, "instantiation": 1, "algorithm": 1, "context": 1, "actorcritic": 1, "reinforcement": 1, "learn": 1}, {"network": 4, "leave": 1, "figure": 1, "1": 2, "compose": 1, "three": 1, "modules": 1, "encoder": 1, "take": 1, "last": 1, "observations": 1, "oti1t": 1, "input": 1, "output": 1, "state": 4, "representation": 1, "xt": 3, "": 3, "2": 1, "main": 1, "task": 1, "give": 2, "estimate": 1, "policy": 1, "4": 1, "value": 1, "function": 1, "v": 1, "sutton": 1, "1988": 1, "3": 1, "answer": 2, "approximate": 1, "gvf": 1}, {"paper": 1, "function": 2, "": 2, "v": 1, "linear": 1, "state": 1, "xt": 1}, {"maintask": 1, "network": 1, "parameters": 1, "main": 1, "": 1, "affect": 1, "rl": 1, "component": 1, "update": 1, "define": 1, "equation": 1, "2": 1}, {"actorcritic": 1, "agent": 1, "main": 1, "union": 1, "parameters": 2, "v": 2, "state": 1, "value": 1, "": 2, "softmax": 1, "policy": 1}, {"therefore": 1, "update": 3, "main": 1, "lrl": 2, "": 25, "rl": 1, "sum": 1, "value": 2, "v": 5, "gvt": 1, "vxt": 2, "vx": 2, "policy": 1, "l": 1, "p": 1, "jw": 1, "j": 1, "log": 1, "xt": 1, "w": 2, "1": 2, "g": 1, "r": 1, "multistep": 1, "gt": 1, "tj1": 1, "tw": 1, "j0": 1, "truncate": 1, "return": 1, "use": 1, "agents": 1, "estimate": 1, "state": 1, "bootstrapping": 1, "step": 1}, {"answer": 1, "network": 1, "parameters": 1, "": 1, "instead": 1, "affect": 1, "second": 1, "term": 1, "update": 1, "equation": 1, "2": 1}, {"since": 1, "answer": 1, "estimate": 1, "onpolicy": 1, "": 2, "expect": 1, "cumulative": 1, "discount": 1, "sum": 1, "cumulants": 1, "may": 1, "use": 1, "generalize": 1, "temporal": 1, "difference": 1, "learn": 1, "algorithm": 1, "update": 1}, {"agents": 1, "vector": 1, "linear": 1, "function": 1, "state": 1, "therefore": 1, "gvf": 1, "prediction": 1, "yi": 3, "separately": 1, "parameterised": 1, "lans": 1, "parameters": 1, "may": 1, "write": 1, "": 5}, {"update": 1, "yi": 4, "": 4, "gt": 2, "xt": 2, "iy": 1, "multistep": 1, "truncate": 1, "discount": 1, "sum": 1, "cumulants": 1, "ui": 1, "time": 1, "onwards": 1}, {"main": 1, "task": 1, "update": 1, "notation": 1, "gyt": 1, "highlight": 1, "use": 1, "answer": 1, "network": 1, "estimate": 1, "yi": 1, "xt": 1, "": 2, "xtt": 1, "iy": 1, "bootstrap": 1, "fix": 1, "number": 1, "step": 1}, {"maintask": 1, "answernetwork": 1, "pseudo": 1, "losses": 1, "lrl": 1, "": 2, "lans": 1, "use": 2, "update": 1, "also": 1, "straightforwardly": 1, "instantiate": 2, "equation": 2, "2": 1, "parameters": 2, "enc": 1, "encoder": 1, "network": 2, "3": 1, "question": 1}, {"share": 1, "state": 1, "representation": 1, "enc": 2, "": 5, "explore": 1, "two": 1, "update": 1, "1": 1, "use": 2, "gradients": 2, "main": 1, "task": 1, "answer": 2, "network": 2, "ie": 1, "0": 3, "k1": 6, "lrl": 1, "lans": 1, "2": 1, "ans": 1, "l": 1}, {"use": 1, "maintask": 2, "answer": 1, "network": 1, "components": 1, "consistent": 1, "exist": 1, "literature": 1, "auxiliary": 1, "task": 1, "ignore": 1, "update": 1, "provide": 1, "stringent": 1, "test": 1, "whether": 1, "algorithm": 1, "capable": 1, "metalearning": 1, "question": 1, "drive": 1, "even": 1, "learn": 1, "adequate": 1, "state": 1, "representations": 1}, {"3": 1, "": 2, "experimental": 2, "setup": 2, "section": 1, "outline": 1, "include": 1, "environments": 1, "use": 1, "testbeds": 1, "high": 1, "level": 1, "agent": 1, "neural": 1, "network": 1, "architectures": 1}, {"refer": 1, "appendix": 1, "detail": 1}, {"31": 1, "": 3, "domains": 1, "puddleworld": 1, "domain": 2, "continuous": 1, "state": 2, "gridworld": 1, "degris": 1, "et": 1, "al": 1, "2012": 1, "space": 1, "2dimensional": 1, "position": 1, "0": 1, "12": 1}, {"agent": 2, "5": 1, "action": 3, "four": 2, "move": 1, "one": 1, "cardinal": 1, "directions": 1, "mean": 1, "offset": 2, "005": 1, "last": 1, "0": 1}, {"action": 2, "stochastic": 1, "effect": 1, "environment": 1, "step": 1, "uniform": 1, "noise": 1, "sample": 1, "range": 1, "0025": 2, "add": 1, "component": 1}, {"refer": 1, "degris": 1, "et": 1, "al": 1}, {"2012": 1, "detail": 1, "environment": 1}, {"collectobjects": 1, "domain": 1, "fourroom": 1, "gridworld": 1, "agent": 1, "reward": 1, "collect": 1, "two": 1, "object": 1, "right": 1, "order": 1}, {"agent": 1, "move": 1, "deterministically": 1, "one": 1, "four": 1, "cardinal": 1, "directions": 1}, {"episode": 1, "start": 1, "position": 1, "choose": 1, "randomly": 1}, {"locations": 1, "two": 1, "object": 1, "across": 1, "episodes": 1}, {"agent": 1, "receive": 1, "reward": 2, "1": 1, "pick": 2, "first": 2, "object": 2, "2": 1, "second": 1, "one": 1}, {"maximum": 1, "length": 1, "episode": 1, "40": 1}, {"atari": 2, "domain": 1, "game": 1, "design": 1, "challenge": 1, "fun": 1, "human": 1, "players": 1, "package": 1, "canonical": 1, "benchmark": 1, "rl": 1, "agents": 1, "arcade": 1, "learn": 1, "environment": 1, "bellemare": 1, "et": 4, "al": 4, "2013": 1, "mnih": 1, "2015": 2, "2016": 1, "schulman": 1, "2017": 1, "hessel": 1, "2018": 1}, {"summarize": 1, "result": 1, "benchmark": 1, "follow": 1, "common": 1, "approach": 1, "first": 1, "normalize": 1, "score": 2, "game": 1, "use": 1, "random": 1, "human": 1, "agents": 1, "van": 1, "hasselt": 1, "et": 1, "al": 1, "2016": 1}, {"32": 1, "": 2, "agents": 1, "gridworld": 1, "experiment": 1, "implement": 1, "metagradients": 1, "top": 1, "5step": 1, "actorcritic": 1, "agent": 1, "16": 1, "parallel": 1, "actor": 1, "thread": 1, "mnih": 1, "et": 1, "al": 1, "2016": 1}, {"atari": 1, "experiment": 1, "use": 1, "20step": 1, "impala": 1, "espeholt": 1, "et": 1, "al": 1, "2018": 1, "agent": 1, "200": 1, "distribute": 1, "actors": 1}, {"nonvisual": 1, "domain": 1, "5": 1, "": 1, "puddleworld": 1, "encoder": 1, "simple": 1, "mlp": 1, "two": 1, "fullyconnected": 1, "layer": 1}, {"domains": 1, "encoder": 1, "convolutional": 1, "neural": 1, "network": 1}, {"maintask": 1, "value": 1, "policy": 1, "answer": 1, "network": 1, "linear": 1, "function": 1, "state": 1, "xt": 1, "": 1}, {"gridworlds": 1, "question": 1, "network": 1, "output": 1, "set": 1, "cumulants": 1, "discount": 1, "factor": 1, "jointly": 1, "define": 1, "gvfs": 1, "handtuned": 1}, {"atari": 1, "experiment": 1, "question": 1, "network": 1, "output": 1, "cumulants": 1, "correspond": 1, "discount": 1}, {"experiment": 1, "report": 1, "score": 1, "curve": 1, "average": 1, "result": 1, "3": 1, "independent": 1, "run": 1, "agent": 1, "task": 1, "hyperparameter": 1, "configuration": 1}, {"atari": 1, "use": 1, "single": 1, "set": 1, "hyperparameters": 1, "across": 1, "game": 1}, {"33": 1, "": 2, "baselines": 1, "handcraft": 1, "question": 1, "auxiliary": 2, "task": 2, "experiment": 1, "consider": 1, "follow": 1, "baseline": 1, "literature": 1}, {"reward": 1, "prediction": 1, "baseline": 1, "agent": 1, "question": 1, "network": 1}, {"instead": 1, "use": 1, "scalar": 1, "reward": 1, "obtain": 1, "next": 1, "time": 1, "step": 1, "target": 1, "answer": 1, "network": 1}, {"auxiliary": 1, "task": 1, "loss": 1, "function": 1, "": 5, "2": 1, "reward": 1, "prediction": 1, "baseline": 1, "lans": 1, "yt": 1, "xt": 1, "rt1": 1}, {"pixel": 1, "control": 1, "baseline": 1, "also": 1, "question": 1, "network": 1}, {"auxiliary": 1, "task": 1, "learn": 1, "optimally": 1, "control": 1, "change": 1, "pixel": 1, "intensities": 1}, {"specifically": 1, "answer": 1, "network": 1, "must": 1, "estimate": 1, "optimal": 1, "action": 1, "value": 1, "cumulants": 1, "ci": 1, "correspond": 1, "average": 1, "absolute": 1, "change": 1, "pixel": 1, "intensities": 1, "consecutive": 1, "time": 1, "observations": 1, "cell": 1, "n": 2, "": 1, "nonoverlapping": 1, "grid": 1, "overlay": 1, "onto": 1, "observation": 1}, {"auxiliary": 1, "loss": 1, "function": 1, "action": 1, "value": 1, "ith": 2, "cell": 2, "lans": 1, "": 7, "21": 1, "esas0": 1, "gci": 2, "maxa0": 1, "qi": 2, "s0": 1, "a0": 1, "a2": 1, "refer": 1, "discount": 1, "sum": 1, "p": 1, "pseudorewards": 1}, {"auxiliary": 1, "loss": 1, "sum": 1, "entire": 1, "grid": 1, "lans": 2, "": 2}, {"random": 1, "question": 2, "baseline": 1, "agent": 2, "metagradient": 1, "base": 1, "except": 1, "network": 1, "keep": 1, "fix": 1, "randomly": 1, "initialize": 1, "parameters": 1, "train": 1}, {"answer": 1, "network": 2, "still": 1, "train": 1, "predict": 1, "value": 1, "cumulants": 1, "define": 1, "fix": 1, "question": 1}, {"4": 1, "": 2, "empirical": 1, "find": 1, "section": 2, "empirically": 1, "investigate": 1, "performance": 1, "propose": 1, "algorithm": 1, "discovery": 1, "instantiate": 1, "24": 1}, {"refer": 1, "metalearning": 1, "agent": 2, "discover": 1, "gvfs": 1}, {"experiment": 1, "address": 1, "follow": 1, "question": 1, "1": 1}, {"metagradients": 1, "discover": 1, "gvfquestions": 1, "learn": 1, "answer": 1, "sufficient": 1, "build": 1, "representations": 1, "good": 1, "enough": 1, "solve": 1, "complex": 1, "rl": 1, "task": 1}, {"refer": 1, "representation": 1, "learn": 1, "experiment": 1}, {"2": 1}, {"metagradients": 1, "discover": 1, "gvfs": 1, "question": 1, "learn": 1, "answer": 1, "along": 1, "side": 1, "main": 1, "task": 1, "improve": 1, "data": 1, "efficiency": 1, "rl": 1, "agent": 1}, {"experiment": 2, "representation": 1, "shape": 1, "update": 2, "base": 1, "discover": 1, "gvfs": 1, "well": 1, "main": 1, "task": 1, "thus": 1, "refer": 1, "joint": 1, "learn": 1}, {"3": 1}, {"settings": 1, "auxiliary": 1, "task": 2, "discover": 1, "via": 1, "metagradients": 1, "compare": 1, "handcraft": 1, "literature": 1}, {"also": 1, "performance": 1, "affect": 1, "design": 1, "decisions": 1, "number": 2, "question": 1, "inner": 1, "step": 1, "use": 1, "compute": 1, "metagradients": 1, "choice": 1, "area": 1, "curve": 1, "versus": 1, "final": 1, "loss": 1, "metaobjective": 1}, {"note": 1, "representation": 1, "learn": 2, "experiment": 2, "stringent": 1, "test": 1, "metalearning": 1, "algorithm": 1, "discovery": 1, "compare": 1, "joint": 1}, {"however": 1, "latter": 1, "consistent": 1, "literature": 1, "auxiliary": 1, "task": 1, "useful": 1, "practice": 1}, {"41": 1, "": 2, "representation": 1, "learn": 1, "experiment": 2, "parameters": 1, "encoder": 1, "network": 1, "unaffected": 1, "gradients": 1, "maintask": 1, "update": 1}, {"figure": 1, "2": 1, "3": 1, "compare": 1, "performance": 1, "metagradient": 1, "agents": 2, "baseline": 1, "train": 1, "state": 1, "representation": 1, "use": 1, "handcraft": 1, "auxiliary": 1, "task": 1, "describe": 1, "section": 1, "33": 1}, {"always": 1, "include": 1, "reference": 1, "curve": 1, "black": 1, "correspond": 1, "baseline": 1, "actorcritic": 1, "agent": 1, "answer": 1, "question": 1, "network": 1, "representation": 1, "train": 1, "directly": 1, "use": 1, "maintask": 1, "update": 1}, {"report": 2, "result": 1, "collectobjects": 1, "domain": 1, "puddleworld": 1, "three": 1, "atari": 1, "game": 1, "appendix": 1}, {"experiment": 1, "highlight": 1, "follow": 1, "6": 1, "": 1, "figure": 1, "2": 1, "mean": 1, "return": 1, "collectobjects": 1, "leave": 1, "puddleworld": 1, "right": 1, "discover": 1, "gvfs": 2, "agent": 1, "red": 1, "alongside": 1, "therandom": 1, "blue": 1, "reward": 1, "prediction": 1, "purple": 1, "baselines": 1}, {"dash": 1, "black": 1, "line": 1, "final": 1, "performance": 1, "actorcritic": 1, "whose": 1, "representation": 1, "train": 1, "use": 1, "main": 1, "task": 1, "update": 1}, {"figure": 1, "3": 2, "mean": 1, "episode": 1, "return": 1, "atari": 1, "domains": 1, "discover": 1, "gvfs": 2, "agent": 1, "red": 1, "alongside": 1, "random": 1, "blue": 1, "reward": 1, "prediction": 1, "purple": 1, "pixel": 1, "control": 1, "green": 1, "baselines": 1}, {"dash": 1, "black": 1, "line": 1, "final": 1, "performance": 1, "actorcritic": 1, "whose": 1, "representation": 1, "train": 1, "main": 1, "task": 1, "update": 1}, {"figure": 1, "4": 1, "mean": 1, "episode": 1, "return": 1, "3": 1, "atari": 1, "domains": 1, "two": 1, "discover": 1, "gvfs": 1, "agents": 1, "optimize": 1, "sum": 1, "metaloss": 2, "red": 1, "end": 1, "orange": 1, "respectively": 1}, {"dash": 1, "black": 1, "line": 1, "final": 1, "performance": 1, "actorcritic": 1, "whose": 1, "representation": 1, "train": 1, "main": 1, "task": 1, "update": 1}, {"figure": 1, "5": 1, "parameter": 1, "study": 1, "collectobjects": 1, "discover": 1, "gvfs": 1, "agent": 1, "function": 1, "number": 2, "question": 1, "use": 1, "auxiliary": 1, "task": 1, "leave": 1, "step": 1, "unroll": 1, "compute": 1, "metagradient": 1, "right": 1}, {"dash": 1, "solid": 1, "red": 1, "line": 1, "correspond": 1, "final": 1, "average": 1, "episode": 1, "return": 1, "respectively": 1}, {"7": 1, "": 1, "discovery": 1, "domains": 1, "find": 1, "evidence": 1, "state": 1, "representation": 1, "learn": 3, "solely": 1, "gvfanswers": 1, "discover": 1, "question": 1, "sufficient": 1, "support": 1, "good": 1, "policies": 1}, {"specifically": 1, "two": 1, "gridworld": 1, "domains": 2, "result": 2, "policies": 2, "optimal": 1, "see": 2, "figure": 2, "2": 1, "atari": 1, "comparable": 1, "achieve": 1, "state": 1, "art": 1, "impala": 1, "agent": 1, "train": 1, "200m": 1, "frame": 1, "3": 1}, {"one": 1, "main": 1, "result": 1, "confirm": 1, "nonmyopic": 1, "metagradients": 1, "discover": 1, "question": 1, "form": 1, "cumulants": 1, "discount": 1, "useful": 1, "capture": 1, "rich": 1, "enough": 1, "knowledge": 1, "world": 1, "support": 1, "learn": 1, "staterepresentations": 1, "yield": 1, "good": 1, "policies": 1, "even": 1, "complex": 1, "rl": 1, "task": 1}, {"baselines": 1, "also": 1, "find": 1, "learn": 2, "answer": 2, "question": 2, "discover": 1, "use": 1, "metagradients": 1, "result": 2, "state": 1, "representations": 2, "support": 1, "better": 1, "performance": 1, "main": 1, "task": 1, "compare": 1, "popular": 1, "handcraft": 1, "literature": 1}, {"consider": 1, "gridworld": 1, "experiment": 1, "figure": 1, "2": 1, "learn": 2, "representation": 1, "use": 1, "reward": 1, "prediction": 1, "purple": 1, "random": 1, "gvfs": 2, "blue": 1, "result": 1, "notably": 1, "worse": 1, "policies": 1, "agent": 1, "discover": 1}, {"similarly": 1, "atari": 1, "show": 1, "figure": 1, "3": 1, "handcraft": 1, "auxiliary": 1, "task": 1, "include": 1, "pixel": 1, "control": 1, "baseline": 1, "green": 1, "result": 1, "almost": 1, "learn": 1}, {"maintask": 2, "drive": 1, "representations": 1, "note": 1, "actorcritic": 1, "agent": 1, "train": 2, "state": 1, "representation": 2, "use": 2, "update": 1, "directly": 1, "learn": 1, "faster": 1, "agents": 1, "exclusively": 1, "auxiliary": 1, "task": 1}, {"baseline": 1, "require": 1, "3m": 1, "step": 1, "gridworlds": 1, "200m": 1, "frame": 1, "atari": 1, "reach": 1, "final": 1, "performance": 1}, {"expect": 1, "true": 1, "metagradient": 1, "solution": 1, "well": 1, "auxiliary": 1, "task": 1, "literature": 1}, {"use": 1, "representation": 1, "learn": 1, "set": 1, "investigate": 1, "number": 1, "design": 1, "choices": 1}, {"first": 1, "compare": 1, "optimize": 1, "area": 1, "curve": 1, "length": 1, "unroll": 1, "metagradient": 2, "computation": 1, "sum": 1, "metaloss": 2, "compute": 1, "last": 1, "batch": 1, "alone": 1, "end": 1}, {"show": 1, "figure": 1, "4": 1, "approach": 1, "effective": 1, "find": 1, "optimize": 1, "area": 1, "curve": 1, "stable": 1}, {"next": 1, "examine": 1, "role": 1, "number": 2, "gvf": 1, "question": 1, "effect": 1, "vary": 1, "step": 1, "unroll": 1, "metagradient": 1, "calculation": 1}, {"purpose": 1, "use": 1, "less": 1, "computeintensive": 1, "gridworlds": 1, "collectobjects": 1, "report": 1, "puddleworld": 1, "appendix": 1}, {"leave": 1, "figure": 1, "5": 1, "report": 1, "parameter": 1, "study": 1, "plot": 1, "performance": 2, "agent": 1, "metalearned": 1, "auxiliary": 1, "task": 1, "function": 1, "number": 1, "question": 1, "dash": 1, "black": 1, "line": 1, "correspond": 1, "optimal": 1, "final": 1}, {"question": 1, "": 2, "2": 2, "provide": 1, "enough": 1, "signal": 1, "learn": 1, "good": 1, "representations": 1, "dash": 1, "red": 1, "line": 1, "thus": 1, "far": 1, "optimal": 1}, {"value": 1, "lead": 1, "learn": 1, "good": 1, "representation": 1, "capable": 1, "support": 1, "optimal": 1, "policy": 1}, {"however": 1, "many": 1, "question": 1, "eg": 1}, {"": 1, "128": 1, "make": 1, "learn": 1, "slower": 1, "show": 1, "average": 1, "performance": 1, "drop": 1}, {"number": 1, "question": 1, "therefore": 1, "important": 1, "hyperparameter": 1, "algorithm": 1}, {"right": 1, "figure": 1, "5": 1, "report": 1, "effect": 1, "performance": 1, "number": 1, "k": 1, "unroll": 1, "step": 1, "use": 1, "metagradient": 1, "computation": 1}, {"use": 1, "k": 1, "": 1, "1": 1, "correspond": 1, "myopic": 1, "metagradient": 1, "contrast": 1, "previous": 1, "work": 1, "xu": 1, "et": 1, "al": 1}, {"2018": 1, "zheng": 1, "et": 1, "al": 1}, {"2018": 1, "representation": 1, "learn": 1, "k": 2, "": 2, "1": 1, "2": 1, "insufficient": 1, "final": 1, "policy": 1, "anything": 1, "meaningful": 1}, {"performance": 1, "generally": 1, "get": 1, "better": 1, "increase": 2, "unroll": 1, "length": 1, "although": 1, "computational": 1, "cost": 1, "metagradients": 1, "also": 1}, {"trend": 1, "fully": 1, "monotonic": 1, "largest": 1, "unroll": 1, "length": 1, "k": 2, "": 2, "50": 1, "perform": 1, "worse": 1, "25": 1, "term": 1, "final": 1, "average": 1, "performance": 1}, {"conjecture": 1, "may": 1, "due": 1, "increase": 2, "variance": 1, "metagradient": 1, "estimate": 1, "unroll": 1, "length": 1}, {"number": 1, "unroll": 1, "step": 1, "k": 1, "therefore": 1, "also": 1, "sensitive": 1, "hyperparameter": 1}, {"note": 1, "neither": 1, "k": 2, "tune": 1, "experiment": 1, "result": 1, "use": 1, "fix": 1, "settings": 1, "": 2, "128": 1, "10": 1}, {"42": 1, "": 2, "joint": 1, "learn": 2, "experiment": 2, "next": 1, "set": 2, "use": 2, "common": 1, "literature": 1, "auxiliary": 2, "task": 3, "representation": 1, "jointly": 1, "update": 2, "main": 1}, {"accelerate": 1, "learn": 2, "useful": 1, "question": 3, "provide": 1, "encode": 2, "state": 1, "representation": 1, "input": 1, "network": 3, "instead": 1, "separate": 1, "differ": 1, "previous": 1, "experiment": 1, "completely": 1, "independent": 1, "consistently": 1, "objective": 1, "stringent": 1, "evaluation": 1, "algorithm": 1}, {"use": 1, "benchmark": 1, "consist": 1, "57": 1, "distinct": 1, "atari": 1, "game": 1, "evaluate": 1, "discover": 1, "gvfs": 1, "agent": 1, "together": 1, "actorcritic": 1, "baseline": 1, "impala": 1, "two": 1, "auxiliary": 1, "task": 1, "literature": 1, "reward": 1, "prediction": 1, "pixel": 1, "control": 1}, {"8": 1, "": 1, "figure": 1, "6": 1, "leave": 1, "relative": 1, "performance": 1, "improvements": 1, "discover": 1, "gvf": 1, "agent": 1, "plain": 1, "impala": 1}, {"10": 1, "game": 1, "pixel": 1, "control": 1, "baseline": 1, "show": 1, "largest": 1, "gain": 1, "impala": 1}, {"right": 1, "plot": 1, "median": 1, "normalize": 1, "score": 1, "agents": 1, "different": 1, "subsets": 1, "57": 2, "atari": 1, "game": 1, "n5": 1, "10": 1, "20": 1, "40": 1}, {"order": 1, "inclusion": 1, "game": 1, "determine": 1, "accord": 1, "performance": 1, "gain": 1, "pixelcontrol": 1}, {"none": 1, "auxiliary": 1, "task": 1, "outperform": 1, "impala": 1, "every": 1, "57": 1, "game": 1}, {"analyse": 1, "result": 1, "rank": 1, "game": 2, "accord": 1, "performance": 2, "agent": 1, "pixelcontrol": 1, "question": 1, "identify": 1, "conducive": 1, "improve": 1, "use": 1, "auxiliary": 1, "task": 1}, {"leave": 1, "figure": 1, "6": 2, "report": 1, "relative": 1, "gain": 3, "discover": 1, "gvfs": 1, "agent": 1, "impala": 1, "top10": 1, "game": 2, "pixel": 1, "control": 1, "baseline": 1, "observe": 1, "large": 1, "10": 1, "small": 1, "2": 2, "losses": 1}, {"right": 1, "figure": 1, "6": 1, "provid": 1, "comprehensive": 1, "view": 1, "performance": 1, "agents": 1}, {"number": 1, "n": 2, "xaxis": 1, "": 1, "5": 1, "10": 1, "20": 1, "40": 1, "57": 1, "present": 1, "median": 1, "human": 1, "normalize": 1, "score": 1, "achieve": 1, "method": 1, "topn": 1, "game": 1, "select": 1, "accord": 1, "pixel": 1, "control": 1, "baseline": 1}, {"visually": 1, "clear": 1, "discover": 1, "question": 2, "via": 1, "metalearning": 1, "fast": 1, "enough": 1, "compete": 1, "handcraft": 1, "game": 1, "well": 1, "suit": 1, "auxiliary": 1, "task": 1, "greatly": 1, "improve": 1, "performance": 1, "baselines": 1}, {"particularly": 1, "impressive": 1, "find": 1, "metagradient": 1, "solution": 1, "outperform": 1, "pixel": 1, "control": 1, "game": 2, "despite": 1, "rank": 1, "bias": 1, "favour": 1, "pixelcontrol": 1}, {"reward": 1, "prediction": 1, "baseline": 2, "interest": 1, "comparison": 1, "profile": 1, "closest": 1, "actorcritic": 1, "never": 1, "improve": 1, "performance": 1, "significantly": 1, "hurt": 1, "either": 1}, {"5": 1, "": 2, "conclusions": 1, "discussion": 1, "many": 1, "form": 1, "question": 1, "intelligent": 1, "agent": 1, "may": 1, "want": 1, "discover": 1}, {"paper": 1, "introduce": 1, "novel": 1, "efficient": 1, "multistep": 1, "metagradient": 1, "procedure": 1, "discovery": 1, "question": 1, "form": 1, "onpolicy": 1, "gvfs": 1}, {"stringent": 1, "test": 1, "representation": 1, "learn": 4, "experiment": 1, "demonstrate": 1, "metagradient": 1, "approach": 1, "capable": 1, "discover": 1, "useful": 1, "question": 1, "answer": 1, "drive": 1, "state": 1, "representations": 1, "good": 1, "enough": 1, "support": 1, "main": 1, "reinforcement": 1, "task": 1}, {"furthermore": 1, "auxiliary": 1, "task": 1, "experiment": 1, "demonstrate": 1, "metalearning": 1, "base": 1, "discovery": 1, "approach": 1, "dataefficient": 1, "enough": 1, "compete": 1, "well": 1, "term": 1, "performance": 1, "many": 1, "case": 1, "even": 1, "outperform": 1, "handcraft": 1, "question": 1, "develop": 1, "prior": 1, "work": 1}, {"prior": 1, "work": 1, "auxiliary": 1, "task": 3, "rely": 1, "human": 1, "ingenuity": 1, "define": 1, "question": 2, "useful": 2, "shape": 1, "state": 1, "representation": 1, "use": 1, "give": 1, "hard": 1, "create": 1, "general": 1, "ie": 1, "apply": 1, "across": 1, "many": 1}, {"bellemare": 1, "et": 1, "al": 1}, {"2019": 1, "introduce": 1, "geometrical": 1, "perspective": 1, "understand": 1, "auxiliary": 1, "task": 1, "give": 1, "rise": 1, "good": 1, "representations": 1}, {"solution": 1, "differ": 1, "line": 1, "work": 1, "sidestep": 1, "question": 2, "design": 1, "good": 1, "auxiliary": 1, "metalearning": 1, "instead": 1, "directly": 1, "optimize": 1, "utility": 1, "context": 1, "give": 1, "task": 1}, {"approach": 1, "fit": 1, "general": 1, "trend": 1, "increasingly": 1, "rely": 1, "data": 1, "rather": 1, "human": 1, "design": 1, "inductive": 1, "bias": 1, "construct": 1, "effective": 1, "learn": 1, "algorithms": 1, "silver": 1, "et": 2, "al": 2, "2017": 1, "hessel": 1, "2019b": 1}, {"promise": 1, "direction": 1, "future": 1, "research": 1, "investigate": 1, "offpolicy": 1, "gvfs": 1, "policy": 2, "make": 1, "predictions": 1, "differ": 1, "maintask": 1}, {"also": 1, "note": 1, "approach": 1, "discovery": 1, "quite": 1, "general": 1, "could": 1, "extend": 1, "metalearning": 1, "kind": 1, "question": 1, "fit": 1, "canonical": 1, "gvf": 1, "formulation": 1, "see": 1, "van": 1, "hasselt": 1, "et": 1, "al": 1}, {"2019": 1, "one": 1, "class": 1, "predictive": 1, "question": 1}, {"finally": 1, "emphasize": 1, "unroll": 1, "multistep": 1, "metagradient": 1, "algorithm": 1, "likely": 1, "benefit": 1, "previous": 1, "applications": 2, "myopic": 2, "metagradients": 1, "well": 1, "possibly": 1, "open": 1, "discovery": 1, "approximation": 1, "would": 1, "fail": 1}, {"9": 1, "": 1, "acknowledgments": 1, "thank": 1, "john": 1, "holler": 1, "zeyu": 1, "zheng": 1, "many": 1, "useful": 1, "comment": 1, "discussions": 1}, {"work": 1, "author": 1, "university": 1, "michigan": 1, "support": 1, "grant": 2, "darpas": 1, "l2m": 1, "program": 1, "nsf": 1, "iis1526059": 1}, {"opinions": 1, "find": 1, "conclusions": 1, "recommendations": 1, "express": 1, "author": 1, "necessarily": 1, "reflect": 1, "view": 1, "sponsor": 1}, {"reference": 1, "maruan": 1, "alshedivat": 1, "trapit": 1, "bansal": 1, "yura": 1, "burda": 1, "ilya": 1, "sutskever": 1, "igor": 1, "mordatch": 1, "pieter": 1, "abbeel": 1}, {"continuous": 1, "adaptation": 1, "via": 1, "metalearning": 1, "nonstationary": 1, "competitive": 1, "environments": 1}, {"6th": 1, "international": 1, "conference": 2, "learn": 1, "representations": 1, "iclr": 1, "2018": 3, "vancouver": 1, "bc": 1, "canada": 1, "april": 1, "30": 1, "": 1, "may": 1, "3": 1, "track": 1, "proceed": 1}, {"marcin": 1, "andrychowicz": 1, "misha": 1, "denil": 1, "sergio": 1, "gomez": 1, "matthew": 1, "w": 1, "hoffman": 1, "david": 1, "pfau": 1, "tom": 1, "schaul": 1, "brendan": 1, "shillingford": 1, "nando": 1, "de": 1, "freitas": 1}, {"learn": 2, "gradient": 2, "descent": 2}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "pp": 1}, {"39813989": 1, "2016": 1}, {"marc": 1, "g": 1, "bellemare": 1, "yavar": 1, "naddaf": 1, "joel": 1, "veness": 1, "michael": 1, "bowl": 1}, {"arcade": 1, "learn": 1, "environment": 1, "evaluation": 1, "platform": 1, "general": 1, "agents": 1}, {"j": 1, "artif": 1}, {"intell": 1}, {"res": 1, "47253279": 1, "2013": 1}, {"marc": 1, "g": 1, "bellemare": 1, "dabney": 1, "robert": 1, "dadashi": 1, "adrien": 1, "ali": 1, "taga": 1, "pablo": 1, "samuel": 1, "castro": 1, "nicolas": 1, "le": 1, "roux": 1, "dale": 1, "schuurmans": 1, "tor": 1, "lattimore": 1, "clare": 1, "lyle": 1}, {"geometric": 1, "perspective": 1, "optimal": 1, "representations": 1, "reinforcement": 1, "learn": 1}, {"arxiv": 1, "preprint": 1, "arxiv190111530": 1, "2019": 1}, {"yutian": 1, "chen": 1, "matthew": 1, "w": 1, "hoffman": 1, "sergio": 1, "gomez": 1, "colmenarejo": 1, "misha": 1, "denil": 1, "timothy": 1, "p": 1, "lillicrap": 1, "nando": 1, "de": 1, "freitas": 1}, {"learn": 2, "global": 1, "optimization": 1, "black": 1, "box": 1, "function": 1}, {"arxiv": 1, "preprint": 1, "arxiv161103824": 1, "2016": 1}, {"thomas": 1, "degris": 1, "martha": 1, "white": 1, "richard": 1, "sutton": 1}, {"offpolicy": 1, "actorcritic": 1}, {"proceed": 1, "29th": 1, "international": 2, "coference": 1, "conference": 1, "machine": 1, "learn": 1, "pp": 1}, {"179186": 1}, {"omnipress": 1, "2012": 1}, {"yan": 1, "duan": 1, "marcin": 1, "andrychowicz": 1, "bradly": 1, "stadie": 1, "openai": 1, "jonathan": 1, "ho": 1, "jonas": 1, "schneider": 1, "ilya": 1, "sutskever": 1, "pieter": 1, "abbeel": 1, "wojciech": 1, "zaremba": 1}, {"oneshot": 1, "imitation": 1, "learn": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "pp": 1}, {"10871098": 1, "2017": 1}, {"lasse": 1, "espeholt": 1, "hubert": 1, "soyer": 1, "remi": 1, "munos": 1, "karen": 1, "simonyan": 1, "volodymyr": 1, "mnih": 1, "tom": 1, "ward": 1, "yotam": 1, "doron": 1, "vlad": 1, "firoiu": 1, "tim": 1, "harley": 1, "iain": 1, "dun": 1, "et": 1, "al": 1}, {"impala": 1, "scalable": 1, "distribute": 1, "deeprl": 1, "importance": 1, "weight": 1, "actorlearner": 1, "architectures": 1}, {"international": 1, "conference": 1, "machine": 1, "learn": 1, "pp": 1}, {"14061415": 1, "2018": 1}, {"benjamin": 1, "eysenbach": 1, "abhishek": 1, "gupta": 1, "julian": 1, "ibarz": 1, "sergey": 1, "levine": 1}, {"diversity": 1, "need": 1, "learn": 1, "skills": 1, "without": 1, "reward": 1, "function": 1}, {"arxiv": 1, "preprint": 1, "arxiv180206070": 1, "2018": 1}, {"chelsea": 1, "finn": 1, "pieter": 1, "abbeel": 1, "sergey": 1, "levine": 1}, {"modelagnostic": 1, "metalearning": 1, "fast": 1, "adaptation": 1, "deep": 1, "network": 1}, {"proceed": 1, "34th": 1, "international": 1, "conference": 1, "machine": 1, "learningvolume": 1, "70": 1, "pp": 1}, {"11261135": 1}, {"jmlr": 1}, {"org": 1, "2017": 1}, {"carlos": 1, "florensa": 1, "david": 1, "hold": 1, "xinyang": 1, "geng": 1, "pieter": 1, "abbeel": 1}, {"automatic": 1, "goal": 1, "generation": 1, "reinforcement": 1, "learn": 1, "agents": 1}, {"proceed": 1, "35th": 1, "international": 1, "conference": 1, "machine": 1, "learn": 1, "icml": 1, "2018": 1, "pp": 1}, {"15141523": 1, "2018": 1}, {"andreas": 1, "griewank": 1, "andrea": 1, "walther": 1}, {"evaluate": 1, "derivatives": 1, "principles": 1, "techniques": 1, "algorithmic": 1, "differentiation": 1, "volume": 1, "105": 1}, {"siam": 1, "2008": 1}, {"abhishek": 1, "gupta": 1, "benjamin": 1, "eysenbach": 1, "chelsea": 1, "finn": 1, "sergey": 1, "levine": 1}, {"unsupervised": 1, "metalearning": 1, "reinforcement": 1, "learn": 1}, {"arxiv": 1, "preprint": 1, "arxiv180604640": 1, "2018": 1}, {"matteo": 1, "hessel": 1, "joseph": 1, "modayil": 1, "hado": 1, "van": 1, "hasselt": 1, "tom": 1, "schaul": 1, "georg": 1, "ostrovski": 1, "dabney": 1, "dan": 1, "horgan": 1, "bilal": 1, "piot": 1, "mohammad": 1, "gheshlaghi": 1, "azar": 1, "david": 1, "silver": 1}, {"rainbow": 1, "combine": 1, "improvements": 1, "deep": 1, "reinforcement": 1, "learn": 1}, {"proceed": 1, "thirtysecond": 1, "aaai": 1, "conference": 1, "artificial": 1, "intelligence": 1, "pp": 1}, {"32153222": 1, "2018": 1}, {"10": 1, "": 1, "matteo": 1, "hessel": 1, "hubert": 1, "soyer": 1, "lasse": 1, "espeholt": 1, "wojciech": 1, "czarnecki": 1, "simon": 1, "schmitt": 1, "hado": 1, "van": 1, "hasselt": 1}, {"multitask": 1, "deep": 1, "reinforcement": 1, "learn": 1, "popart": 1}, {"proceed": 1, "aaai": 1, "conference": 1, "artificial": 1, "intelligence": 1, "330137963803": 1, "jul": 1}, {"2019a": 1}, {"doi": 1, "101609aaaiv33i0133013796": 1}, {"matteo": 1, "hessel": 1, "hado": 1, "van": 1, "hasselt": 1, "joseph": 1, "modayil": 1, "david": 1, "silver": 1}, {"inductive": 1, "bias": 1, "deep": 1, "reinforcement": 1, "learn": 1}, {"arxiv": 1, "preprint": 1, "arxiv190702908": 1, "2019b": 1}, {"kyle": 1, "hsu": 1, "sergey": 1, "levine": 1, "chelsea": 1, "finn": 1}, {"unsupervised": 1, "learn": 1, "via": 1, "metalearning": 1}, {"arxiv": 1, "preprint": 1, "arxiv181002334": 1, "2018": 1}, {"max": 1, "jaderberg": 1, "volodymyr": 1, "mnih": 1, "wojciech": 1, "marian": 1, "czarnecki": 1, "tom": 1, "schaul": 1, "joel": 1, "z": 1, "leibo": 1, "david": 1, "silver": 1, "koray": 1, "kavukcuoglu": 1}, {"reinforcement": 1, "learn": 1, "unsupervised": 1, "auxiliary": 1, "task": 1}, {"5th": 1, "international": 1, "conference": 1, "learn": 1, "representations": 1, "iclr": 1, "2017": 1}, {"ke": 1, "li": 1, "jitendra": 1, "malik": 1}, {"learn": 1, "optimize": 1}, {"5th": 1, "international": 1, "conference": 1, "learn": 1, "representations": 1, "iclr": 1, "2017": 1}, {"takaki": 1, "makino": 1, "toshihisa": 1, "takagi": 1}, {"online": 1, "discovery": 1, "temporaldifference": 1, "network": 1}, {"proceed": 1, "25th": 1, "international": 1, "conference": 1, "machine": 1, "learn": 1, "pp": 1}, {"632639": 1}, {"acm": 1, "2008": 1}, {"daniel": 1, "j": 1, "mankowitz": 1, "augustin": 1, "zdek": 1, "andre": 1, "barreto": 1, "dan": 1, "horgan": 1, "matteo": 1, "hessel": 1, "john": 1, "quan": 1, "junhyuk": 1, "oh": 1, "hado": 1, "van": 1, "hasselt": 1, "david": 1, "silver": 1, "tom": 1, "schaul": 1}, {"unicorn": 1, "continual": 1, "learn": 1, "universal": 1, "offpolicy": 1, "agent": 1}, {"arxiv": 1, "preprint": 1, "arxiv180208294": 1, "2018": 1}, {"piotr": 1, "mirowski": 1, "razvan": 1, "pascanu": 1, "fabio": 1, "viola": 1, "hubert": 1, "soyer": 1, "andy": 1, "ballard": 1, "andrea": 1, "banino": 1, "misha": 1, "denil": 1, "ross": 1, "goroshin": 1, "laurent": 1, "sifre": 1, "koray": 1, "kavukcuoglu": 1, "dharshan": 1, "kumaran": 1, "raia": 1, "hadsell": 1}, {"learn": 1, "navigate": 1, "complex": 1, "environments": 1}, {"5th": 1, "international": 1, "conference": 1, "learn": 1, "representations": 1, "iclr": 1, "2017": 1}, {"nikhil": 1, "mishra": 1, "mostafa": 1, "rohaninejad": 1, "xi": 1, "chen": 1, "pieter": 1, "abbeel": 1}, {"simple": 1, "neural": 1, "attentive": 1, "metalearner": 1}, {"6th": 1, "international": 1, "conference": 2, "learn": 1, "representations": 1, "iclr": 1, "2018": 3, "vancouver": 1, "bc": 1, "canada": 1, "april": 1, "30": 1, "": 1, "may": 1, "3": 1, "track": 1, "proceed": 1}, {"volodymyr": 1, "mnih": 1, "koray": 1, "kavukcuoglu": 1, "david": 1, "silver": 1, "andrei": 1, "rusu": 1, "joel": 1, "veness": 1, "marc": 1, "g": 1, "bellemare": 1, "alex": 1, "grave": 1, "martin": 1, "riedmiller": 1, "andreas": 1, "k": 1, "fidjeland": 1, "georg": 1, "ostrovski": 1, "et": 1, "al": 1}, {"humanlevel": 1, "control": 1, "deep": 1, "reinforcement": 1, "learn": 1}, {"nature": 1, "5187540529": 1, "2015": 1}, {"volodymyr": 1, "mnih": 1, "adria": 1, "puigdomenech": 1, "badia": 1, "mehdi": 1, "mirza": 1, "alex": 1, "grave": 1, "timothy": 1, "lillicrap": 1, "tim": 1, "harley": 1, "david": 1, "silver": 1, "koray": 1, "kavukcuoglu": 1}, {"asynchronous": 1, "methods": 1, "deep": 1, "reinforcement": 1, "learn": 1}, {"international": 1, "conference": 1, "machine": 1, "learn": 1, "pp": 1}, {"19281937": 1, "2016": 1}, {"anusha": 1, "nagabandi": 1, "chelsea": 1, "finn": 1, "sergey": 1, "levine": 1}, {"deep": 1, "online": 1, "learn": 1, "via": 1, "metalearning": 1, "continual": 1, "adaptation": 1, "modelbased": 1, "rl": 1}, {"arxiv": 1, "preprint": 1, "arxiv181207671": 1, "2018": 1}, {"sachin": 1, "ravi": 1, "hugo": 1, "larochelle": 1}, {"optimization": 1, "model": 1, "fewshot": 1, "learn": 1}, {"5th": 1, "international": 1, "conference": 1, "learn": 1, "representations": 1, "iclr": 1, "2017": 1}, {"martin": 1, "riedmiller": 1, "roland": 1, "hafner": 1, "thomas": 1, "lampe": 1, "michael": 1, "neunert": 1, "jonas": 1, "degrave": 1, "tom": 1, "van": 1, "de": 1, "wiele": 1, "vlad": 1, "mnih": 1, "nicolas": 1, "heess": 1, "jost": 1, "tobias": 1, "springenberg": 1}, {"learn": 1, "play": 1, "solve": 1, "sparse": 1, "reward": 1, "task": 1, "scratch": 1}, {"proceed": 1, "35th": 1, "international": 1, "conference": 1, "machine": 1, "learn": 1, "icml": 1, "pp": 1}, {"43414350": 1, "2018": 1}, {"matthew": 1, "schlegel": 1, "andrew": 1, "patterson": 1, "adam": 1, "white": 2, "martha": 1}, {"discovery": 1, "predictive": 1, "representations": 1, "network": 1, "general": 1, "value": 1, "function": 1, "2018": 1}, {"url": 1, "httpsopenreview": 1}, {"netforumidryzelgz0z": 1}, {"juergen": 1, "schmidhuber": 1, "jieyu": 1, "zhao": 1, "wiering": 1}, {"simple": 1, "principles": 1, "metalearning": 1}, {"technical": 1, "report": 1, "idsia": 1, "69123": 1, "1996": 1}, {"john": 1, "schulman": 1, "sergey": 1, "levine": 1, "philipp": 1, "moritz": 1, "michael": 1, "jordan": 1, "pieter": 1, "abbeel": 1}, {"trust": 1, "region": 1, "policy": 1, "optimization": 1}, {"arxiv": 1, "preprint": 1, "arxiv150205477": 1, "2015": 1}, {"john": 1, "schulman": 1, "filip": 1, "wolski": 1, "prafulla": 1, "dhariwal": 1, "alec": 1, "radford": 1, "oleg": 1, "klimov": 1}, {"proximal": 1, "policy": 1, "optimization": 1, "algorithms": 1}, {"arxiv": 1, "preprint": 1, "arxiv170706347": 1, "2017": 1}, {"11": 1, "": 1, "evan": 1, "shelhamer": 1, "parsa": 1, "mahmoudieh": 1, "max": 1, "argus": 1, "trevor": 1, "darrell": 1}, {"loss": 1, "reward": 1, "selfsupervision": 1, "reinforcement": 1, "learn": 1}, {"5th": 1, "international": 1, "conference": 1, "learn": 1, "representations": 1, "iclr": 1, "2017": 1}, {"silver": 1, "hubert": 1, "schrittwieser": 1, "antonoglou": 1, "lai": 1, "guez": 1, "lanctot": 1, "sifre": 1, "kumaran": 1, "graepel": 1, "lillicrap": 1, "simonyan": 1, "hassabis": 1}, {"master": 1, "chess": 1, "shogi": 1, "selfplay": 1, "general": 1, "reinforcement": 1, "learn": 1, "algorithm": 1}, {"arxiv": 1, "preprint": 1, "arxiv171201815": 1, "2017": 1}, {"satinder": 1, "singh": 1, "michael": 1, "r": 2, "jam": 1, "matthew": 1, "rudary": 1}, {"predictive": 1, "state": 1, "representations": 1, "new": 1, "theory": 1, "model": 1, "dynamical": 1, "systems": 1}, {"proceed": 1, "20th": 1, "conference": 1, "uncertainty": 1, "artificial": 1, "intelligence": 1, "pp": 1}, {"512519": 1}, {"auai": 1, "press": 1, "2004": 1}, {"jake": 1, "snell": 1, "kevin": 1, "swersky": 1, "richard": 1, "zemel": 1}, {"prototypical": 1, "network": 1, "fewshot": 1, "learn": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "pp": 1}, {"40774087": 1, "2017": 1}, {"bradly": 1, "c": 1, "stadie": 1, "ge": 1, "yang": 1, "rein": 1, "houthooft": 1, "xi": 1, "chen": 1, "yan": 1, "duan": 1, "yuhuai": 1, "wu": 1, "pieter": 1, "abbeel": 1, "ilya": 1, "sutskever": 1}, {"considerations": 1, "learn": 2, "explore": 1, "via": 1, "metareinforcement": 1}, {"arxiv": 1, "preprint": 1, "arxiv180301118": 1, "2018": 1}, {"richard": 1, "sutton": 1}, {"learn": 1, "predict": 1, "methods": 1, "temporal": 1, "differences": 1}, {"machine": 1, "learn": 1, "3": 1, "1944": 1, "1988": 1}, {"richard": 1, "sutton": 1, "andrew": 1, "g": 1, "barto": 1}, {"reinforcement": 1, "learn": 1, "introduction": 1}, {"mit": 1, "press": 1, "cambridge": 1, "2018": 1}, {"richard": 1, "sutton": 1, "brian": 1, "tanner": 1}, {"temporaldifference": 1, "network": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "pp": 1}, {"13771384": 1, "2005": 1}, {"richard": 1, "sutton": 1, "joseph": 1, "modayil": 1, "michael": 1, "delp": 1, "thomas": 1, "degris": 1, "patrick": 1, "pilarski": 1, "adam": 1, "white": 1, "doina": 1, "precup": 1}, {"horde": 1, "scalable": 1, "realtime": 1, "architecture": 1, "learn": 1, "knowledge": 1, "unsupervised": 1, "sensorimotor": 1, "interaction": 1}, {"10th": 1, "international": 1, "conference": 1, "autonomous": 1, "agents": 1, "multiagent": 1, "systemsvolume": 1, "2": 1, "pp": 1}, {"761768": 1}, {"international": 1, "foundation": 1, "autonomous": 1, "agents": 1, "multiagent": 1, "systems": 1, "2011": 1}, {"sebastian": 1, "thrun": 1, "lorien": 1, "pratt": 1}, {"learn": 2, "introduction": 1, "overview": 1}, {"learn": 2, "pp": 1}, {"317": 1}, {"springer": 1, "1998": 1}, {"hado": 1, "van": 1, "hasselt": 1, "arthur": 1, "guez": 1, "david": 1, "silver": 1}, {"deep": 1, "reinforcement": 1, "learn": 1, "double": 1, "qlearning": 1}, {"aaai": 1, "2016": 1}, {"hado": 1, "van": 1, "hasselt": 1, "john": 1, "quan": 1, "matteo": 1, "hessel": 1, "zhongwen": 1, "xu": 1, "diana": 1, "borsa": 1, "andre": 1, "barreto": 1}, {"general": 1, "nonlinear": 1, "bellman": 1, "equations": 1}, {"arxiv": 1, "preprint": 1, "arxiv190703687": 1, "2019": 1}, {"vivek": 1, "veeriah": 1, "junhyuk": 1, "oh": 1, "satinder": 1, "singh": 1}, {"manygoals": 1, "reinforcement": 1, "learn": 1}, {"arxiv": 1, "preprint": 1, "arxiv180609605": 1, "2018": 1}, {"c": 2, "j": 1, "h": 1, "watkins": 1}, {"learn": 1, "delay": 1, "reward": 1}, {"phd": 1, "thesis": 1, "kings": 1, "college": 1, "cambridge": 1, "england": 1, "1989": 1}, {"adam": 1, "white": 1}, {"develop": 1, "predictive": 1, "approach": 1, "knowledge": 1}, {"university": 1, "alberta": 1, "2015": 1}, {"olga": 1, "wichrowska": 1, "niru": 1, "maheswaranathan": 1, "matthew": 1, "w": 1, "hoffman": 1, "sergio": 1, "gomez": 1, "colmenarejo": 1, "misha": 1, "denil": 1, "nando": 1, "de": 1, "freitas": 1, "jascha": 1, "sohldickstein": 1}, {"learn": 1, "optimizers": 1, "scale": 1, "generalize": 1}, {"proceed": 1, "34th": 1, "international": 1, "conference": 1, "machine": 1, "learningvolume": 1, "70": 1, "pp": 1}, {"37513760": 1}, {"jmlr": 1}, {"org": 1, "2017": 1}, {"ronald": 1, "j": 1, "williams": 1}, {"simple": 1, "statistical": 1, "gradientfollowing": 1, "algorithms": 1, "connectionist": 1, "reinforcement": 1, "learn": 1}, {"mach": 1}, {"learn": 1, "834229256": 1, "may": 1, "1992": 1}, {"issn": 1, "08856125": 1, "doi": 1, "101007": 1, "bf00992696": 1}, {"zhongwen": 1, "xu": 1, "hado": 1, "p": 1, "van": 1, "hasselt": 1, "david": 1, "silver": 1}, {"metagradient": 1, "reinforcement": 1, "learn": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "pp": 1}, {"23962407": 1, "2018": 1}, {"zeyu": 1, "zheng": 1, "junhyuk": 1, "oh": 1, "satinder": 1, "singh": 1}, {"learn": 1, "intrinsic": 1, "reward": 1, "policy": 1, "gradient": 1, "methods": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "pp": 1}, {"46444654": 1, "2018": 1}, {"12": 1}]
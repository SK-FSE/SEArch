[{"plan": 2, "goalconditioned": 1, "policies": 1, "": 2, "soroush": 1, "nasiriany": 1, "vitchyr": 1, "h": 1, "pong": 1, "steven": 1, "lin": 1, "sergey": 1, "levine": 1, "university": 1, "california": 1, "berkeley": 1, "snasirianyvitchyrstevenlin598svlevineberkeleyedu": 1, "abstract": 1, "methods": 1, "solve": 1, "temporally": 1, "extend": 1, "sequential": 1, "decision": 1, "make": 1, "problems": 1, "compose": 1, "simple": 1, "behaviors": 1}, {"however": 1, "plan": 1, "require": 1, "suitable": 1, "abstractions": 1, "state": 1, "transition": 1, "typically": 1, "need": 1, "design": 1, "hand": 1}, {"contrast": 1, "modelfree": 1, "reinforcement": 1, "learn": 1, "rl": 1, "acquire": 1, "behaviors": 1, "lowlevel": 1, "input": 1, "directly": 1, "often": 1, "struggle": 1, "temporally": 1, "extend": 1, "task": 1}, {"utilize": 1, "reinforcement": 1, "learn": 1, "automatically": 1, "form": 1, "abstractions": 1, "need": 1, "plan": 1, "thus": 1, "obtain": 1, "best": 1, "approach": 1}, {"show": 1, "goalconditioned": 1, "policies": 1, "learn": 1, "rl": 1, "incorporate": 1, "plan": 1, "planner": 1, "focus": 1, "state": 2, "reach": 2, "rather": 1}, {"however": 1, "complex": 1, "state": 2, "observations": 1, "image": 1, "input": 1, "represent": 1, "valid": 1}, {"therefore": 1, "also": 1, "propose": 1, "use": 1, "latent": 2, "variable": 2, "model": 2, "compactly": 1, "represent": 1, "set": 1, "valid": 1, "state": 2, "planner": 1, "policies": 1, "provide": 2, "abstraction": 2, "action": 1}, {"compare": 1, "method": 2, "planningbased": 1, "modelfree": 1, "methods": 1, "find": 1, "significantly": 1, "outperform": 1, "prior": 1, "work": 1, "evaluate": 1, "imagebased": 1, "robot": 1, "navigation": 1, "manipulation": 1, "task": 1, "require": 1, "nongreedy": 1, "multistaged": 1, "behavior": 1}, {"1": 1, "": 2, "introduction": 1, "reinforcement": 1, "learn": 2, "acquire": 1, "complex": 1, "skills": 1, "direct": 1, "interaction": 1, "environment": 1, "sidestep": 1, "need": 1, "accurate": 1, "model": 1, "manual": 1, "engineer": 1}, {"however": 1, "complex": 1, "temporally": 1, "extend": 1, "sequential": 1, "decision": 1, "make": 1, "require": 1, "wellhoned": 1, "reactions": 1}, {"agents": 1, "generalize": 1, "effectively": 1, "new": 3, "situations": 1, "task": 1, "must": 1, "reason": 1, "consequences": 1, "action": 1, "solve": 1, "problems": 1, "via": 1, "plan": 1}, {"accomplish": 1, "entirely": 1, "modelfree": 2, "rl": 1, "often": 1, "prove": 1, "challenge": 1, "purely": 1, "learn": 1, "inherently": 1, "provide": 1, "temporal": 1, "compositionality": 1, "skills": 1}, {"plan": 2, "trajectory": 1, "optimization": 1, "algorithms": 1, "encode": 1, "temporal": 1, "compositionality": 1, "design": 1, "require": 1, "accurate": 1, "model": 1}, {"model": 4, "specify": 1, "manually": 1, "plan": 2, "powerful": 1, "learn": 1, "present": 2, "major": 1, "obstacles": 1, "complex": 1, "environments": 1, "highdimensional": 1, "observations": 2, "image": 1, "direct": 1, "prediction": 1, "future": 1, "difficult": 1, "problem": 1, "4": 1, "43": 1, "36": 1, "6": 1, "27": 1, "3": 1, "31": 1, "errors": 1, "accumulate": 1, "time": 1, "39": 1, "make": 1, "predictions": 1, "inaccurate": 1, "precisely": 1, "longhorizon": 1, "settings": 1, "need": 1, "compositionality": 1, "methods": 1}, {"obtain": 1, "benefit": 1, "temporal": 1, "compositionality": 1, "inherent": 1, "modelbased": 1, "plan": 1, "without": 1, "need": 1, "model": 1, "environment": 1, "lowest": 1, "level": 1, "term": 1, "time": 1, "state": 1, "representation": 1}, {"one": 1, "way": 1, "avoid": 1, "model": 1, "environment": 1, "detail": 1, "plan": 2, "abstractions": 1, "simplify": 1, "representations": 1, "state": 1, "transition": 1, "easier": 1, "construct": 1, "predictions": 1}, {"temporal": 1, "abstractions": 2, "allow": 2, "plan": 3, "coarser": 1, "time": 1, "scale": 1, "skip": 1, "highfrequency": 1, "detail": 1, "instead": 1, "higherlevel": 1, "subgoals": 1, "state": 1, "": 3, "equal": 1, "contribution": 1, "33rd": 1, "conference": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "neurips": 1, "2019": 1, "vancouver": 1, "canada": 1}, {"simpler": 1, "representation": 1, "state": 1}, {"make": 1, "model": 1, "plan": 1, "easier": 1}, {"paper": 1, "study": 1, "modelfree": 1, "rl": 1, "use": 1, "provide": 1, "abstraction": 1, "modelbased": 1, "planner": 1}, {"first": 1, "glance": 1, "might": 1, "seem": 1, "like": 1, "strange": 1, "proposition": 1, "since": 1, "modelfree": 1, "rl": 1, "methods": 1, "learn": 1, "value": 1, "function": 1, "policies": 1, "model": 1}, {"however": 1, "precisely": 1, "make": 1, "ideal": 1, "abstract": 1, "away": 1, "complexity": 1, "temporally": 1, "extend": 1, "task": 1, "highdimensional": 1, "observations": 2, "avoid": 1, "lowlevel": 2, "eg": 1, "pixellevel": 1, "prediction": 1, "modelfree": 1, "rl": 1, "acquire": 1, "behaviors": 1, "manipulate": 1, "without": 1, "need": 1, "predict": 1, "explicitly": 1}, {"leave": 1, "planner": 1, "free": 1, "operate": 1, "higher": 1, "level": 1, "abstraction": 1, "reason": 1, "capabilities": 1, "lowlevel": 1, "modelfree": 1, "policies": 1}, {"build": 1, "idea": 1, "propose": 1, "modelfree": 1, "plan": 1, "framework": 1}, {"temporal": 1, "abstraction": 1, "learn": 1, "lowlevel": 1, "goalconditioned": 1, "policies": 2, "use": 1, "value": 1, "function": 1, "implicit": 1, "model": 1, "planner": 1, "plan": 1, "goals": 1, "pass": 1}, {"goalconditioned": 1, "policies": 2, "train": 1, "reach": 1, "goal": 1, "state": 1, "provide": 1, "additional": 1, "input": 1, "24": 1, "55": 1, "53": 1, "48": 1}, {"principle": 1, "policies": 1, "solve": 1, "goalreaching": 1, "problem": 1, "practice": 1, "effectiveness": 1, "constrain": 1, "nearby": 1, "goals": 2, "longdistance": 1, "require": 1, "plan": 1, "tend": 1, "substantially": 1, "less": 1, "effective": 1, "illustrate": 1, "experiment": 1}, {"however": 1, "policies": 1, "train": 1, "together": 1, "value": 2, "function": 2, "actorcritic": 1, "algorithms": 1, "provide": 1, "indication": 1, "whether": 1, "particular": 1, "goal": 1, "reachable": 1}, {"planner": 1, "plan": 1, "intermediate": 1, "subgoals": 1, "use": 1, "goalconditioned": 1, "value": 1, "function": 1, "evaluate": 1, "reachability": 1}, {"major": 1, "challenge": 1, "setup": 1, "need": 1, "actually": 1, "optimize": 1, "subgoals": 1}, {"domains": 1, "highdimensional": 1, "observations": 1, "image": 2, "may": 1, "require": 1, "explicitly": 1, "optimize": 1, "pixels": 1}, {"optimization": 1, "challenge": 1, "realistic": 1, "image": 1, "": 2, "general": 1, "feasible": 1, "state": 2, "typically": 1, "form": 1, "thin": 1, "lowdimensional": 1, "manifold": 1, "within": 1, "larger": 1, "space": 1, "possible": 1, "observation": 1, "value": 1, "34": 1}, {"address": 1, "also": 1, "build": 1, "abstractions": 1, "state": 2, "observation": 1, "learn": 1, "compact": 1, "latent": 1, "variable": 1, "representation": 1, "make": 1, "feasible": 1, "optimize": 2, "goals": 1, "domains": 1, "highdimensional": 1, "observations": 1, "image": 2, "without": 1, "explicitly": 1, "pixels": 1}, {"learn": 2, "representation": 1, "allow": 1, "planner": 2, "determine": 1, "subgoals": 1, "actually": 1, "represent": 1, "feasible": 1, "state": 2, "goalconditioned": 1, "value": 1, "function": 1, "tell": 1, "whether": 1, "reachable": 1}, {"contribution": 1, "method": 1, "combine": 1, "modelfree": 1, "rl": 1, "shorthorizon": 1, "goalreaching": 1, "modelbased": 1, "plan": 1, "latent": 1, "variable": 1, "representation": 1, "subgoals": 1}, {"evaluate": 1, "method": 1, "temporally": 1, "extend": 1, "task": 1, "require": 1, "multistage": 1, "reason": 1, "handle": 1, "image": 1, "observations": 1}, {"lowlevel": 1, "goalreaching": 1, "policies": 1, "cannot": 1, "solve": 1, "task": 1, "effectively": 1, "plan": 1, "subgoals": 1, "therefore": 1, "benefit": 1, "temporal": 1, "compositionality": 1}, {"plan": 1, "without": 1, "state": 1, "representation": 1, "learn": 1, "also": 1, "fail": 1, "perform": 1, "task": 1, "optimize": 1, "directly": 1, "image": 1, "result": 1, "invalid": 1, "subgoals": 1}, {"contrast": 1, "method": 1, "call": 1, "latent": 2, "embeddings": 1, "abstract": 1, "plan": 1, "leap": 1, "able": 1, "successfully": 1, "determine": 1, "suitable": 1, "subgoals": 2, "search": 1, "representation": 1, "space": 1, "reach": 1, "via": 1, "modelfree": 1, "policy": 1}, {"2": 2, "": 2, "relate": 1, "work": 2, "goalconditioned": 1, "reinforcement": 1, "learn": 1, "study": 1, "number": 1, "prior": 1, "24": 1, "25": 1, "37": 1, "18": 1, "53": 1, "48": 1, "57": 1, "40": 1, "59": 1}, {"goalconditioned": 1, "methods": 1, "excel": 1, "train": 1, "policies": 1, "greedily": 1, "reach": 1, "goals": 1, "often": 1, "fail": 1, "solve": 1, "longhorizon": 1, "problems": 1}, {"rather": 1, "propose": 2, "new": 1, "goalconditioned": 2, "rl": 1, "method": 1, "use": 1, "policies": 1, "abstraction": 1, "plan": 1, "order": 1, "handle": 1, "task": 1, "longer": 1, "horizon": 1}, {"modelbased": 1, "plan": 1, "deep": 1, "reinforcement": 1, "learn": 1, "wellstudied": 1, "problem": 1, "context": 1, "lowdimensional": 1, "state": 1, "space": 1, "50": 1, "32": 1, "39": 1, "7": 1}, {"observations": 1, "highdimensional": 1, "image": 1, "model": 1, "errors": 1, "direct": 1, "prediction": 1, "compound": 1, "quickly": 1, "make": 1, "modelbased": 1, "rl": 1, "difficult": 1, "15": 1, "13": 1, "5": 1, "14": 1, "26": 1}, {"rather": 1, "plan": 2, "directly": 1, "image": 1, "observations": 1, "propose": 1, "temporallyabstract": 1, "level": 1, "utilize": 1, "goalconditioned": 1, "policies": 1}, {"number": 1, "paper": 1, "study": 1, "embed": 1, "highdimensional": 1, "observations": 1, "lowdimensional": 1, "latent": 1, "space": 1, "plan": 1, "60": 1, "16": 1, "62": 1, "22": 1, "29": 1}, {"method": 2, "also": 1, "plan": 3, "latent": 1, "space": 1, "additionally": 1, "use": 1, "modelfree": 1, "goalconditioned": 1, "policy": 1, "abstraction": 1, "allow": 1, "temporal": 1, "abstractions": 2, "rather": 1, "state": 1}, {"automatically": 1, "set": 1, "subgoals": 1, "lowlevel": 1, "goalreaching": 1, "policy": 1, "bear": 1, "resemblance": 1, "hierarchical": 1, "rl": 1, "prior": 1, "methods": 1, "use": 1, "modelfree": 1, "learn": 1, "top": 1, "goalconditioned": 1, "policies": 1, "10": 1, "61": 1, "2": 1, "": 1, "12": 1, "58": 1, "33": 1, "20": 1, "38": 1}, {"instead": 1, "use": 1, "planner": 1, "higher": 1, "level": 1, "method": 1, "flexibly": 1, "plan": 2, "solve": 1, "new": 1, "task": 1, "benefit": 1, "compositional": 1, "structure": 1}, {"method": 1, "build": 1, "temporal": 1, "difference": 1, "model": 1, "48": 1, "tdms": 1, "finitehorizon": 1, "goalconditioned": 1, "value": 1, "function": 1}, {"prior": 1, "work": 1, "tdms": 1, "use": 1, "together": 1, "singlestep": 1, "planner": 1, "optimize": 1, "single": 1, "goal": 1, "represent": 1, "lowdimensional": 1, "grind": 1, "truth": 1, "state": 2, "assumption": 1, "valid": 1, "48": 1}, {"also": 1, "use": 1, "tdms": 1, "implicit": 1, "model": 1, "contrast": 1, "prior": 1, "work": 1, "plan": 1, "multiple": 1, "subgoals": 1, "demonstrate": 1, "method": 1, "perform": 1, "temporally": 1, "extend": 1, "task": 1}, {"critically": 1, "method": 1, "also": 1, "learn": 1, "abstractions": 1, "state": 3, "make": 1, "plan": 1, "process": 1, "much": 1, "practical": 1, "require": 1, "assume": 1, "vectors": 1, "represent": 1, "feasible": 1}, {"plan": 2, "goalconditioned": 1, "value": 1, "function": 1, "also": 1, "study": 1, "discrete": 1, "number": 1, "predetermine": 1, "goals": 1, "30": 1, "skills": 1, "1": 1, "case": 1, "graphsearch": 1, "algorithms": 1, "use": 1}, {"paper": 1, "provide": 1, "concrete": 1, "instantiation": 1, "plan": 2, "goalconditioned": 1, "value": 1, "function": 1, "also": 1, "present": 1, "new": 1, "method": 1, "scale": 1, "approach": 1, "image": 1, "reside": 1, "lowerdimensional": 1, "manifold": 1}, {"lastly": 1, "note": 1, "number": 1, "paper": 1, "study": 2, "combine": 1, "modelfree": 3, "modelbased": 1, "methods": 1, "54": 2, "41": 2, "23": 2, "56": 2, "44": 2, "51": 2, "39": 2, "method": 1, "substantially": 1, "different": 1, "approach": 1, "use": 2, "policies": 1, "abstraction": 1, "plan": 1, "rather": 1, "model": 1, "planninginspired": 1, "architectures": 1, "21": 1, "accelerate": 1, "learn": 1}, {"3": 1, "": 8, "background": 1, "consider": 1, "finitehorizon": 1, "goalconditioned": 1, "markov": 1, "decision": 1, "process": 1, "mdp": 1, "define": 1, "tuple": 1, "g": 4, "p": 1, "r": 2, "tmax": 2, "0": 2, "set": 3, "state": 2, "goals": 1, "action": 1, "pst1": 1, "st": 1, "timeinvariant": 1, "unknown": 1, "dynamics": 1, "function": 2, "reward": 1, "maximum": 1, "horizon": 1, "initial": 1, "distribution": 2, "goal": 1}, {"objective": 1, "goalconditioned": 1, "rl": 1, "obtain": 1, "policy": 1, "": 12, "st": 3, "g": 4, "maximize": 1, "expect": 1, "sum": 1, "reward": 1, "ptmax": 1, "e": 1, "t0": 1, "rst": 1, "goal": 1, "sample": 2, "state": 1, "accord": 1, "s0": 1, "0": 1, "st1": 1, "pst1": 1}, {"consider": 1, "case": 1, "goals": 1, "reside": 1, "space": 1, "state": 2, "ie": 1, "g": 5, "": 12, "important": 1, "quantity": 1, "goalconditioned": 2, "mdps": 1, "value": 1, "function": 1, "v": 2, "predict": 1, "expect": 1, "sum": 1, "future": 1, "reward": 1, "give": 1, "current": 1, "goal": 1, "time": 1, "max": 1, "x": 1, "0": 1, "e": 1, "rst0": 1, "st": 1, "condition": 1}, {"t0": 1, "": 2, "keep": 1, "notation": 1, "unclutter": 1, "omit": 1, "dependence": 1, "v": 1}, {"various": 1, "timevarying": 1, "reward": 1, "function": 1, "use": 2, "temporal": 1, "difference": 1, "model": 1, "tdms": 1, "48": 1, "follow": 1, "form": 1, "rtdm": 1, "g": 2, "": 2, "tmax": 1, "ds": 1}, {"1": 1, "": 2, "indicator": 1, "function": 2, "distance": 1, "define": 1, "task": 1}, {"particular": 1, "choice": 1, "reward": 1, "function": 1, "give": 2, "tdm": 1, "follow": 1, "interpretation": 1, "state": 1, "close": 1, "goalconditioned": 1, "policy": 1, "": 1, "get": 1, "g": 2, "time": 1, "step": 1, "attempt": 1, "reach": 1}, {"tdms": 1, "thus": 2, "use": 1, "measure": 1, "reachability": 1, "quantify": 1, "close": 1, "another": 1, "state": 1, "policy": 1, "get": 1, "time": 1, "step": 1, "provide": 1, "temporal": 1, "abstraction": 1}, {"however": 1, "tdms": 1, "produce": 1, "reasonable": 1, "reachability": 1, "predictions": 1, "valid": 1, "goals": 2, "": 1, "resemble": 1, "kinds": 1, "state": 1, "tdm": 1, "train": 1}, {"important": 1, "limitation": 1, "require": 1, "us": 1, "also": 1, "utilize": 1, "state": 2, "abstractions": 1, "limit": 1, "search": 1, "valid": 1}, {"next": 1, "section": 1, "discuss": 1, "use": 1, "tdms": 1, "plan": 1, "framework": 1, "highdimensional": 1, "state": 1, "observations": 1, "image": 1}, {"4": 1, "": 2, "plan": 1, "goalconditioned": 1, "policies": 1, "aim": 1, "learn": 1, "model": 1, "solve": 1, "arbitrary": 1, "longhorizon": 1, "goal": 2, "reach": 1, "task": 1, "highdimensional": 1, "observation": 1, "space": 1, "image": 1}, {"modelfree": 1, "goalconditioned": 1, "reinforcement": 1, "learn": 1, "algorithm": 1, "could": 1, "principle": 1, "solve": 1, "problem": 1}, {"however": 1, "show": 1, "experiment": 1, "practice": 1, "methods": 1, "produce": 1, "overly": 1, "greedy": 1, "policies": 1, "accomplish": 1, "shortterm": 1, "goals": 2, "struggle": 1, "temporally": 1, "extend": 1}, {"instead": 1, "combine": 1, "3": 1, "": 1, "figure": 1, "1": 1, "summary": 1, "latent": 1, "embeddings": 1, "abstract": 1, "plan": 1, "leap": 1}, {"1": 1, "planner": 1, "give": 1, "goal": 1, "state": 1}, {"2": 1, "planner": 1, "plan": 1, "intermediate": 1, "subgoals": 1, "lowdimensional": 1, "latent": 1, "space": 1}, {"plan": 1, "latent": 1, "space": 1, "subgoals": 1, "correspond": 1, "valid": 1, "state": 1, "observations": 1}, {"3": 1, "goalconditioned": 1, "policy": 1, "try": 1, "reach": 1, "first": 1, "subgoal": 1}, {"t1": 1, "time": 1, "step": 2, "policy": 1, "replans": 1, "repeat": 1, "2": 1, "3": 1, "": 1, "goalconditioned": 1, "policies": 1, "train": 1, "achieve": 1, "subgoals": 2, "planner": 1, "decompose": 1, "longhorizon": 1, "goalreaching": 1, "task": 1, "k": 1, "shorter": 1, "horizon": 1}, {"specifically": 1, "planner": 1, "choose": 1, "k": 1, "subgoals": 1, "g1": 1, "": 2}, {"": 1}, {"": 1}, {"": 3, "gk": 1, "goalreaching": 1, "policy": 1, "attempt": 1, "reach": 1, "first": 2, "subgoal": 1, "g1": 1, "t1": 1, "time": 1, "step": 1, "move": 1, "onto": 1, "second": 1, "goal": 1, "g2": 1, "forth": 1, "show": 1, "figure": 1, "1": 1}, {"procedure": 1, "require": 1, "train": 1, "goalconditioned": 1, "policy": 1, "solve": 1, "shorthorizon": 1, "task": 1}, {"moreover": 1, "plan": 1, "appropriate": 1, "subgoals": 1, "agent": 1, "compose": 1, "previously": 1, "learn": 1, "goalreaching": 1, "behavior": 1, "solve": 1, "new": 1, "temporally": 1, "extend": 1, "task": 1}, {"success": 1, "approach": 1, "depend": 1, "heavily": 1, "choice": 1, "subgoals": 1}, {"section": 1, "outline": 1, "one": 1, "measure": 1, "quality": 1, "subgoals": 1}, {"address": 1, "issue": 1, "arise": 1, "optimize": 1, "subgoals": 1, "highdimensional": 1, "state": 1, "space": 1, "image": 1}, {"lastly": 1, "summarize": 1, "overall": 1, "method": 1, "provide": 1, "detail": 1, "implementation": 1}, {"41": 1, "": 2, "plan": 1, "subgoals": 3, "suitable": 1, "ones": 1, "reachable": 2, "planner": 1, "choose": 1, "subsequent": 1, "subgoal": 3, "give": 1, "previous": 1, "reach": 1, "goal": 2, "ensure": 1, "last": 1, "true": 1}, {"use": 1, "goalconditioned": 1, "policy": 1, "reach": 1, "goals": 1, "quantify": 1, "reachable": 1, "subgoals": 1}, {"one": 1, "natural": 1, "choice": 1, "use": 1, "goalconditioned": 1, "value": 1, "function": 1, "previously": 1, "discuss": 1, "provide": 1, "measure": 1, "reachability": 1}, {"particular": 1, "give": 1, "current": 1, "state": 1, "policy": 1, "reach": 1, "goal": 1, "g": 2, "time": 1, "step": 1, "v": 1, "": 1, "0": 1}, {"generally": 1, "give": 1, "k": 1, "intermediate": 1, "subgoals": 1, "g1k": 1, "": 3, "g1": 1}, {"": 1}, {"": 1}, {"": 4, "gk": 1, "k": 1, "1": 1, "time": 1, "intervals": 1, "t1": 1}, {"": 1}, {"": 1}, {"": 17, "tk1": 1, "sum": 1, "tmax": 1, "define": 1, "feasibility": 1, "vector": 1, "v": 2, "g1": 2, "t1": 1, "g2": 1, "t2": 1}, {"vs": 1, "g1k": 1, "": 5, "t1k1": 1, "g": 1}, {"": 17, "v": 2, "g": 4, "k1": 1, "gk": 2, "tk": 1, "tk1": 1, "feasibility": 3, "vector": 3, "provide": 1, "quantitative": 1, "measure": 2, "plan": 2, "first": 3, "element": 3, "describe": 2, "close": 2, "policy": 2, "reach": 2, "subgoal": 3, "g1": 1, "start": 2, "initial": 1, "state": 1, "second": 2, "g2": 1, "last": 1, "term": 1, "reachability": 1, "true": 1, "goal": 1, "create": 1, "feasible": 1, "would": 1, "like": 1, "zero": 1, "minimize": 1, "norm": 1, "lg1k": 1, "vs": 1, "g1k": 1, "t1k1": 1}, {"2": 2, "word": 1, "minimize": 1, "equation": 1, "search": 1, "subgoals": 1, "overall": 1, "path": 1, "feasible": 1, "terminate": 1, "true": 1, "goal": 1}, {"next": 1, "section": 1, "turn": 1, "optimize": 1, "equation": 1, "2": 1, "address": 1, "issue": 1, "arise": 1, "highdimensional": 1, "state": 1, "space": 1}, {"42": 1, "": 2, "optimize": 1, "image": 2, "consider": 1, "imagebased": 1, "environments": 1, "set": 2, "state": 1, "valid": 1, "observations": 1, "domain": 1}, {"imagebased": 1, "environments": 1, "solve": 1, "optimization": 1, "equation": 1, "2": 1, "present": 1, "two": 1, "4": 1, "": 1, "problems": 1}, {"first": 1, "optimization": 1, "variables": 1, "g1k": 1, "highdimensional": 1, "": 1, "even": 1, "64x64": 1, "image": 1, "3": 1, "subgoals": 1, "10000": 1, "dimension": 1}, {"second": 1, "perhaps": 1, "subtle": 1, "optimization": 1, "iterate": 1, "must": 1, "constrain": 1, "set": 1, "valid": 1, "image": 1, "observations": 1, "subgoals": 1, "correspond": 1, "meaningful": 1, "state": 1}, {"plethora": 1, "constrain": 1, "optimization": 1, "methods": 1, "exist": 1, "typically": 1, "require": 1, "know": 1, "set": 2, "valid": 1, "state": 1, "42": 1, "able": 1, "project": 1, "onto": 1, "46": 1}, {"imagebased": 1, "domains": 1, "set": 2, "state": 1, "unknown": 1, "rdimensional": 1, "manifold": 1, "embed": 1, "higherdimensional": 1, "space": 1, "rn": 1, "": 3, "n": 1, "r": 1, "34": 1, "ie": 1, "valid": 1, "image": 1, "observations": 1}, {"optimize": 2, "equation": 1, "2": 1, "would": 2, "much": 1, "easier": 1, "could": 1, "directly": 1, "r": 2, "dimension": 1, "underlie": 1, "representation": 1, "since": 2, "": 2, "n": 1, "crucially": 1, "worry": 1, "constrain": 1, "planner": 1, "unknown": 1, "manifold": 1}, {"may": 1, "know": 1, "set": 1, "priori": 1, "learn": 1, "latentvariable": 1, "model": 2, "compact": 1, "latent": 2, "space": 2, "capture": 1, "optimize": 1}, {"end": 1, "use": 1, "variationalautoencoder": 1, "vae": 1, "28": 1, "52": 1, "train": 1, "image": 1, "randomly": 1, "sample": 1, "environment": 1}, {"figure": 1, "2": 1, "optimize": 1, "directly": 1, "ima": 1, "vae": 1, "consist": 1, "encoder": 1, "q": 1, "z": 2, "": 2, "decoder": 1, "p": 1}, {"inference": 1, "network": 1, "map": 2, "highdimensional": 1, "state": 1, "": 1, "distribution": 1, "lowerdimensional": 1, "latent": 1, "variables": 1, "z": 2, "lower": 1, "dimensional": 1, "space": 1, "generative": 1, "model": 1, "reverse": 1}, {"moreover": 1, "vae": 1, "train": 1, "marginal": 1, "distribution": 2, "z": 1, "match": 1, "prior": 1, "p0": 1, "": 1, "standard": 1, "gaussian": 1}, {"age": 1, "manifold": 1, "b": 1, "challenge": 1, "generally": 1, "unknown": 1, "reside": 1, "highdimensional": 1, "space": 1}, {"optimize": 1, "latent": 1, "state": 1, "use": 1, "decoder": 1, "generate": 1, "image": 1}, {"long": 1, "latent": 2, "state": 2, "high": 1, "likelihood": 2, "prior": 1, "green": 1, "correspond": 1, "realistic": 1, "image": 1, "low": 1, "red": 1}, {"last": 1, "property": 1, "vaes": 1, "crucial": 1, "allow": 1, "us": 1, "tractably": 1, "optimize": 1, "manifold": 2, "valid": 2, "state": 2, "long": 1, "latent": 1, "variables": 1, "high": 1, "likelihood": 1, "prior": 1, "correspond": 1, "image": 1, "remain": 1, "inside": 1, "show": 1, "figure": 1, "2": 1}, {"fact": 1, "dai": 1, "wipf": 1, "9": 1, "show": 1, "vae": 1, "gaussian": 1, "prior": 1, "always": 1, "recover": 1, "true": 1, "manifold": 1, "make": 1, "choice": 1, "latentvariable": 1, "model": 1, "particularly": 1, "appeal": 1}, {"summary": 1, "rather": 1, "minimize": 2, "equation": 1, "2": 1, "require": 1, "optimize": 1, "highdimensional": 1, "unknown": 1, "space": 1, "k": 1, "": 33, "x": 1, "lleap": 1, "z1k": 3, "vs": 2, "t1k1": 2, "gp": 1, "log": 1, "pzk": 1, "3": 1, "k1": 1, "v": 2, "z1": 2, "t1": 1, "z2": 1, "t2": 1, "g": 1}, {"": 13, "v": 2, "z": 3, "k1": 1, "zk": 2, "tk": 1, "g": 1, "tk1": 1, "arg": 1, "max": 1, "p": 1, "g0": 1}, {"g0": 1, "": 3, "procedure": 1, "optimize": 1, "latent": 1, "variables": 1, "zk": 1, "map": 1, "onto": 1, "highdimensional": 1, "goal": 1, "state": 1, "gk": 1, "use": 1, "maximum": 1, "likelihood": 1, "estimate": 1, "mle": 1, "decoder": 1, "arg": 1, "maxg": 1, "g": 1, "z": 1}, {"case": 1, "mle": 1, "compute": 1, "close": 1, "form": 1, "take": 1, "mean": 1, "decoder": 1}, {"term": 2, "sum": 1, "log": 1, "pzk": 1, "": 2, "penalize": 1, "latent": 1, "variables": 1, "low": 1, "likelihood": 1, "prior": 1, "p": 1, "hyperparameter": 1, "control": 1, "importance": 1, "second": 1}, {"norm": 2, "could": 1, "use": 2, "": 1, "force": 1, "element": 1, "feasibility": 1, "vector": 1, "near": 1, "zero": 1}, {"find": 1, "": 1, "norm": 2, "outperform": 1, "1": 1, "force": 1, "sum": 1, "absolute": 1, "value": 1, "elements": 1, "near": 1, "zero": 1}, {"2": 1, "43": 1, "": 2, "goalconditioned": 2, "reinforcement": 2, "learn": 2, "algorithm": 1, "use": 1, "temporal": 1, "difference": 1, "model": 1, "tdms": 1, "48": 1}, {"tdms": 1, "learn": 1, "q": 1, "function": 2, "rather": 1, "v": 2, "compute": 1, "evaluate": 1, "2": 1, "": 1, "see": 1, "subsection": 1, "a1": 1, "comparison": 1}, {"5": 1, "": 3, "q": 1, "action": 1, "deterministic": 1, "policy": 1, "v": 1, "g": 2, "qs": 1, "tasgt": 1}, {"improve": 1, "efficiency": 1, "method": 1, "also": 1, "utilize": 1, "vae": 1, "use": 1, "recover": 1, "latent": 1, "space": 1, "plan": 1, "state": 1, "representation": 1, "tdms": 1}, {"could": 1, "train": 1, "reinforcement": 1, "learn": 3, "agents": 1, "scratch": 1, "expensive": 1, "term": 1, "sample": 1, "efficiency": 1, "much": 1, "focus": 1, "simply": 1, "good": 1, "convolution": 1, "filter": 1}, {"therefore": 1, "use": 1, "pretrained": 1, "meanencoder": 1, "vae": 1, "state": 1, "encoder": 1, "policy": 1, "value": 1, "function": 1, "network": 1, "train": 1, "additional": 1, "fullyconnected": 1, "layer": 1, "rl": 1, "top": 1, "representations": 1}, {"detail": 1, "architecture": 1, "provide": 1, "appendix": 1, "c": 1, "show": 1, "section": 1, "5": 1, "method": 1, "work": 1, "without": 1, "reuse": 2, "vae": 1, "meanencoder": 1, "parameter": 1, "primarily": 1, "help": 1, "increase": 1, "speed": 1, "learn": 1}, {"44": 1, "": 2, "summary": 1, "latent": 2, "embeddings": 2, "abstract": 2, "plan": 2, "overall": 1, "method": 1, "call": 1, "leap": 1, "summarize": 1, "algorithm": 1, "1": 1}, {"first": 1, "train": 1, "goalconditioned": 1, "policy": 1, "variationalautoencoder": 1, "randomly": 1, "collect": 1, "state": 1}, {"test": 1, "time": 1, "give": 1, "new": 1, "goal": 1, "choose": 1, "subgoals": 1, "minimize": 1, "equation": 1, "3": 1}, {"plan": 1, "choose": 1, "first": 1, "goal": 1, "z1": 1, "": 1, "give": 1, "policy": 1}, {"t1": 1, "step": 1, "repeat": 1, "procedure": 1, "produce": 1, "plan": 1, "k": 2, "": 1, "1": 1, "rather": 1, "subgoals": 1, "give": 1, "first": 1, "goal": 1, "policy": 1}, {"work": 1, "fix": 1, "time": 1, "intervals": 1, "evenly": 1, "space": 1, "ie": 1, "t1": 1, "": 2, "t2": 1}, {"": 1}, {"": 1}, {"tk1": 1, "": 2, "btmax": 1, "k": 1, "1c": 1, "additionally": 1, "optimize": 1, "time": 1, "intervals": 1, "would": 1, "promise": 1, "future": 1, "extension": 1}, {"algorithm": 1, "1": 2, "latent": 1, "embeddings": 1, "abstract": 1, "plan": 1, "leap": 1, "2": 1, "3": 1, "4": 1, "5": 1, "6": 1, "7": 1, "8": 1, "9": 1, "10": 1, "11": 1, "12": 1, "13": 1, "": 2, "train": 1, "vae": 1, "encoder": 1, "q": 1, "decoder": 1, "p": 1}, {"train": 1, "tdm": 1, "policy": 1, "": 2, "value": 1, "function": 1, "v": 1}, {"initialize": 1, "state": 1, "goal": 2, "time": 1, "s1": 1, "": 5, "0": 1, "g": 2, "1": 1}, {"assign": 1, "last": 1, "subgoal": 1, "true": 1, "goal": 1, "gk1": 1, "": 2, "g": 1, "k": 1, "1": 1}, {"": 1}, {"": 1}, {"": 4, "k": 1, "1": 1, "optimize": 1, "equation": 1, "3": 1, "choose": 1, "latent": 1, "subgoals": 1, "zk": 1}, {"": 1}, {"": 1}, {"": 4, "zk": 3, "use": 1, "v": 1, "p": 1, "k": 2, "decode": 1, "obtain": 1, "goal": 1, "gk": 1}, {"t0": 1, "1": 1, "": 1}, {"": 1}, {"": 1}, {"": 7, "tk": 2, "sample": 1, "next": 1, "action": 1, "use": 1, "goalconditioned": 1, "policy": 1, "st": 1, "gk": 1, "t0": 1}, {"execute": 1, "obtain": 1, "next": 1, "state": 1, "st1": 1, "increment": 1, "global": 1, "timer": 1, "": 5, "1": 2, "end": 2, "5": 1, "experiment": 2, "study": 1, "follow": 1, "two": 1, "question": 1, "leap": 1, "compare": 1, "modelbased": 1, "methods": 1, "directly": 2, "predict": 1, "time": 1, "step": 1, "modelfree": 1, "rl": 1, "optimize": 1, "final": 1, "goal": 1}, {"2": 1, "use": 1, "latent": 1, "state": 1, "representation": 1, "design": 1, "decisions": 1, "impact": 1, "performance": 1, "leap": 1}, {"51": 1, "": 2, "visionbased": 2, "comparison": 1, "result": 1, "study": 1, "first": 1, "question": 1, "two": 1, "distinct": 1, "task": 1, "require": 1, "temporallyextended": 1, "plan": 1, "handle": 1, "highdimensional": 1, "image": 1, "observations": 1}, {"first": 1, "task": 1, "2d": 1, "navigation": 1, "require": 1, "navigate": 1, "around": 1, "ushaped": 1, "wall": 1, "reach": 1, "goal": 1, "show": 1, "figure": 1, "3": 1}, {"state": 1, "observation": 1, "topdown": 1, "image": 1, "environment": 1}, {"use": 1, "task": 1, "conduct": 1, "ablation": 1, "study": 1, "test": 1, "component": 1, "leap": 1, "contribute": 1, "final": 1, "performance": 1}, {"also": 1, "use": 2, "environment": 1, "generate": 1, "visualizations": 1, "help": 1, "us": 1, "better": 1, "understand": 1, "method": 1, "goalconditioned": 1, "value": 1, "function": 1, "evaluate": 1, "reachability": 1, "image": 1}, {"visually": 1, "simple": 1, "task": 1, "far": 1, "trivial": 1, "goalconditioned": 1, "plan": 1, "methods": 1, "greedy": 1, "goalreaching": 1, "policy": 1, "move": 1, "directly": 1, "towards": 1, "goal": 2, "never": 1, "reach": 1}, {"agent": 1, "must": 1, "plan": 1, "temporallyextended": 1, "path": 1, "move": 2, "around": 1, "wall": 1, "sometimes": 1, "away": 1, "goal": 1}, {"also": 1, "use": 1, "environment": 1, "compare": 1, "method": 1, "prior": 1, "work": 1, "goalconditioned": 1, "modelbased": 1, "rl": 1}, {"6": 2, "": 25, "2d": 1, "navigation": 1, "leap": 1, "tdm25": 1, "tdm100": 1, "rig": 1, "pet": 1, "state": 1, "final": 2, "distance": 2, "goal": 2, "5": 2, "4": 1, "3": 2, "2": 1, "1": 1, "0": 4, "100": 1, "200": 1, "300": 1, "400": 1, "number": 2, "environment": 2, "step": 2, "total": 2, "x1000": 2, "push": 1, "reach": 1, "35": 1, "puck": 1, "cm": 1, "30": 1, "25": 1, "20": 1, "15": 1, "10": 1, "500": 2, "250": 1, "750": 1, "1000": 1, "1250": 1, "1500": 1, "1750": 1, "2000": 1, "figure": 1, "comparisons": 1, "two": 1, "visionbased": 1, "domains": 1, "evaluate": 1, "temporally": 1, "extend": 1, "control": 1, "illustrations": 1, "task": 1}, {"2d": 1, "navigation": 1, "leave": 1, "goal": 2, "navigate": 1, "around": 1, "ushaped": 1, "wall": 1, "reach": 1}, {"push": 2, "reach": 1, "manipulation": 1, "task": 1, "right": 1, "robot": 1, "must": 1, "first": 1, "puck": 1, "target": 1, "location": 3, "blue": 1, "star": 2, "may": 1, "require": 1, "move": 2, "hand": 3, "away": 1, "goal": 1, "another": 1, "red": 1}, {"curve": 1, "average": 1, "multiple": 1, "seed": 1, "shade": 1, "regions": 1, "represent": 1, "one": 1, "standard": 1, "deviation": 1}, {"method": 1, "show": 1, "red": 1, "outperform": 1, "prior": 1, "methods": 1, "task": 1}, {"push": 1, "reach": 1, "task": 1, "prior": 1, "methods": 1, "typically": 1, "get": 1, "hand": 1, "close": 1, "right": 1, "location": 1, "perform": 1, "much": 1, "worse": 1, "move": 1, "puck": 1, "indicate": 1, "overly": 1, "greedy": 1, "strategy": 1, "approach": 1, "succeed": 1}, {"evaluate": 1, "leap": 1, "complex": 1, "task": 2, "utilize": 1, "robotic": 1, "manipulation": 1, "simulation": 1, "push": 1, "reach": 1}, {"task": 1, "require": 1, "control": 1, "simulate": 1, "sawyer": 1, "robot": 1, "1": 1, "move": 2, "puck": 1, "target": 2, "location": 2, "2": 1, "end": 1, "effector": 1}, {"task": 1, "visually": 1, "complex": 1, "require": 1, "temporally": 1, "extend": 1, "reason": 1}, {"initial": 1, "arm": 3, "puck": 1, "locations": 1, "randomize": 1, "agent": 1, "must": 1, "decide": 1, "reposition": 1, "reach": 1, "around": 1, "object": 2, "push": 1, "desire": 1, "direction": 1, "move": 1, "correct": 1, "location": 1, "show": 1, "figure": 1, "3": 1}, {"common": 1, "failure": 1, "case": 1, "modelfree": 1, "policies": 1, "set": 1, "adopt": 1, "overly": 1, "greedy": 1, "strategy": 1, "move": 1, "arm": 1, "goal": 1, "ignore": 1, "puck": 1}, {"train": 1, "methods": 1, "randomly": 1, "initialize": 1, "goals": 1, "initial": 1, "state": 1}, {"however": 1, "evaluation": 1, "intentionally": 1, "select": 1, "difficult": 1, "start": 1, "goal": 1, "state": 1, "evaluate": 1, "longhorizon": 1, "reason": 1}, {"2d": 1, "navigation": 1, "initialize": 1, "policy": 1, "randomly": 1, "inside": 1, "center": 1, "square": 1, "sample": 1, "goal": 1, "region": 1, "directly": 1, "ushaped": 1, "wall": 1}, {"require": 1, "initially": 1, "move": 1, "away": 1, "goal": 1, "navigate": 1, "around": 1, "wall": 1}, {"push": 1, "reach": 1, "evaluate": 1, "5": 1, "distinct": 1, "challenge": 1, "configurations": 1, "require": 1, "agent": 1, "first": 1, "plan": 1, "move": 2, "puck": 2, "arm": 1, "desire": 1, "location": 1}, {"one": 1, "configuration": 1, "example": 1, "initialize": 1, "hand": 2, "puck": 2, "opposite": 1, "side": 2, "workspace": 1, "set": 1, "goals": 1, "must": 1, "switch": 1}, {"compare": 1, "method": 1, "modelfree": 1, "methods": 2, "modelbased": 1, "plan": 1, "learn": 1, "model": 1}, {"task": 1, "use": 2, "tmax": 1, "": 2, "100": 1, "leap": 1, "cem": 1, "optimize": 1, "k": 1, "3": 1, "subgoals": 1, "25": 1, "time": 1, "step": 1, "apart": 1}, {"compare": 1, "directly": 1, "modelfree": 1, "tdms": 1, "label": 1, "tdm25": 1}, {"since": 1, "task": 1, "evaluate": 1, "horizon": 1, "length": 1, "tmax": 2, "": 2, "100": 2, "also": 1, "compare": 1, "modelfree": 1, "tdm": 1, "policy": 1, "train": 1, "label": 1, "tdm100": 1}, {"compare": 1, "reinforcement": 1, "learn": 1, "imagine": 1, "goals": 1, "rig": 1, "40": 1, "stateoftheart": 1, "method": 1, "solve": 1, "imagebased": 1, "goalconditioned": 1, "task": 1}, {"rig": 1, "learn": 1, "reward": 2, "function": 2, "image": 1, "rather": 1, "use": 1, "predetermine": 1}, {"find": 1, "provide": 1, "rig": 2, "distance": 1, "function": 1, "method": 1, "improve": 1, "performance": 1, "use": 1, "stronger": 1, "variant": 1, "ensure": 1, "fair": 1, "comparison": 1}, {"addition": 1, "compare": 1, "hindsight": 1, "experiment": 1, "replay": 1, "2": 1, "use": 1, "sparse": 1, "indicator": 1, "reward": 1}, {"lastly": 1, "compare": 1, "probabilistic": 1, "ensembles": 1, "trajectory": 1, "sample": 1, "pet": 1, "7": 1, "stateoftheart": 1, "modelbased": 1, "rl": 1, "method": 1}, {"favorably": 1, "implement": 1, "pet": 2, "groundtruth": 1, "lowdimensional": 1, "state": 2, "representation": 1, "label": 1}, {"result": 1, "show": 1, "figure": 1, "3": 1}, {"leap": 1, "significantly": 1, "outperform": 1, "prior": 1, "work": 1, "task": 2, "particularly": 1, "harder": 1, "push": 1, "reach": 1}, {"tdm": 1, "use": 2, "leap": 2, "tdm25": 1, "perform": 1, "poorly": 1, "compose": 1, "3": 1, "different": 1, "subgoals": 1, "result": 1, "much": 1, "better": 1, "performance": 1}, {"400k": 1, "environment": 1, "step": 1, "leap": 1, "already": 1, "achieve": 1, "final": 1, "puck": 1, "distance": 1, "10": 1, "cm": 1, "next": 1, "best": 1, "method": 1, "tdm100": 1, "require": 1, "5": 1, "time": 1, "many": 1, "sample": 1}, {"detail": 2, "task": 1, "appendix": 2, "b": 1, "algorithm": 1, "implementation": 1, "give": 1, "c": 1, "visualize": 1, "subgoals": 2, "choose": 1, "leap": 1, "figure": 1, "4": 1, "decode": 1, "latent": 1, "zt1k": 1, "image": 1, "vae": 1, "decoder": 1, "p": 1, "": 1}, {"push": 1, "reach": 1, "image": 1, "correspond": 1, "natural": 1, "subgoals": 1, "task": 1}, {"figure": 1, "4": 1, "also": 1, "show": 1, "visualization": 1, "value": 1, "function": 1, "use": 1, "planner": 1, "determine": 1, "reachability": 1}, {"note": 1, "value": 1, "function": 1, "generally": 1, "recognize": 1, "wall": 1, "impassable": 1, "7": 1, "": 1, "figure": 1, "4": 1, "leave": 1, "visualization": 1, "subgoals": 2, "reconstruct": 1, "vae": 1, "bottom": 1, "row": 2, "actual": 1, "image": 1, "see": 1, "reach": 1, "top": 1}, {"give": 1, "initial": 1, "state": 1, "s0": 1, "goal": 2, "image": 2, "g": 2, "planner": 1, "choose": 1, "meaningful": 1, "subgoals": 2, "gt1": 1, "": 1, "move": 2, "towards": 1, "puck": 2, "gt2": 1, "begin": 1, "push": 2, "gt3": 1, "complete": 1, "motion": 1, "hand": 1, "position": 1, "middle": 1, "top": 1, "row": 1, "show": 1, "superimpose": 1, "one": 1, "another": 1}, {"blue": 1, "circle": 3, "start": 1, "position": 2, "green": 1, "target": 1, "intermediate": 1, "show": 1, "progression": 1, "subgoals": 1, "bright": 1, "red": 1, "gt1": 1, "": 2, "brown": 1, "gt3": 1}, {"color": 1, "circle": 1, "show": 1, "subgoals": 1, "latent": 2, "space": 1, "bottom": 1, "row": 1, "two": 1, "active": 1, "vae": 2, "dimension": 1, "well": 1, "sample": 1, "aggregate": 1, "posterior": 1, "35": 1}, {"right": 1, "heatmap": 1, "value": 2, "function": 1, "v": 1, "g": 1, "column": 1, "show": 2, "different": 1, "time": 1, "horizon": 1, "fix": 1, "state": 1, "warmer": 1, "color": 1, "higher": 1}, {"image": 1, "indicate": 1, "value": 2, "function": 2, "possible": 1, "goals": 2, "g": 1, "time": 1, "horizon": 1, "decrease": 1, "recognize": 1, "reach": 1, "nearby": 1}, {"make": 1, "reasonable": 1, "predictions": 1, "different": 1, "time": 1, "horizons": 1}, {"videos": 1, "final": 1, "policies": 1, "generate": 1, "subgoals": 1, "code": 1, "implementation": 1, "leap": 1, "available": 1, "paper": 1, "website3": 1, "": 1}, {"52": 1, "": 2, "plan": 1, "nonvisionbased": 1, "environments": 1, "unknown": 1, "state": 1, "space": 1, "leap": 1, "present": 1, "context": 1, "optimize": 1, "image": 1, "also": 1, "study": 1, "utility": 1, "nonvision": 1, "base": 1, "domains": 1}, {"specifically": 1, "compare": 1, "leap": 1, "prior": 1, "work": 1, "ant": 1, "navigation": 1, "task": 1, "show": 1, "figure": 1, "5": 1, "statespace": 1, "consist": 1, "quadruped": 1, "robots": 1, "joint": 2, "angle": 1, "velocity": 1, "center": 1, "mass": 1}, {"state": 4, "space": 2, "compact": 1, "image": 1, "certain": 1, "combinations": 1, "value": 1, "actually": 1, "valid": 1, "obstacle": 2, "environment": 1, "unknown": 1, "agent": 1, "mean": 1, "nave": 1, "optimization": 1, "easily": 1, "result": 1, "invalid": 1, "eg": 1, "put": 1, "robot": 1, "inside": 1}, {"task": 1, "significantly": 1, "longer": 1, "horizon": 1, "tmax": 1, "": 2, "600": 1, "leap": 1, "use": 1, "cem": 1, "optimize": 1, "k": 1, "11": 1, "subgoals": 1, "50": 1, "time": 1, "step": 1, "apart": 1}, {"visionbased": 1, "comparisons": 1, "compare": 1, "modelfree": 1, "tdms": 1, "shorthorizon": 1, "set": 2, "tdm50": 1, "leap": 1, "build": 1, "top": 1, "longhorizon": 1, "tdm600": 1}, {"addition": 1, "compare": 1, "variant": 1, "use": 1, "reward": 1, "relabeling": 1, "strategy": 1, "rig": 1, "label": 1}, {"exclude": 1, "pet": 1, "baseline": 1, "unable": 1, "solve": 1, "longhorizon": 1, "task": 1}, {"section": 1, "add": 1, "comparison": 1, "hierarchical": 2, "reinforcement": 1, "learn": 1, "offpolicy": 1, "correction": 1, "hiro": 1, "38": 1, "method": 1, "statebased": 1, "goals": 1}, {"evaluate": 1, "baselines": 1, "challenge": 1, "configuration": 1, "task": 1, "ant": 1, "must": 1, "navigate": 1, "one": 1, "corner": 1, "maze": 1, "side": 1, "go": 1, "around": 1, "long": 1, "wall": 1}, {"desire": 1, "behavior": 1, "result": 2, "large": 1, "negative": 1, "reward": 1, "trajectory": 1, "optimal": 1, "final": 1, "state": 1}, {"see": 1, "figure": 1, "5": 1, "leap": 1, "method": 1, "successfully": 1, "navigate": 1, "ant": 1, "goal": 1}, {"hiro": 1, "dont": 1, "attempt": 1, "go": 1, "around": 1, "wall": 1, "result": 1, "large": 1, "sum": 1, "negative": 1, "reward": 1}, {"tdm50": 1, "short": 1, "horizon": 1, "result": 1, "greedy": 1, "behavior": 1, "tdm600": 1, "fail": 1, "learn": 1, "due": 1, "temporal": 1, "sparsity": 1, "reward": 1}, {"final": 2, "distance": 1, "goal": 1, "": 11, "ant": 3, "navigation": 2, "12": 1, "leap": 1, "tdm50": 1, "tdm600": 1, "hiro": 1, "10": 1, "8": 1, "6": 1, "4": 1, "2": 1, "0": 2, "200": 1, "400": 1, "600": 1, "800": 1, "number": 1, "environment": 1, "step": 1, "total": 1, "x1000": 1, "1000": 1, "figure": 1, "5": 1, "task": 1, "must": 1, "move": 1, "around": 1, "long": 1, "wall": 1, "incur": 1, "large": 1, "negative": 1, "reward": 1, "trajectory": 1, "result": 1, "optimal": 1, "state": 1}, {"illustrate": 1, "task": 1, "purple": 1, "ant": 2, "show": 2, "start": 1, "state": 1, "green": 1, "goal": 1}, {"use": 1, "3": 1, "subgoals": 1, "illustration": 1}, {"method": 2, "show": 1, "red": 1, "plot": 1, "successfully": 1, "navigate": 1, "ant": 1, "goal": 1}, {"3": 1, "": 5, "httpssitesgooglecomviewgoalplanning": 1, "8": 1, "53": 1, "ablation": 1, "study": 1, "analyze": 1, "importance": 1, "plan": 1, "latent": 1, "space": 2, "oppose": 1, "image": 1, "navigation": 1, "task": 1}, {"comparison": 1, "implement": 1, "planner": 1, "directly": 1, "optimize": 1, "image": 1, "subgoals": 1, "ie": 1, "pixel": 1, "space": 1}, {"also": 1, "study": 1, "importance": 1, "reuse": 1, "pretrained": 1, "vae": 1, "encoder": 1, "replicate": 1, "experiment": 1, "rl": 1, "network": 1, "train": 1, "scratch": 1}, {"see": 1, "figure": 1, "6": 1, "model": 1, "reuse": 1, "vae": 1, "encoder": 1, "succeed": 1, "take": 1, "much": 1, "longer": 1}, {"importantly": 1, "plan": 2, "latent": 1, "state": 1, "achieve": 1, "dramatically": 1, "better": 1, "performance": 1, "raw": 1, "image": 1}, {"figure": 1, "6": 1, "also": 1, "show": 1, "intermediate": 1, "subgoals": 1, "output": 1, "optimizer": 1, "optimize": 1, "image": 1}, {"subgoals": 1, "may": 1, "high": 1, "value": 2, "accord": 1, "equation": 1, "2": 1, "clearly": 1, "correspond": 1, "valid": 2, "state": 2, "observations": 1, "indicate": 1, "planner": 1, "exploit": 1, "function": 1, "choose": 1, "image": 1, "far": 1, "outside": 1, "manifold": 1}, {"final": 1, "distance": 1, "goal": 1, "": 2, "include": 1, "ablations": 1, "appendix": 1, "study": 1, "sensitivity": 1, "equation": 1, "3": 1, "subsection": 3, "a3": 1, "choice": 2, "norm": 1, "a1": 1, "optimizer": 1, "a2": 1}, {"result": 1, "show": 1, "leap": 1, "work": 1, "well": 1, "wide": 1, "range": 1, "": 2, "norm": 1, "perform": 1, "better": 1, "cem": 1, "consistently": 1, "outperform": 1, "gradientbased": 1, "optimizers": 1, "term": 1, "optimizer": 1, "loss": 1, "policy": 1, "performance": 1}, {"6": 2, "": 9, "2dnavigation": 1, "learn": 1, "ablation": 1, "4": 1, "2": 1, "00": 1, "100": 1, "200": 1, "300": 1, "400": 1, "latent": 2, "share": 2, "number": 1, "environment": 1, "step": 1, "total": 1, "x1000": 1, "figure": 1, "leave": 1, "ablative": 1, "study": 1, "2d": 1, "navigation": 1}, {"keep": 1, "components": 1, "leap": 1, "replace": 1, "optimize": 2, "latent": 2, "space": 2, "image": 1}, {"separately": 1, "train": 1, "rl": 1, "methods": 1, "scratch": 1, "rather": 1, "reuse": 1, "vae": 1, "mean": 1, "encoder": 1, "share": 2, "also": 1, "test": 1, "ablations": 1, "together": 1, "latent": 1}, {"see": 1, "share": 1, "encoder": 1, "weight": 1, "rl": 1, "policy": 1, "result": 1, "faster": 1, "learn": 1, "optimize": 1, "latent": 1, "space": 1, "critical": 1, "success": 1, "method": 1}, {"right": 1, "visualization": 1, "subgoals": 1, "generate": 1, "optimize": 2, "latent": 1, "space": 1, "decode": 1, "image": 2, "top": 1, "directly": 1, "bottom": 1}, {"goals": 1, "generate": 1, "plan": 1, "image": 1, "space": 1, "meaningful": 1, "explain": 1, "poor": 1, "performance": 1, "latent": 1, "show": 1, "leave": 1}, {"6": 1, "": 2, "discussion": 1, "present": 1, "latent": 1, "embeddings": 1, "abstract": 1, "plan": 1, "leap": 1, "approach": 1, "solve": 1, "temporally": 1, "extend": 1, "task": 1, "highdimensional": 1, "state": 1, "observations": 1, "image": 1}, {"key": 1, "idea": 1, "leap": 1, "form": 1, "temporal": 1, "abstractions": 2, "use": 2, "goalreaching": 1, "policies": 1, "evaluate": 1, "reachability": 1, "state": 2, "representation": 2, "learn": 1, "provide": 1, "convenient": 1, "plan": 1}, {"plan": 2, "state": 2, "learn": 1, "latent": 1, "space": 1, "use": 1, "subgoals": 1, "goalconditioned": 1, "policies": 2, "leap": 1, "solve": 2, "task": 1, "difficult": 1, "conventional": 1, "modelfree": 1, "goalreaching": 1, "avoid": 1, "challenge": 1, "model": 1, "lowlevel": 1, "observations": 1, "associate": 1, "fully": 1, "modelbased": 1, "methods": 1}, {"generally": 1, "combination": 1, "modelfree": 1, "rl": 2, "plan": 1, "excite": 1, "research": 1, "direction": 1, "hold": 1, "potential": 1, "make": 1, "methods": 1, "flexible": 1, "capable": 1, "broadly": 1, "applicable": 1}, {"method": 1, "represent": 1, "step": 1, "direction": 1, "though": 1, "many": 1, "crucial": 1, "question": 1, "remain": 1, "answer": 1}, {"work": 2, "largely": 1, "neglect": 1, "question": 2, "exploration": 2, "goalconditioned": 1, "policies": 1, "though": 1, "study": 1, "recent": 1, "17": 1, "45": 1, "59": 1, "49": 1, "examine": 1, "interact": 1, "plan": 1, "excite": 1, "future": 1, "direction": 1}, {"another": 1, "excite": 1, "direction": 1, "future": 1, "work": 1, "study": 1, "lossy": 1, "state": 2, "abstractions": 1, "might": 1, "improve": 1, "performance": 1, "planner": 1, "explicitly": 1, "discard": 1, "information": 1, "irrelevant": 1, "higherlevel": 1, "plan": 1}, {"7": 1, "": 2, "acknowledgments": 1, "work": 1, "support": 1, "office": 1, "naval": 1, "research": 1, "national": 1, "science": 1, "foundation": 1, "google": 1, "nvidia": 1, "amazon": 1, "arl": 1, "dcist": 1, "cra": 1, "w911nf1720181": 1}, {"9": 1, "": 1, "reference": 1, "1": 1, "arpit": 1, "agarwal": 1, "katharina": 1, "muelling": 1, "katerina": 1, "fragkiadaki": 1}, {"model": 1, "learn": 1, "lookahead": 1, "exploration": 1, "continuous": 1, "control": 1}, {"aaai": 1, "2019": 1}, {"2": 1, "marcin": 1, "andrychowicz": 1, "filip": 1, "wolski": 1, "alex": 1, "ray": 1, "jonas": 1, "schneider": 1, "rachel": 1, "fong": 1, "peter": 1, "welinder": 1, "bob": 1, "mcgrew": 1, "josh": 1, "tobin": 1, "pieter": 1, "abbeel": 1, "wojciech": 1, "zaremba": 1}, {"hindsight": 1, "experience": 1, "replay": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "2017": 1}, {"3": 1, "mohammad": 1, "babaeizadeh": 1, "chelsea": 1, "finn": 1, "dumitru": 1, "erhan": 1, "roy": 1, "h": 1, "campbell": 1, "sergey": 1, "levine": 1}, {"stochastic": 1, "variational": 1, "video": 1, "prediction": 1}, {"international": 1, "conference": 1, "learn": 1, "representations": 1, "2018": 1}, {"4": 1, "byron": 1, "boot": 1, "arunkumar": 1, "byravan": 1, "dieter": 1, "fox": 1}, {"learn": 1, "predictive": 1, "model": 1, "depth": 1, "camera": 1, "": 1, "manipulator": 1, "raw": 1, "execution": 1, "trace": 1}, {"ieee": 1, "international": 1, "conference": 1, "robotics": 1, "automation": 1, "2014": 1}, {"5": 1, "arunkumar": 1, "byravan": 1, "felix": 1, "leeb": 1, "franziska": 1, "meier": 1, "dieter": 1, "fox": 1}, {"se3posenets": 1, "structure": 1, "deep": 1, "dynamics": 1, "model": 1, "visuomotor": 1, "plan": 1, "control": 1}, {"ieee": 1, "international": 1, "conference": 1, "robotics": 1, "automation": 1}, {"6": 1, "silvia": 1, "chiappa": 1, "sbastien": 1, "racaniere": 1, "daan": 1, "wierstra": 1, "shakir": 1, "mohamed": 1}, {"recurrent": 1, "environment": 1, "simulators": 1}, {"international": 1, "conference": 1, "learn": 1, "representations": 1, "2017": 1}, {"7": 1, "kurtland": 1, "chua": 1, "roberto": 1, "calandra": 1, "rowan": 1, "mcallister": 1, "sergey": 1, "levine": 1}, {"deep": 1, "reinforcement": 1, "learn": 1, "handful": 1, "trials": 1, "use": 1, "probabilistic": 1, "dynamics": 1, "model": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "2018": 1}, {"8": 1, "cdric": 1, "colas": 1, "pierre": 1, "fournier": 1, "olivier": 1, "sigaud": 1, "pierreyves": 1, "oudeyer": 1}, {"curious": 1, "intrinsically": 1, "motivate": 1, "multitask": 1, "multigoal": 1, "reinforcement": 1, "learn": 1}, {"international": 1, "conference": 1, "machine": 1, "learn": 1, "2019": 1}, {"9": 1, "bin": 1, "dai": 1, "david": 1, "wipf": 1}, {"diagnose": 1, "enhance": 1, "vae": 1, "model": 1}, {"international": 1, "conference": 1, "learn": 1, "representations": 1, "2019": 1}, {"10": 1, "peter": 1, "dayan": 1, "geoffrey": 1, "e": 1, "hinton": 1}, {"feudal": 1, "reinforcement": 1, "learn": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "1993": 1}, {"11": 1, "pietertjerk": 1, "de": 1, "boer": 1, "dirk": 1, "p": 1, "kroese": 1, "shie": 1, "mannor": 1, "reuven": 1, "rubinstein": 1}, {"tutorial": 1, "crossentropy": 1, "method": 1}, {"annals": 1, "operations": 1, "research": 1, "1341": 1, "2005": 1}, {"12": 1, "thomas": 1, "g": 1, "dietterich": 1}, {"hierarchical": 1, "reinforcement": 1, "learn": 1, "maxq": 1, "value": 1, "function": 1, "decomposition": 1}, {"journal": 1, "artificial": 1, "intelligence": 1, "research": 1, "13": 1, "2000": 1}, {"13": 1, "frederik": 1, "ebert": 1, "chelsea": 1, "finn": 1, "alex": 1, "x": 1, "lee": 1, "sergey": 1, "levine": 1}, {"selfsupervised": 1, "visual": 1, "plan": 1, "temporal": 1, "skip": 1, "connections": 1}, {"conference": 1, "robot": 1, "learn": 1, "2017": 1}, {"14": 1, "frederik": 1, "ebert": 1, "chelsea": 1, "finn": 1, "sudeep": 1, "dasari": 1, "annie": 1, "xie": 1, "alex": 1, "lee": 1, "sergey": 1, "levine": 1}, {"visual": 1, "foresight": 1, "modelbased": 1, "deep": 1, "reinforcement": 1, "learn": 1, "visionbased": 1, "robotic": 1, "control": 1}, {"arxiv": 1, "preprint": 1, "arxiv181200568": 1, "2018": 1}, {"15": 1, "chelsea": 1, "finn": 1, "sergey": 1, "levine": 1}, {"deep": 1, "visual": 1, "foresight": 1, "plan": 1, "robot": 1, "motion": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "2016": 1}, {"16": 1, "chelsea": 1, "finn": 1, "xin": 1, "yu": 1, "tan": 1, "yan": 1, "duan": 1, "trevor": 1, "darrell": 1, "sergey": 1, "levine": 1, "pieter": 1, "abbeel": 1}, {"deep": 1, "spatial": 1, "autoencoders": 1, "visuomotor": 1, "learn": 1}, {"ieee": 1, "international": 1, "conference": 1, "robotics": 1, "automation": 1, "2016": 1}, {"17": 1, "carlos": 1, "florensa": 1, "david": 1, "hold": 1, "xinyang": 1, "geng": 1, "pieter": 1, "abbeel": 1}, {"automatic": 1, "goal": 1, "generation": 1, "reinforcement": 1, "learn": 1, "agents": 1}, {"international": 1, "conference": 1, "machine": 1, "learn": 1, "2018": 1}, {"18": 1, "david": 1, "foster": 1, "peter": 1, "dayan": 1}, {"structure": 1, "space": 1, "value": 1, "function": 1}, {"machine": 1, "learn": 1, "49": 1, "23": 1, "2002": 1}, {"19": 1, "scott": 1, "fujimoto": 1, "herke": 1, "van": 1, "hoof": 1, "david": 1, "meger": 1}, {"address": 1, "function": 1, "approximation": 1, "error": 1, "actorcritic": 1, "methods": 1}, {"international": 1, "conference": 1, "machine": 1, "learn": 1, "2018": 1}, {"20": 1, "dibya": 1, "ghosh": 1, "abhishek": 1, "gupta": 1, "sergey": 1, "levine": 1}, {"learn": 1, "actionable": 1, "representations": 1, "goalconditioned": 1, "policies": 1}, {"international": 1, "conference": 1, "learn": 1, "representations": 1, "2019": 1}, {"21": 1, "arthur": 1, "guez": 1, "mehdi": 1, "mirza": 1, "karol": 1, "gregor": 1, "rishabh": 1, "kabra": 1, "sbastien": 1, "racanire": 1, "thophane": 1, "weber": 1, "david": 1, "raposo": 1, "adam": 1, "santoro": 1, "laurent": 1, "orseau": 1, "tom": 1, "eccles": 1, "et": 1, "al": 1}, {"investigation": 1, "modelfree": 1, "plan": 1}, {"international": 1, "conference": 1, "machine": 1, "learn": 1, "2019": 1}, {"10": 1, "": 1, "22": 1, "danijar": 1, "hafner": 1, "timothy": 1, "lillicrap": 1, "ian": 1, "fischer": 1, "ruben": 1, "villegas": 1, "david": 1, "ha": 1, "honglak": 1, "lee": 1, "jam": 1, "davidson": 1}, {"learn": 1, "latent": 1, "dynamics": 1, "plan": 1, "pixels": 1}, {"international": 1, "conference": 1, "machine": 1, "learn": 1, "2019": 1}, {"23": 1, "nicolas": 1, "heess": 1, "greg": 1, "wayne": 1, "david": 1, "silver": 1, "timothy": 1, "lillicrap": 1, "yuval": 1, "tassa": 1, "tom": 1, "erez": 1}, {"learn": 1, "continuous": 1, "control": 1, "policies": 1, "stochastic": 1, "value": 1, "gradients": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "2015": 1}, {"24": 1, "leslie": 1, "pack": 1, "kaelbling": 1}, {"learn": 1, "achieve": 1, "goals": 1}, {"international": 1, "joint": 1, "conference": 1, "artificial": 1, "intelligence": 1, "ijcai": 1, "volume": 1, "vol2": 1, "1993": 1}, {"25": 1, "leslie": 1, "pack": 1, "kaelbling": 1}, {"hierarchical": 1, "learn": 1, "stochastic": 1, "domains": 1, "preliminary": 1, "result": 1}, {"international": 1, "conference": 1, "machine": 1, "learn": 1, "1993": 1}, {"26": 1, "lukasz": 1, "kaiser": 1, "mohammad": 1, "babaeizadeh": 1, "piotr": 2, "milos": 1, "blazej": 1, "osinski": 1, "roy": 1, "h": 1, "campbell": 1, "konrad": 1, "czechowski": 1, "dumitru": 1, "erhan": 1, "chelsea": 1, "finn": 1, "kozakowski": 1, "sergey": 1, "levine": 1, "et": 1, "al": 1}, {"modelbased": 1, "reinforcement": 1, "learn": 1, "atari": 1}, {"arxiv": 1, "preprint": 1, "arxiv190300374": 1, "2019": 1}, {"27": 1, "nal": 1, "kalchbrenner": 1, "aron": 1, "van": 1, "den": 1, "oord": 1, "karen": 1, "simonyan": 1, "ivo": 1, "danihelka": 1, "oriol": 1, "vinyals": 1, "alex": 1, "grave": 1, "koray": 1, "kavukcuoglu": 1}, {"video": 1, "pixel": 1, "network": 1}, {"international": 1, "conference": 1, "machine": 1, "learn": 1, "2017": 1}, {"28": 1, "diederik": 1, "p": 1, "kingma": 1, "max": 1, "well": 1}, {"autoencoding": 1, "variational": 1, "bay": 1}, {"international": 1, "conference": 1, "learn": 1, "representations": 1, "2014": 1}, {"29": 1, "thanard": 1, "kurutach": 1, "aviv": 1, "tamar": 1, "ge": 1, "yang": 1, "stuart": 1, "j": 1, "russell": 1, "pieter": 1, "abbeel": 1}, {"learn": 1, "plannable": 1, "representations": 1, "causal": 1, "infogan": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "2018": 1}, {"30": 1, "terran": 1, "lane": 1, "leslie": 1, "pack": 1, "kaelbling": 1}, {"toward": 1, "hierarchical": 1, "decomposition": 1, "plan": 1, "uncertain": 1, "environments": 1}, {"proceed": 1, "2001": 2, "ijcai": 1, "workshop": 1, "plan": 1, "uncertainty": 1, "incomplete": 1, "information": 1}, {"31": 1, "alex": 1, "x": 1, "lee": 1, "richard": 1, "zhang": 1, "frederik": 1, "ebert": 1, "pieter": 1, "abbeel": 1, "chelsea": 1, "finn": 1, "sergey": 1, "levine": 1}, {"stochastic": 1, "adversarial": 1, "video": 1, "prediction": 1}, {"arxiv": 1, "preprint": 1, "arxiv180401523": 1, "2018": 1}, {"32": 1, "ian": 1, "lenz": 1, "ross": 1, "knepper": 1, "ashutosh": 1, "saxena": 1}, {"deepmpc": 1, "learn": 1, "deep": 1, "latent": 1, "feature": 1, "model": 1, "predictive": 1, "control": 1}, {"robotics": 1, "science": 1, "systems": 1, "rss": 1, "2015": 1}, {"33": 1, "andrew": 1, "levy": 1, "george": 1, "konidaris": 1, "robert": 1, "platt": 1, "kate": 1, "saenko": 1}, {"learn": 1, "multilevel": 1, "hierarchies": 1, "hindsight": 1}, {"international": 1, "conference": 1, "learn": 1, "representations": 1, "2019": 1}, {"34": 1, "hawminn": 1, "lu": 1, "yeshaiahu": 1, "fainman": 1, "robert": 1, "hechtnielsen": 1}, {"image": 1, "manifold": 1}, {"applications": 1, "artificial": 1, "neural": 1, "network": 1, "image": 1, "process": 1, "iii": 1, "volume": 1, "3307": 1, "international": 1, "society": 1, "optics": 1, "photonics": 1, "1998": 1}, {"35": 1, "alireza": 1, "makhzani": 1, "jonathon": 1, "shlens": 1, "navdeep": 1, "jaitly": 1, "ian": 1, "goodfellow": 1, "brendan": 1, "frey": 1}, {"adversarial": 1, "autoencoders": 1}, {"international": 1, "conference": 1, "learn": 1, "representations": 1, "2016": 1}, {"36": 1, "michael": 1, "mathieu": 1, "camille": 1, "couprie": 1, "yann": 1, "lecun": 1}, {"deep": 1, "multiscale": 1, "video": 1, "prediction": 1, "beyond": 1, "mean": 1, "square": 1, "error": 1}, {"international": 1, "conference": 1, "learn": 1, "representations": 1, "2016": 1}, {"37": 1, "andrew": 1, "w": 1, "moore": 1, "leemon": 1, "baird": 1, "leslie": 1, "p": 1, "kaelbling": 1}, {"multivaluefunctions": 1, "effcient": 1, "automatic": 1, "action": 1, "hierarchies": 1, "multiple": 1, "goal": 1, "mdps": 1}, {"proceed": 1, "international": 1, "joint": 1, "conference": 1, "artificial": 1, "intelligence": 1, "1999": 1}, {"38": 1, "ofir": 1, "nachum": 1, "google": 1, "brain": 1, "shane": 1, "gu": 1, "honglak": 1, "lee": 1, "sergey": 1, "levine": 1}, {"dataefficient": 1, "hierarchical": 1, "reinforcement": 1, "learn": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "2018": 1}, {"39": 1, "anusha": 1, "nagabandi": 1, "gregory": 1, "kahn": 1, "ronald": 1, "fear": 1, "sergey": 1, "levine": 1}, {"neural": 1, "network": 1, "dynamics": 1, "modelbased": 1, "deep": 1, "reinforcement": 1, "learn": 1, "modelfree": 1, "finetuning": 1}, {"ieee": 1, "international": 1, "conference": 1, "robotics": 1, "automation": 1, "2018": 1}, {"40": 1, "ashvin": 1, "nair": 1, "vitchyr": 1, "pong": 1, "murtaza": 1, "dalal": 1, "shikhar": 1, "bahl": 1, "steven": 1, "lin": 1, "sergey": 1, "levine": 1}, {"visual": 1, "reinforcement": 1, "learn": 1, "imagine": 1, "goals": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "2018": 1}, {"41": 1, "derrick": 1, "h": 1, "nguyen": 1, "bernard": 1, "widrow": 1}, {"neural": 1, "network": 1, "selflearning": 1, "control": 1, "systems": 1}, {"ieee": 1, "control": 1, "systems": 1, "magazine": 1, "1990": 1}, {"42": 1, "jorge": 1, "nocedal": 1, "stephen": 1, "wright": 1}, {"numerical": 1, "optimization": 1}, {"springer": 1, "science": 1, "": 1, "business": 1, "media": 1, "2006": 1}, {"11": 1, "": 1, "43": 1, "junhyuk": 1, "oh": 1, "xiaoxiao": 1, "guo": 1, "honglak": 1, "lee": 1, "richard": 1, "lewis": 1, "satinder": 1, "singh": 1}, {"actionconditional": 1, "video": 1, "prediction": 1, "use": 1, "deep": 1, "network": 1, "atari": 1, "game": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "2015": 1}, {"44": 1, "junhyuk": 1, "oh": 1, "satinder": 1, "singh": 1, "honglak": 1, "lee": 1}, {"value": 1, "prediction": 1, "network": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "2017": 1}, {"45": 1, "fabio": 1, "pardo": 1, "vitaly": 1, "levdik": 1, "petar": 1, "kormushev": 1}, {"qmap": 1, "convolutional": 1, "approach": 1, "goaloriented": 1, "reinforcement": 1, "learn": 1}, {"corr": 1, "abs181002927": 1, "2018": 1}, {"46": 1, "neal": 1, "parikh": 1, "stephen": 1, "boyd": 1, "et": 1, "al": 1}, {"proximal": 1, "algorithms": 1}, {"foundations": 1, "trend": 1, "r": 1, "optimization": 1, "13": 1, "2014": 1}, {"47": 1, "matthias": 1, "plappert": 1, "marcin": 1, "andrychowicz": 1, "alex": 1, "ray": 1, "bob": 1, "mcgrew": 1, "bowen": 1, "baker": 1, "glenn": 1, "powell": 1, "jonas": 1, "schneider": 1, "josh": 1, "tobin": 1, "maciek": 1, "chociej": 1, "peter": 1, "welinder": 1, "vikash": 1, "kumar": 1, "wojciech": 1, "zaremba": 1}, {"multigoal": 1, "reinforcement": 1, "learn": 1, "challenge": 1, "robotics": 1, "environments": 1, "request": 1, "research": 1}, {"arxiv": 1, "preprint": 1, "arxiv180209464": 1, "2018": 1}, {"48": 1, "vitchyr": 1, "pong": 1, "shixiang": 1, "gu": 1, "murtaza": 1, "dalal": 1, "sergey": 1, "levine": 1}, {"temporal": 1, "difference": 1, "model": 1, "modelfree": 1, "deep": 1, "rl": 1, "modelbased": 1, "control": 1}, {"international": 1, "conference": 1, "learn": 1, "representations": 1, "2018": 1}, {"49": 1, "vitchyr": 1, "h": 1, "pong": 1, "murtaza": 1, "dalal": 1, "steven": 1, "lin": 1, "ashvin": 1, "nair": 1, "shikhar": 1, "bahl": 1, "sergey": 1, "levine": 1}, {"skewfit": 1, "statecovering": 1, "selfsupervised": 1, "reinforcement": 1, "learn": 1}, {"corr": 1, "abs190303698": 1, "2019": 1}, {"50": 1, "ali": 1, "punjani": 1, "pieter": 1, "abbeel": 1}, {"deep": 1, "learn": 1, "helicopter": 1, "dynamics": 1, "model": 1}, {"ieee": 1, "international": 1, "conference": 1, "robotics": 1, "automation": 1, "2015": 1}, {"51": 1, "sbastien": 1, "racanire": 1, "thophane": 1, "weber": 1, "david": 1, "reichert": 1, "lars": 1, "buesing": 1, "arthur": 1, "guez": 1, "danilo": 1, "jimenez": 1, "rezende": 1, "adria": 1, "puigdomenech": 1, "badia": 1, "oriol": 1, "vinyals": 1, "nicolas": 1, "heess": 1, "yujia": 1, "li": 1, "et": 1, "al": 1}, {"imaginationaugmented": 1, "agents": 1, "deep": 1, "reinforcement": 1, "learn": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "2017": 1}, {"52": 1, "danilo": 1, "j": 1, "rezende": 1, "shakir": 1, "mohamed": 1, "daan": 1, "wierstra": 1}, {"stochastic": 1, "backpropagation": 1, "approximate": 1, "inference": 1, "deep": 1, "generative": 1, "model": 1}, {"international": 1, "conference": 1, "machine": 1, "learn": 1, "2014": 1}, {"53": 1, "tom": 1, "schaul": 1, "daniel": 1, "horgan": 1, "karol": 1, "gregor": 1, "david": 1, "silver": 1}, {"universal": 1, "value": 1, "function": 1, "approximators": 1}, {"international": 1, "conference": 1, "machine": 1, "learn": 1, "2015": 1}, {"54": 1, "richard": 1, "sutton": 1}, {"integrate": 1, "architectures": 1, "learn": 1, "plan": 1, "react": 1, "base": 1, "approximate": 1, "dynamic": 1, "program": 1}, {"machine": 1, "learn": 1, "proceed": 1, "1990": 1}, {"elsevier": 1, "1990": 1}, {"55": 1, "richard": 1, "sutton": 1, "joseph": 1, "modayil": 1, "michael": 1, "delp": 1, "thomas": 1, "degris": 1, "patrick": 1, "pilarski": 1, "adam": 1, "white": 1, "doina": 1, "precup": 1}, {"horde": 1, "scalable": 1, "realtime": 1, "architecture": 1, "learn": 1, "knowledge": 1, "unsupervised": 1, "sensorimotor": 1, "interaction": 1}, {"international": 1, "conference": 1, "autonomous": 1, "agents": 1, "multiagent": 1, "systems": 1, "2011": 1}, {"56": 1, "aviv": 1, "tamar": 1, "yi": 1, "wu": 1, "garrett": 1, "thomas": 1, "sergey": 1, "levine": 1, "pieter": 1, "abbeel": 1}, {"value": 1, "iteration": 1, "network": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "2016": 1}, {"57": 1, "vivek": 1, "veeriah": 1, "junhyuk": 1, "oh": 1, "satinder": 1, "singh": 1}, {"manygoals": 1, "reinforcement": 1, "learn": 1}, {"arxiv": 1, "preprint": 1, "arxiv180609605": 1, "2018": 1}, {"58": 1, "alexander": 1, "sasha": 1, "vezhnevets": 1, "simon": 1, "osindero": 1, "tom": 1, "schaul": 1, "nicolas": 1, "heess": 1, "max": 1, "jaderberg": 1, "david": 1, "silver": 1, "koray": 1, "kavukcuoglu": 1}, {"feudal": 1, "network": 1, "hierarchical": 1, "reinforcement": 1, "learn": 1}, {"international": 1, "conference": 1, "machine": 1, "learn": 1, "2017": 1}, {"59": 1, "david": 1, "wardefarley": 1, "tom": 1, "van": 1, "de": 1, "wiele": 1, "tejas": 1, "kulkarni": 1, "catalin": 1, "ionescu": 1, "steven": 1, "hansen": 1, "volodymyr": 1, "mnih": 1}, {"unsupervised": 1, "control": 1, "nonparametric": 1, "discriminative": 1, "reward": 1}, {"corr": 1, "abs181111359": 1, "2018": 1}, {"60": 1, "manuel": 1, "watter": 1, "jost": 1, "tobias": 1, "springenberg": 1, "joschka": 1, "boedecker": 1, "martin": 1, "riedmiller": 1}, {"embed": 1, "control": 2, "locally": 1, "linear": 1, "latent": 1, "dynamics": 1, "model": 1, "raw": 1, "image": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "2015": 1}, {"61": 1, "marco": 1, "wiering": 1, "jrgen": 1, "schmidhuber": 1}, {"hqlearning": 1}, {"adaptive": 1, "behavior": 1, "62": 1, "1997": 1}, {"62": 1, "marvin": 1, "zhang": 1, "sharad": 1, "vikram": 1, "laura": 1, "smith": 1, "pieter": 1, "abbeel": 1, "matthew": 1, "j": 1, "johnson": 1, "sergey": 1, "levine": 1}, {"solar": 1, "deep": 1, "structure": 1, "latent": 1, "representations": 1, "modelbased": 1, "reinforcement": 1, "learn": 1}, {"international": 1, "conference": 1, "machine": 1, "learn": 1, "2019": 1}, {"12": 1}]
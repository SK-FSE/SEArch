[{"stabilize": 1, "offpolicy": 2, "qlearning": 1, "via": 1, "bootstrapping": 1, "error": 1, "reduction": 1, "aviral": 1, "kumar": 1, "uc": 3, "berkeley": 3, "aviralkberkeleyedu": 1, "george": 1, "tucker": 1, "google": 2, "brain": 2, "gjtgooglecom": 1, "": 2, "justin": 1, "fu": 1, "justinjfueecsberkeleyedu": 1, "sergey": 1, "levine": 1, "svlevineeecsberkeleyedu": 1, "abstract": 1, "reinforcement": 1, "learn": 2, "aim": 1, "leverage": 1, "experience": 1, "collect": 1, "prior": 1, "policies": 1, "sampleefficient": 1}, {"however": 1, "practice": 1, "commonly": 1, "use": 1, "offpolicy": 1, "approximate": 1, "dynamic": 1, "program": 1, "methods": 2, "base": 1, "qlearning": 1, "actorcritic": 1, "highly": 1, "sensitive": 1, "data": 2, "distribution": 1, "make": 1, "limit": 1, "progress": 1, "without": 1, "collect": 1, "additional": 1, "onpolicy": 1}, {"step": 1, "towards": 1, "robust": 1, "offpolicy": 2, "algorithms": 1, "study": 1, "set": 1, "experience": 1, "fix": 1, "interaction": 1, "environment": 1}, {"identify": 1, "bootstrapping": 1, "error": 1, "key": 1, "source": 1, "instability": 1, "current": 1, "methods": 1}, {"bootstrapping": 2, "error": 1, "due": 1, "action": 1, "lie": 1, "outside": 1, "train": 1, "data": 1, "distribution": 1, "accumulate": 1, "via": 1, "bellman": 1, "backup": 1, "operator": 1}, {"theoretically": 1, "analyze": 1, "bootstrapping": 1, "error": 1, "demonstrate": 1, "carefully": 1, "constrain": 1, "action": 1, "selection": 1, "backup": 1, "mitigate": 1}, {"base": 1, "analysis": 1, "propose": 1, "practical": 1, "algorithm": 1, "bootstrapping": 1, "error": 1, "accumulation": 1, "reduction": 1, "bear": 1}, {"demonstrate": 1, "bear": 1, "able": 1, "learn": 1, "robustly": 1, "different": 1, "offpolicy": 1, "distributions": 1, "include": 1, "random": 1, "suboptimal": 1, "demonstrations": 1, "range": 1, "continuous": 1, "control": 1, "task": 1}, {"1": 1, "": 2, "introduction": 1, "one": 1, "primary": 1, "drivers": 1, "success": 1, "machine": 1, "learn": 2, "methods": 1, "openworld": 1, "perception": 1, "settings": 1, "computer": 1, "vision": 1, "19": 1, "nlp": 1, "8": 1, "ability": 1, "highcapacity": 1, "function": 1, "approximators": 1, "deep": 1, "neural": 1, "network": 1, "generalizable": 1, "model": 1, "large": 1, "amount": 1, "data": 1}, {"reinforcement": 1, "learn": 1, "rl": 2, "prove": 1, "comparatively": 1, "difficult": 1, "scale": 1, "unstructured": 1, "realworld": 1, "settings": 1, "algorithms": 1, "require": 1, "active": 1, "data": 1, "collection": 1}, {"result": 1, "rl": 1, "algorithms": 1, "learn": 1, "complex": 1, "behaviors": 1, "simulation": 1, "data": 2, "collection": 2, "straightforward": 1, "realworld": 1, "performance": 1, "limit": 1, "expense": 1, "active": 1}, {"domains": 1, "autonomous": 1, "drive": 1, "38": 1, "recommender": 1, "systems": 1, "3": 1, "previously": 1, "collect": 1, "datasets": 1, "plentiful": 1}, {"algorithms": 1, "utilize": 1, "datasets": 1, "effectively": 1, "would": 2, "make": 1, "realworld": 1, "rl": 1, "practical": 1, "also": 1, "enable": 1, "substantially": 1, "better": 1, "generalization": 1, "incorporate": 1, "diverse": 1, "prior": 1, "experience": 1}, {"principle": 1, "offpolicy": 3, "rl": 1, "algorithms": 2, "leverage": 1, "data": 2, "however": 1, "practice": 1, "limit": 1, "ability": 1, "learn": 1, "entirely": 1}, {"recent": 1, "offpolicy": 1, "rl": 1, "methods": 1, "eg": 1, "18": 1, "29": 1, "23": 2, "9": 1, "demonstrate": 1, "sampleefficient": 1, "performance": 1, "complex": 1, "task": 1, "robotics": 1, "simulate": 1, "environments": 1, "36": 1}, {"however": 1, "methods": 1, "still": 1, "fail": 1, "learn": 1, "present": 1, "arbitrary": 1, "offpolicy": 1, "data": 1, "without": 1, "opportunity": 1, "collect": 1, "experience": 1, "": 3, "equal": 1, "contribution": 1, "33rd": 1, "conference": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "neurips": 1, "2019": 1, "vancouver": 1, "canada": 1}, {"environment": 1}, {"issue": 1, "persist": 1, "even": 1, "offpolicy": 1, "data": 1, "come": 1, "effective": 1, "expert": 1, "policies": 1, "principle": 1, "address": 1, "exploration": 1, "challenge": 1, "6": 1, "12": 1, "11": 1}, {"sensitivity": 1, "train": 2, "data": 1, "distribution": 1, "limitation": 1, "practical": 1, "offpolicy": 2, "rl": 1, "algorithms": 1, "one": 1, "would": 1, "hope": 1, "algorithm": 1, "able": 1, "learn": 1, "reasonable": 1, "policies": 1, "static": 1, "datasets": 1, "deploy": 1, "real": 1, "world": 1}, {"paper": 1, "aim": 1, "develop": 1, "offpolicy": 1, "valuebased": 1, "rl": 1, "methods": 1, "learn": 1, "large": 1, "static": 1, "datasets": 1}, {"show": 1, "crucial": 1, "challenge": 1, "apply": 1, "valuebased": 1, "methods": 1, "offpolicy": 2, "scenarios": 1, "arise": 1, "bootstrapping": 1, "process": 1, "employ": 1, "qfunctions": 1, "evaluate": 1, "outofdistribution": 1, "action": 1, "input": 1, "compute": 1, "backup": 1, "train": 1, "data": 1}, {"may": 1, "introduce": 1, "errors": 2, "qfunction": 1, "algorithm": 1, "unable": 1, "collect": 1, "new": 1, "data": 1, "order": 1, "remedy": 1, "make": 1, "train": 1, "unstable": 1, "potentially": 1, "diverge": 1}, {"primary": 1, "contribution": 1, "analysis": 1, "error": 2, "accumulation": 1, "bootstrapping": 1, "process": 1, "due": 1, "outofdistribution": 1, "input": 1, "practical": 1, "way": 1, "address": 1}, {"first": 1, "formalize": 1, "analyze": 1, "reason": 1, "instability": 1, "poor": 1, "performance": 1, "learn": 1, "offpolicy": 1, "data": 1}, {"show": 1, "careful": 1, "action": 1, "selection": 1, "error": 1, "propagation": 1, "qfunction": 1, "mitigate": 1}, {"propose": 1, "principled": 1, "algorithm": 1, "call": 1, "bootstrapping": 2, "error": 3, "accumulation": 2, "reduction": 1, "bear": 1, "control": 1, "practice": 1, "use": 1, "notion": 1, "supportset": 1, "match": 1, "prevent": 1}, {"systematic": 1, "experiment": 1, "show": 1, "effectiveness": 1, "method": 1, "continuouscontrol": 1, "mujoco": 1, "task": 1, "variety": 1, "offpolicy": 1, "datasets": 1, "generate": 1, "random": 1, "suboptimal": 1, "optimal": 1, "policies": 1}, {"bear": 1, "consistently": 1, "robust": 1, "train": 1, "dataset": 1, "match": 1, "exceed": 1, "stateoftheart": 1, "case": 1, "whereas": 1, "exist": 1, "algorithms": 1, "perform": 1, "well": 1, "specific": 1, "datasets": 1}, {"2": 1, "": 2, "relate": 1, "work": 2, "study": 1, "offpolicy": 1, "reinforcement": 1, "learn": 1, "static": 1, "datasets": 1}, {"errors": 1, "arise": 1, "inadequate": 1, "sample": 1, "distributional": 1, "shift": 1, "function": 1, "approximation": 1, "rigorously": 1, "study": 1, "error": 1, "propagation": 1, "approximate": 1, "dynamic": 1, "program": 1, "adp": 1, "4": 1, "27": 1, "10": 1, "33": 1}, {"work": 1, "often": 1, "study": 1, "bellman": 1, "errors": 1, "accumulate": 1, "propagate": 1, "nearby": 1, "state": 1, "via": 1, "bootstrapping": 1}, {"work": 1, "build": 1, "upon": 1, "tool": 1, "analysis": 1, "show": 1, "perform": 1, "bellman": 1, "backups": 1, "static": 1, "datasets": 1, "lead": 1, "error": 1, "accumulation": 1, "due": 1, "outofdistribution": 1, "value": 1}, {"approach": 1, "motivate": 1, "reduce": 1, "rate": 1, "propagation": 2, "error": 1, "state": 1}, {"approach": 1, "constrain": 1, "actor": 1, "update": 1, "action": 1, "remain": 1, "support": 1, "train": 1, "dataset": 1, "distribution": 1}, {"several": 1, "work": 1, "explore": 1, "similar": 1, "ideas": 1, "context": 1, "offpolicy": 1, "learn": 2, "online": 1, "settings": 1}, {"kakade": 1, "langford": 1, "22": 1, "show": 1, "large": 1, "policy": 2, "update": 2, "destructive": 1, "propose": 1, "conservative": 1, "iteration": 1, "scheme": 1, "constrain": 1, "actor": 1, "small": 1, "provably": 1, "convergent": 1, "learn": 1}, {"graumoya": 1, "et": 1, "al": 1}, {"16": 1, "use": 1, "learn": 1, "prior": 1, "action": 1, "maximum": 1, "entropy": 1, "rl": 1, "framework": 1, "25": 1, "justify": 1, "regularizer": 1, "base": 1, "mutual": 1, "information": 1}, {"however": 1, "none": 1, "methods": 1, "use": 1, "static": 1, "datasets": 1}, {"importance": 1, "sample": 1, "base": 1, "distribution": 1, "reweighting": 1, "29": 1, "15": 1, "30": 1, "26": 1, "also": 1, "explore": 1, "primarily": 1, "context": 1, "offpolicy": 1, "policy": 1, "evaluation": 1}, {"closely": 1, "relate": 1, "work": 1, "batchconstrained": 1, "qlearning": 1, "bcq": 1, "12": 1, "spibb": 1, "24": 1, "also": 1, "discuss": 1, "instability": 1, "arise": 1, "previously": 1, "unseen": 1, "action": 1}, {"fujimoto": 1, "et": 1, "al": 1}, {"12": 1, "show": 1, "convergence": 1, "properties": 1, "actionconstrained": 1, "bellman": 1, "backup": 1, "operator": 1, "tabular": 1, "errorfree": 1, "settings": 1}, {"prove": 1, "stronger": 1, "result": 1, "approximation": 1, "errors": 1, "provide": 1, "bind": 1, "suboptimality": 1, "solution": 1}, {"crucial": 1, "drive": 1, "design": 1, "choices": 1, "practical": 1, "algorithm": 1}, {"consequence": 1, "although": 1, "experimentally": 1, "find": 1, "12": 2, "outperform": 2, "standard": 1, "qlearning": 1, "methods": 1, "offpolicy": 2, "data": 2, "collect": 2, "expert": 1, "bear": 1, "suboptimal": 1, "policy": 1, "common": 1, "reallife": 1, "applications": 1}, {"empirically": 1, "find": 1, "bear": 1, "achieve": 1, "stronger": 1, "consistent": 1, "result": 1, "bcq": 1, "across": 1, "wide": 1, "variety": 1, "datasets": 1, "environments": 1}, {"explain": 1, "bcq": 2, "constraint": 1, "aggressive": 1, "generally": 1, "fail": 1, "substantially": 1, "improve": 2, "behavior": 1, "policy": 2, "method": 1, "actually": 1, "data": 1, "collection": 1, "suboptimal": 1, "random": 1}, {"spibb": 1, "24": 1, "like": 1, "bear": 1, "algorithm": 1, "base": 1, "constrain": 1, "learn": 1, "policy": 2, "support": 1, "behavior": 1}, {"however": 1, "author": 1, "extend": 1, "safe": 1, "performance": 1, "guarantee": 1, "batchconstrained": 1, "case": 2, "relax": 1, "supportconstrained": 1, "evaluate": 1, "highdimensional": 1, "control": 1, "task": 1}, {"3": 1, "": 7, "background": 1, "represent": 1, "environment": 1, "markov": 1, "decision": 1, "process": 1, "mdp": 1, "define": 1, "tuple": 1, "p": 2, "r": 1, "0": 3, "state": 2, "space": 2, "action": 1, "s0": 1, "transition": 1, "2": 1, "distribution": 2, "initial": 1, "rs": 1, "reward": 1, "function": 1, "1": 1, "discount": 1, "factor": 1}, {"goal": 1, "rl": 1, "find": 1, "policy": 1, "maximize": 1, "expect": 1, "cumulative": 1, "discount": 1, "reward": 1, "also": 1, "know": 1, "return": 1}, {"notation": 1, "": 3, "denote": 1, "p": 1, "discount": 1, "state": 2, "marginal": 1, "policy": 2, "define": 1, "average": 1, "visit": 1, "t0": 1, "pt": 1}, {"p": 1, "": 3, "shorthand": 1, "transition": 1, "matrix": 1, "s0": 1, "follow": 1, "certain": 1, "policy": 1, "ps0": 2, "e": 1}, {"qlearning": 1, "learn": 1, "optimal": 1, "stateaction": 1, "value": 1, "function": 1, "q": 1, "represent": 1, "expect": 1, "cumulative": 1, "discount": 1, "reward": 1, "start": 1, "take": 1, "action": 1, "act": 1, "optimally": 1, "thereafter": 1}, {"optimal": 1, "policy": 1, "recover": 1, "q": 1, "choose": 1, "maximize": 1, "action": 1}, {"qlearning": 1, "algorithms": 1, "base": 1, "iterate": 1, "bellman": 1, "optimality": 1, "operator": 1, "": 5, "define": 1, "qs": 1, "rs": 1, "et": 1, "s0": 1, "sa": 1, "max": 1, "qs0": 1, "a0": 1}, {"0": 1, "": 1, "state": 1, "space": 1, "large": 1, "represent": 1, "q": 2, "hypothesis": 1, "set": 1, "function": 1, "approximators": 1, "eg": 1, "neural": 1, "network": 1}, {"theory": 1, "estimate": 1, "qfunction": 1, "update": 1, "project": 1, "q": 3, "ie": 1, "minimize": 1, "mean": 1, "square": 1, "bellman": 1, "error": 1, "e": 1, "": 3, "q2": 1, "state": 1, "occupancy": 1, "measure": 1, "behaviour": 1, "policy": 1}, {"also": 1, "refer": 1, "qiteration": 1}, {"practice": 1, "empirical": 1, "estimate": 1, "q": 1, "form": 2, "sample": 1, "treat": 1, "supervise": 1, "2": 1, "regression": 1, "target": 1, "next": 1, "approximate": 1, "qfunction": 1, "iterate": 1}, {"large": 1, "action": 1, "space": 1, "eg": 1, "continuous": 1, "maximization": 1, "maxa0": 1, "qs0": 1, "": 2, "a0": 1, "generally": 1, "intractable": 1}, {"actorcritic": 1, "methods": 1, "35": 1, "13": 1, "18": 1, "address": 1, "additionally": 1, "learn": 1, "policy": 1, "": 1, "maximize": 1, "qfunction": 1}, {"work": 1, "study": 1, "offpolicy": 1, "learn": 1, "static": 1, "dataset": 1, "transition": 1, "": 2, "s0": 1, "rs": 1, "collect": 1, "unknown": 1, "behavior": 1, "policy": 1}, {"denote": 1, "distribution": 1, "state": 1, "action": 1, "induce": 1, "": 1}, {"4": 1, "": 2, "outofdistribution": 1, "action": 1, "qlearning": 2, "methods": 1, "often": 1, "fail": 1, "learn": 1, "static": 1, "halfcheetahv2": 2, "averagereturn": 1, "logq": 1, "offpolicy": 1, "data": 1, "show": 1, "figure": 1, "1": 1}, {"first": 1, "glance": 1, "resemble": 1, "overfitting": 1, "increase": 1, "size": 1, "static": 1, "dataset": 1, "rectify": 1, "problem": 1, "suggest": 1, "issue": 1, "complex": 1}, {"understand": 1, "source": 1, "instability": 1, "examine": 1, "form": 1, "bellman": 1, "backup": 1}, {"although": 1, "minimize": 1, "mean": 1, "square": 1, "bellman": 1, "error": 1, "correspond": 1, "supervise": 1, "regression": 2, "problem": 1, "target": 1, "figure": 1, "1": 1, "performance": 1, "sac": 1, "halfcheetahv2": 1, "selves": 1, "derive": 1, "current": 1, "qfunction": 1, "esti": 1, "return": 1, "leave": 1, "log": 1, "qvalues": 1, "right": 1, "offpolicy": 1, "mate": 1}, {"target": 1, "calculate": 1, "maximize": 1, "expert": 1, "data": 1, "wrt": 1}, {"number": 1, "train": 1, "sample": 1, "n": 1}, {"note": 1, "learn": 1, "qvalues": 1, "respect": 1, "action": 1, "large": 1, "discrepancy": 1, "return": 1, "negaat": 1, "next": 1, "state": 1}, {"however": 1, "qfunction": 1, "esti": 1, "tive": 1, "log": 1, "qvalues": 1, "large": 1, "positive": 1, "value": 1, "mator": 1, "reliable": 1, "input": 1, "solve": 1, "additional": 1, "sample": 1}, {"distribution": 1, "train": 1, "set": 1}, {"result": 2, "navely": 1, "maximize": 1, "value": 2, "may": 1, "evaluate": 1, "q": 1, "estimator": 1, "action": 1, "lie": 1, "far": 1, "outside": 1, "train": 1, "distribution": 1, "pathological": 1, "incur": 1, "large": 1, "error": 1}, {"refer": 1, "action": 2, "outofdistribution": 1, "ood": 1}, {"1000": 2, "": 27, "30": 1, "n1000": 2, "n10000": 2, "n100000": 2, "n1000000": 2, "750": 2, "500": 2, "250": 2, "25": 1, "20": 1, "0": 2, "15": 1, "10": 1, "5": 1, "00k": 2, "02k": 2, "04k": 2, "06k": 2, "trainsteps": 2, "08k": 2, "10k": 2, "formally": 1, "let": 2, "k": 3, "qk": 2, "q": 1, "denote": 2, "total": 1, "error": 2, "iteration": 1, "qlearning": 1, "qk1": 1, "current": 1, "bellman": 1}, {"k": 2, "": 5, "maxa0": 1, "es0": 1, "k1": 1, "s0": 1, "a0": 1}, {"word": 1, "errors": 2, "s0": 1, "": 2, "a0": 1, "discount": 1, "accumulate": 1, "new": 1, "k": 1, "current": 1, "iteration": 1}, {"expect": 1, "k": 1, "high": 1, "ood": 1, "state": 1, "action": 1, "errors": 1, "stateactions": 1, "never": 1, "directly": 1, "minimize": 1, "train": 1}, {"mitigate": 1, "bootstrapping": 1, "error": 1, "restrict": 1, "policy": 1, "ensure": 1, "output": 1, "action": 1, "lie": 1, "support": 1, "train": 1, "distribution": 1}, {"distinct": 1, "previous": 1, "work": 1, "eg": 1, "bcq": 1, "12": 1, "implicitly": 1, "constrain": 1, "distribution": 1, "learn": 1, "policy": 2, "close": 1, "behavior": 1, "similarly": 1, "behavioral": 1, "clone": 1, "31": 1}, {"sufficient": 1, "ensure": 1, "action": 1, "lie": 1, "train": 1, "set": 1, "high": 1, "probability": 1, "overly": 1, "restrictive": 1}, {"example": 1, "behavior": 1, "policy": 3, "close": 1, "uniform": 1, "learn": 2, "behave": 1, "randomly": 1, "result": 1, "poor": 1, "performance": 1, "even": 1, "data": 1, "sufficient": 1, "strong": 1, "see": 1, "figure": 1, "2": 1, "illustration": 1}, {"formally": 1, "mean": 1, "learn": 1, "policy": 2, "positive": 1, "density": 3, "behaviour": 1, "threshold": 1, "ie": 1, "": 5, "0": 1, "instead": 1, "closeness": 1, "constraint": 1, "value": 1, "3": 1}, {"analysis": 1, "instead": 1, "reveal": 1, "tradeoff": 1, "stay": 1, "within": 1, "data": 1, "distribution": 1, "find": 1, "suboptimal": 1, "solution": 1, "constraint": 1, "restrictive": 1}, {"analysis": 1, "motivate": 1, "us": 1, "restrict": 1, "support": 2, "learn": 1, "policy": 1, "probabilities": 1, "action": 1, "lie": 1, "within": 1}, {"avoid": 1, "evaluate": 1, "qfunction": 1, "estimator": 1, "ood": 1, "action": 1, "remain": 1, "flexible": 1, "order": 1, "find": 1, "performant": 1, "policy": 1}, {"propose": 1, "algorithm": 1, "leverage": 1, "insight": 1}, {"41": 1, "": 2, "distributionconstrained": 1, "backups": 1, "section": 1, "define": 1, "analyze": 1, "backup": 1, "operator": 1, "restrict": 2, "set": 2, "policies": 1, "use": 1, "maximization": 1, "qfunction": 1, "derive": 1, "performance": 1, "bound": 1, "depend": 1}, {"provide": 1, "motivation": 1, "constrain": 1, "policy": 1, "support": 1, "data": 1, "distribution": 1}, {"begin": 1, "definition": 2, "distributionconstrained": 2, "operator": 1, "41": 1, "operators": 1}, {"give": 1, "set": 1, "policies": 1, "": 10, "distributionconstrained": 1, "backup": 1, "operator": 1, "define": 1, "def": 2, "qs": 2, "e": 2, "rs": 1, "max": 2, "ep": 1, "s0": 2, "sa": 1, "v": 2}, {"": 4, "backup": 2, "operator": 1, "satisfy": 1, "properties": 1, "standard": 1, "bellman": 1, "convergence": 1, "fix": 1, "point": 1, "discuss": 1, "appendix": 1}, {"analyze": 1, "suboptimality": 1, "perform": 1, "backup": 1, "approximation": 1, "error": 2, "first": 1, "quantify": 1, "two": 1, "source": 1}, {"first": 1, "suboptimality": 1, "bias": 1}, {"optimal": 1, "policy": 2, "may": 1, "lie": 1, "outside": 1, "constraint": 1, "set": 1, "thus": 1, "suboptimal": 1, "solution": 1, "find": 1}, {"second": 1, "arise": 1, "distribution": 2, "shift": 1, "train": 1, "policies": 1, "use": 1, "backups": 1}, {"formalize": 1, "notion": 1, "ood": 1, "action": 1}, {"capture": 1, "suboptimality": 2, "final": 1, "solution": 1, "define": 1, "constant": 1, "measure": 1, "far": 1, "": 3}, {"definition": 1, "42": 1, "suboptimality": 1, "constant": 1}, {"suboptimality": 1, "constant": 1, "define": 1, "": 4, "max": 1, "q": 2}, {"sa": 1, "": 2, "next": 1, "define": 1, "concentrability": 1, "coefficient": 1, "28": 1, "quantify": 1, "far": 1, "visitation": 1, "distribution": 2, "generate": 1, "policies": 1, "train": 1, "data": 1}, {"constant": 1, "capture": 1, "degree": 1, "state": 1, "action": 1, "distribution": 1}, {"assumption": 1, "41": 1, "concentrability": 1}, {"let": 1, "0": 2, "denote": 2, "initial": 1, "state": 2, "distribution": 2, "train": 1, "data": 1, "": 8, "marginal": 1, "suppose": 1, "exist": 1, "coefficients": 1, "ck": 1, "1": 2, "k": 2, "p": 4, "2": 1, "cks": 1, "transition": 1, "operator": 1, "induce": 1}, {"define": 1, "concentrability": 1, "coefficient": 1, "c": 2, "": 3, "x": 1, "def": 1, "1": 1, "2": 1, "k": 1, "k1": 1, "ck": 1}, {"k1": 1, "": 7, "provide": 1, "intuition": 1, "c": 2, "generate": 1, "single": 1, "policy": 1, "singleton": 1, "set": 1, "would": 1, "1": 1, "smallest": 1, "possible": 1, "value": 1}, {"however": 1, "": 4, "contain": 2, "policies": 1, "far": 1, "value": 1, "could": 1, "large": 1, "potentially": 1, "infinite": 1, "support": 1}, {"bind": 1, "performance": 1, "approximate": 1, "distributionconstrained": 1, "qiteration": 1, "theorem": 1, "41": 1}, {"suppose": 1, "run": 1, "approximate": 1, "distributionconstrained": 1, "value": 1, "iteration": 1, "set": 1, "constrain": 1, "backup": 1, "": 2}, {"assume": 1, "": 3, "maxk": 1, "qk": 1, "qk1": 1, "bound": 1, "bellman": 1, "error": 1}, {"": 13, "1": 2, "ce": 1, "max": 1, "e": 1, "lim": 1, "e0": 1, "v": 2, "k": 2, "2": 1, "proof": 1}, {"see": 1, "appendix": 1, "b": 1, "theorem": 1, "b1": 1, "bind": 1, "formalize": 1, "tradeoff": 1, "keep": 2, "policies": 2, "choose": 1, "backups": 1, "close": 1, "data": 1, "capture": 3, "c": 1, "set": 1, "": 2, "large": 1, "enough": 1, "wellperforming": 1}, {"expand": 1, "set": 1, "policies": 1, "": 2, "increase": 1, "c": 1, "decrease": 1}, {"example": 1, "tradeoff": 1, "careful": 1, "choice": 1, "": 3, "yield": 1, "superior": 1, "result": 1, "give": 1, "tabular": 1, "4": 1, "figure": 1, "2": 1, "visualize": 1, "error": 1, "propagation": 1, "qlearning": 1, "various": 1, "choices": 1, "constraint": 1, "set": 1, "unconstrained": 1, "top": 1, "row": 1, "distributionconstrained": 1, "middle": 1, "constrain": 1, "behaviour": 1, "policy": 1, "policyevaluation": 1, "bottom": 1}, {"triangles": 1, "represent": 1, "qvalues": 1, "action": 1, "move": 1, "different": 1, "directions": 1}, {"task": 2, "leave": 1, "reach": 1, "bottomleft": 1, "corner": 1, "g": 1, "topleft": 1, "behaviour": 1, "policy": 1, "visualize": 1, "arrows": 1, "image": 2, "support": 2, "stateaction": 1, "pair": 1, "show": 1, "black": 1, "set": 1, "travel": 1, "bottomright": 1, "small": 1, "amount": 1, "greedy": 1, "exploration": 1}, {"dark": 1, "value": 2, "indicate": 2, "high": 1, "error": 2, "light": 1, "low": 1}, {"standard": 1, "backups": 1, "propagate": 1, "large": 1, "errors": 1, "lowsupport": 1, "regions": 2, "highsupport": 1, "lead": 1, "high": 1, "error": 1}, {"policy": 2, "evaluation": 1, "reduce": 1, "error": 1, "propagation": 1, "lowsupport": 1, "regions": 1, "introduce": 1, "significant": 1, "suboptimality": 1, "bias": 1, "data": 1, "optimal": 1}, {"carefully": 1, "choose": 1, "distributionconstrained": 1, "backup": 1, "strike": 1, "balance": 1, "two": 1, "extremes": 1, "confine": 1, "error": 1, "propagation": 1, "lowsupport": 1, "region": 1, "introduce": 1, "minimal": 1, "suboptimality": 1, "bias": 1}, {"gridworld": 1, "example": 1, "fig": 1}, {"2": 1, "visualize": 1, "errors": 1, "accumulate": 1, "distributionconstrained": 1, "qiteration": 1, "different": 1, "choices": 1, "": 1}, {"finally": 1, "motivate": 1, "use": 1, "support": 1, "set": 1, "construct": 1, "": 1}, {"interest": 1, "case": 1, "": 9, "0": 1, "whenever": 1, "behavior": 2, "policy": 2, "ie": 1, "set": 1, "policies": 1, "support": 1, "probable": 1, "regions": 1}, {"define": 1, "": 1, "way": 1, "allow": 1, "us": 1, "bind": 1, "concentrability": 1, "coefficient": 1, "theorem": 1, "42": 1}, {"assume": 1, "data": 1, "distribution": 1, "": 2, "generate": 1, "behavior": 1, "policy": 1}, {"let": 1, "marginal": 1, "state": 1, "distribution": 2, "data": 1}, {"define": 1, "": 12, "0": 1, "whenever": 1, "let": 1, "highest": 1, "discount": 1, "marginal": 1, "state": 2, "distribution": 2, "start": 1, "initial": 1, "follow": 1, "policies": 1, "time": 1, "step": 1, "thereafter": 1}, {"exist": 1, "concentrability": 1, "coefficient": 1, "c": 3, "": 16, "bound": 1, "1": 3, "f": 2, "def": 1, "minss": 1, "s0": 1, "0": 1}, {"proof": 1}, {"see": 1, "appendix": 1, "b": 1, "theorem": 1, "b2": 1, "qualitatively": 1, "f": 1, "": 2, "minimum": 1, "discount": 1, "visitation": 1, "marginal": 1, "state": 1, "behaviour": 1, "policy": 1, "action": 1, "likely": 1, "execute": 1, "environment": 1}, {"thus": 1, "use": 1, "support": 1, "set": 1, "give": 1, "us": 1, "single": 1, "lever": 1, "": 2, "simultaneously": 1, "trade": 1, "value": 1, "c": 1}, {"provide": 1, "theoretical": 1, "guarantee": 1, "see": 1, "experiment": 1, "sec": 1}, {"6": 1, "construct": 1, "": 1, "way": 1, "provide": 1, "simple": 1, "effective": 1, "method": 1, "implement": 1, "distributionconstrained": 1, "algorithms": 1}, {"intuitively": 1, "mean": 1, "prevent": 1, "increase": 1, "overall": 1, "error": 2, "qestimate": 1, "select": 1, "policies": 1, "support": 2, "train": 1, "action": 1, "distribution": 1, "would": 1, "ensure": 1, "roughly": 1, "bound": 1, "projection": 1, "k": 1, "reduce": 1, "suboptimality": 1, "bias": 1, "potentially": 1, "large": 1, "amount": 1}, {"bound": 1, "error": 1, "k": 1, "support": 1, "set": 2, "train": 1, "distribution": 1, "reasonable": 1, "assumption": 1, "use": 1, "highly": 1, "expressive": 1, "function": 1, "approximators": 1, "deep": 1, "network": 1, "especially": 1, "will": 1, "reweight": 1, "transition": 1, "32": 1, "11": 1}, {"elaborate": 1, "point": 1, "appendix": 1, "c": 1, "": 3, "5": 1, "bootstrapping": 2, "error": 2, "accumulation": 2, "reduction": 1, "bear": 1, "propose": 1, "practical": 1, "actorcritic": 1, "algorithm": 1, "build": 1, "framework": 1, "td3": 1, "13": 1, "sac": 1, "18": 1, "use": 1, "distributionconstrained": 1, "backups": 1, "reduce": 1}, {"key": 1, "insight": 1, "search": 1, "policy": 1, "support": 1, "train": 1, "distribution": 1, "5": 1, "": 1, "prevent": 1, "accidental": 1, "error": 1, "accumulation": 1}, {"algorithm": 1, "two": 1, "main": 1, "components": 1}, {"analogous": 1, "bcq": 1, "13": 1, "use": 3, "k": 1, "qfunctions": 1, "minimum": 1, "qvalue": 1, "policy": 2, "improvement": 1, "design": 1, "constraint": 1, "search": 1, "set": 1, "policies": 1, "": 2, "share": 1, "support": 1, "behaviour": 1}, {"components": 1, "appear": 1, "modifications": 1, "policy": 1, "improvement": 1, "step": 1, "actorcritic": 1, "style": 1, "algorithms": 1}, {"also": 1, "note": 1, "policy": 1, "improvement": 1, "perform": 1, "mean": 1, "k": 1, "qfunctions": 1, "find": 1, "scheme": 1, "work": 1, "good": 1, "experiment": 1}, {"denote": 1, "set": 1, "qfunctions": 1, "q1": 1, "": 6, "qk": 1}, {"policy": 2, "update": 1, "maximize": 1, "conservative": 1, "estimate": 1, "qvalues": 1, "within": 1, "": 12, "max": 1, "eas": 1, "min": 1, "qj": 1, "j1k": 1, "practice": 1, "behaviour": 1, "unknown": 1, "need": 1, "approximate": 1, "way": 1, "constrain": 1}, {"define": 1, "differentiable": 1, "constraint": 1, "approximately": 2, "constrain": 2, "": 2, "solve": 1, "optimization": 1, "problem": 1, "via": 1, "dual": 1, "gradient": 1, "descent": 1}, {"use": 1, "sample": 2, "version": 1, "maximum": 1, "mean": 1, "discrepancy": 1, "mmd": 1, "17": 1, "unknown": 1, "behaviour": 1, "policy": 1, "": 2, "actor": 1, "estimate": 1, "base": 1, "solely": 1, "distributions": 1}, {"give": 2, "sample": 2, "x1": 2, "": 32, "xn": 2, "p": 2, "y1": 2, "ym": 2, "q": 2, "mmd": 1, "mmd2": 1, "1": 2, "x": 4, "2": 1, "0": 2, "kx": 2, "kyj": 1, "yj": 1}, {"j": 1, "n2": 1, "0": 2, "nm": 1, "ij": 1, "m2": 1, "ii": 1, "": 3, "jj": 1, "k": 1, "universal": 1, "kernel": 1}, {"experiment": 1, "find": 1, "laplacian": 1, "gaussian": 1, "kernels": 1, "work": 1, "well": 1}, {"expression": 1, "mmd": 1, "involve": 1, "density": 1, "either": 1, "distribution": 1, "optimize": 1, "directly": 1, "sample": 1}, {"empirically": 1, "find": 1, "lowintermediate": 1, "sample": 2, "regime": 1, "mmd": 3, "p": 2, "q": 2, "similar": 1, "uniform": 1, "distribution": 1, "support": 2, "make": 1, "roughly": 1, "suit": 1, "constrain": 1, "distributions": 1, "give": 1, "set": 1}, {"see": 1, "appendix": 1, "c3": 1, "numerical": 1, "simulations": 1, "justify": 1, "approach": 1}, {"put": 1, "everything": 1, "together": 1, "optimization": 1, "problem": 1, "policy": 1, "improvement": 1, "step": 1, "": 4, "max": 1, "esd": 1, "eas": 1, "min": 1, "qj": 1, "st": 1}, {"esd": 1, "mmdds": 1, "": 6, "j1k": 1, "1": 1, "approximately": 1, "choose": 1, "threshold": 1}, {"choose": 1, "threshold": 1, "": 2, "005": 1, "experiment": 1}, {"algorithm": 2, "summarize": 1, "1": 1}, {"bear": 1, "connect": 1, "distributionconstrained": 1, "backups": 1, "describe": 1, "section": 1, "41": 1}, {"step": 1, "5": 1, "algorithm": 1, "restrict": 1, "": 2, "lie": 1, "support": 1}, {"insight": 1, "formally": 1, "justify": 1, "theorems": 1, "41": 1, "": 2, "42": 1, "c": 1, "bound": 1}, {"compute": 1, "distributionconstrained": 1, "backup": 1, "exactly": 1, "maximize": 1, "": 3, "intractable": 1, "practice": 1}, {"approximation": 1, "sample": 1, "dirac": 1, "policies": 1, "support": 1, "": 1, "alg": 1, "1": 1, "line": 1, "5": 1, "perform": 1, "empirical": 1, "maximization": 1, "compute": 1, "backup": 1}, {"maximization": 1, "perform": 1, "narrower": 1, "set": 1, "dirac": 1, "policies": 1, "ai": 1, "": 4, "bind": 1, "theorem": 1, "41": 1, "still": 1, "hold": 1}, {"empirically": 1, "show": 1, "section": 1, "6": 1, "approximation": 1, "sufficient": 1, "outperform": 1, "previous": 1, "methods": 1}, {"connection": 1, "briefly": 1, "discuss": 1, "appendix": 1, "c2": 1}, {"algorithm": 1, "1": 3, "bear": 1, "qlearning": 1, "bearql": 1, "input": 1, "": 16, "dataset": 1, "target": 3, "network": 2, "update": 1, "rate": 1, "minibatch": 1, "size": 1, "n": 2, "sample": 1, "action": 1, "mmd": 1, "minimum": 1, "k": 2, "initialize": 1, "qensemble": 1, "qi": 1, "i1": 2, "actor": 2, "lagrange": 1, "multiplier": 1, "qi0": 1, "0": 3, "2": 1}, {"": 1}, {"": 1}, {"": 83, "n": 3, "3": 2, "sample": 4, "minibatch": 1, "transition": 1, "r": 2, "s0": 4, "qupdate": 1, "4": 1, "p": 1, "action": 2, "ai": 4, "0": 7, "pi1": 1, "5": 2, "define": 1, "ys": 2, "maxai": 1, "minj1k": 1, "qj0": 2, "1": 4, "maxj1k": 1, "6": 2, "7": 1, "8": 1, "9": 1, "10": 1, "arg": 1, "mini": 1, "qi": 1, "a2": 1, "policyupdate": 1, "sm": 1, "i1": 1, "aj": 1, "dsj1": 1, "preferably": 1, "intermediate": 1, "integer110": 1, "update": 2, "minimize": 1, "equation": 1, "use": 1, "dual": 1, "gradient": 1, "descent": 1, "lagrange": 1, "multiplier": 1, "target": 1, "network": 1, "i0": 2, "end": 1, "halfcheetahv2": 1, "6000": 1, "walker2dv2": 1, "3500": 1, "bcq": 5, "bearql": 5, "naiverl": 4, "3000": 3, "5000": 1, "2500": 2, "4000": 1, "hopperv2": 1, "bc": 5, "dqfd": 4, "klc": 4, "antv2": 1, "1000": 4, "750": 1, "2000": 3, "500": 4, "1500": 2, "250": 2, "00k": 4, "02k": 4, "04k": 4, "06k": 3, "trainsteps": 4, "08k": 3, "10k": 3, "01k": 1, "03k": 1, "figure": 1, "average": 2, "performance": 1, "nave": 1, "rl": 1, "mediumquality": 1, "data": 1, "seed": 1}, {"bearql": 1, "outperform": 1, "bcq": 1, "nave": 1, "rl": 1}, {"average": 1, "return": 1, "train": 1, "data": 1, "indicate": 1, "magenta": 1, "line": 1}, {"one": 1, "step": 2, "xaxis": 1, "correspond": 1, "1000": 1, "gradient": 1}, {"summary": 1, "actor": 1, "update": 1, "towards": 1, "maximize": 1, "qfunction": 1, "still": 1, "constrain": 1, "remain": 1, "valid": 1, "search": 1, "space": 1, "define": 1, "": 2}, {"qfunction": 1, "use": 1, "action": 1, "sample": 1, "actor": 1, "perform": 1, "distributionconstrained": 1, "qlearning": 1, "reduce": 1, "set": 1, "policies": 1}, {"test": 1, "time": 1, "sample": 1, "p": 1, "action": 2, "": 1, "qvalue": 1, "maximize": 1, "execute": 1, "environment": 1}, {"implementation": 1, "detail": 1, "present": 1, "appendix": 1, "": 3, "6": 1, "experiment": 2, "study": 1, "bear": 1, "perform": 1, "learn": 1, "static": 1, "offpolicy": 1, "data": 1, "variety": 1, "continuous": 1, "control": 1, "benchmark": 1, "task": 1}, {"evaluate": 1, "algorithm": 1, "three": 1, "settings": 1, "dataset": 1, "generate": 1, "1": 1, "completely": 1, "random": 1, "behaviour": 1, "policy": 3, "2": 1, "partially": 1, "train": 1, "medium": 1, "score": 1, "3": 1, "optimal": 1}, {"condition": 1, "2": 1, "particular": 1, "interest": 1, "capture": 1, "many": 1, "common": 1, "usecases": 1, "practice": 1, "learn": 1, "imperfect": 1, "demonstration": 1, "data": 1, "eg": 1, "sort": 1, "commonly": 1, "available": 1, "autonomous": 1, "drive": 1, "14": 1, "reuse": 1, "previously": 1, "collect": 1, "experience": 1, "offpolicy": 1, "rl": 1}, {"compare": 1, "method": 1, "several": 1, "prior": 1, "methods": 1, "baseline": 2, "actorcritic": 2, "algorithm": 3, "td3": 1, "bcq": 1, "12": 1, "aim": 1, "address": 1, "similar": 1, "problem": 2, "discuss": 1, "section": 1, "4": 1, "klcontrol": 1, "21": 1, "solve": 1, "klpenalized": 1, "rl": 2, "similarly": 1, "maximum": 1, "entropy": 1, "static": 1, "version": 1, "dqfd": 1, "20": 1, "constraint": 1, "upweight": 1, "qvalues": 1, "stateaction": 1, "pair": 1, "observe": 1, "dataset": 1, "add": 1, "auxiliary": 1, "loss": 1, "top": 1, "regular": 1, "behaviour": 1, "clone": 1, "bc": 1, "simply": 1, "imitate": 1, "data": 1, "distribution": 1}, {"serve": 1, "measure": 1, "whether": 1, "method": 1, "actually": 1, "perform": 1, "effective": 1, "rl": 1, "simply": 1, "copy": 1, "data": 1}, {"report": 1, "average": 1, "evaluation": 1, "return": 1, "5": 1, "seed": 1, "policy": 1, "give": 1, "learn": 2, "algorithm": 2, "form": 1, "curve": 1, "function": 1, "number": 1, "gradient": 1, "step": 1, "take": 1}, {"sample": 1, "collect": 1, "evaluation": 1, "use": 1, "train": 1}, {"61": 1, "": 2, "performance": 1, "mediumquality": 1, "data": 3, "first": 1, "discuss": 1, "evaluation": 1, "condition": 2, "mediocre": 1, "2": 1, "resemble": 1, "settings": 1, "expect": 1, "train": 1, "offline": 1, "useful": 1}, {"collect": 1, "one": 1, "million": 1, "transition": 1, "partially": 1, "train": 1, "policy": 2, "simulate": 1, "imperfect": 1, "demonstration": 1, "data": 2, "mediocre": 1, "prior": 1}, {"scenario": 1, "find": 1, "bearql": 1, "consistently": 1, "outperform": 1, "bcq": 1, "12": 1, "nave": 1, "offpolicy": 1, "rl": 1, "baseline": 1, "td3": 1, "large": 1, "margins": 1, "show": 1, "figure": 1, "3": 1}, {"scenario": 1, "relevant": 1, "application": 1, "point": 1, "view": 1, "access": 1, "optimal": 1, "data": 2, "may": 1, "feasible": 1, "random": 1, "might": 1, "inadequate": 1, "exploration": 1, "efficient": 1, "learn": 1, "good": 1, "policy": 1}, {"also": 1, "evaluate": 1, "accuracy": 1, "learn": 1, "qfunctions": 1, "predict": 1, "actual": 1, "policy": 1, "return": 1}, {"trend": 1, "provide": 1, "appendix": 1, "e": 1, "note": 1, "performance": 2, "bcq": 2, "often": 1, "track": 1, "bc": 1, "baseline": 1, "suggest": 1, "primarily": 1, "imitate": 1, "data": 1}, {"klcontrol": 1, "baseline": 1, "use": 1, "automatic": 1, "temperature": 1, "tune": 1, "18": 1}, {"find": 1, "klcontrol": 1, "usually": 1, "perform": 1, "similar": 1, "worse": 1, "bc": 1, "whereas": 1, "dqfd": 1, "tend": 1, "diverge": 1, "often": 2, "due": 2, "cumulative": 1, "error": 1, "ood": 1, "action": 1, "exhibit": 1, "huge": 1, "variance": 1, "across": 1, "different": 1, "run": 1, "example": 1, "halfcheetahv2": 1, "environment": 1}, {"62": 1, "": 2, "performance": 2, "random": 2, "optimal": 1, "datasets": 1, "figure": 1, "5": 1, "show": 1, "method": 1, "train": 1, "data": 1, "policy": 2, "top": 1, "nearoptimal": 1, "bottom": 1}, {"case": 1, "method": 1, "bear": 1, "achieve": 1, "good": 1, "result": 1, "consistently": 1, "exceed": 1, "average": 1, "dataset": 1, "return": 2, "random": 1, "data": 2, "match": 1, "optimal": 2, "policy": 1}, {"nave": 1, "rl": 1, "also": 1, "often": 1, "well": 1, "random": 1, "data": 1}, {"random": 1, "data": 1, "policy": 1, "action": 1, "indistribution": 1, "since": 1, "equal": 1, "probability": 1}, {"consistent": 1, "hypothesis": 1, "7": 1, "": 103, "halfcheetahv2": 2, "bcq": 9, "bear": 1, "naiverl": 8, "5000": 3, "4000": 5, "walker2dv2": 2, "1000": 9, "bc": 9, "dqfd": 8, "klc": 8, "3000": 4, "bearql": 8, "800": 2, "hopperv2": 2, "600": 2, "400": 2, "1500": 2, "0": 8, "200": 2, "500": 4, "100": 1, "00k": 8, "02k": 8, "04k": 8, "06k": 6, "trainsteps": 8, "08k": 5, "10k": 5, "14000": 1, "10000": 1, "6000": 2, "12000": 1, "8000": 1, "01k": 2, "3500": 1, "2000": 7, "03k": 2, "antv2": 2, "2500": 1, "300": 1, "700": 1, "figure": 1, "5": 2, "average": 1, "performance": 1, "nave": 1, "rl": 1, "random": 1, "data": 2, "top": 1, "row": 2, "optimal": 1, "bottom": 1, "seed": 1}, {"bearql": 1, "algorithm": 1, "capable": 1, "learn": 1, "scenarios": 1}, {"nave": 1, "rl": 1, "cannot": 1, "handle": 1, "optimal": 1, "data": 2, "since": 1, "illustrate": 1, "mistake": 1, "bcq": 1, "favor": 1, "behavioral": 1, "clone": 2, "strategy": 1, "perform": 1, "quite": 1, "close": 1, "behaviour": 1, "case": 1, "cause": 1, "fail": 1, "random": 1}, {"average": 1, "return": 1, "train": 1, "dataset": 1, "indicate": 1, "dash": 1, "magenta": 1, "line": 1}, {"ood": 1, "action": 1, "one": 1, "main": 1, "source": 1, "error": 1, "offpolicy": 1, "learn": 1, "static": 1, "datasets": 1}, {"prior": 1, "bcq": 1, "method": 1, "12": 1, "perform": 2, "well": 1, "optimal": 1, "data": 2, "poorly": 1, "random": 1, "constraint": 1, "strict": 1}, {"result": 1, "show": 1, "bearql": 1, "robust": 1, "dataset": 1, "composition": 1, "learn": 1, "consistently": 1, "variety": 1, "settings": 1}, {"find": 1, "klcontrol": 1, "dqfd": 1, "unstable": 1, "settings": 1}, {"finally": 1, "figure": 1, "4": 1, "show": 1, "bear": 1, "outperform": 1, "consider": 1, "prior": 1, "methods": 1, "challenge": 1, "humanoidv2": 1, "environment": 1, "well": 1, "two": 1, "case": 1, "": 1, "mediumquality": 1, "data": 2, "random": 1}, {"63": 1, "": 2, "analysis": 1, "bearql": 2, "humanoidv2": 2, "section": 1, "aim": 1, "analyze": 1, "different": 1, "com": 1, "7000": 1, "bcq": 1, "bc": 1, "6000": 1, "dqfd": 1, "ponents": 1, "method": 1, "via": 1, "ablation": 1, "study": 1}, {"naiverl": 1, "klc": 1, "first": 1, "ablation": 1, "study": 1, "support": 2, "constraint": 1, "5000": 1, "discuss": 1, "section": 1, "5": 1, "use": 1, "mmd": 1, "mea": 1, "4000": 1, "3000": 1, "sure": 1}, {"replace": 1, "standard": 1, "2000": 1, "kldivergence": 1, "distribution": 1, "constraint": 1, "1000": 1, "measure": 1, "similarity": 1, "density": 1}, {"hypothesis": 1, "0": 1, "00k": 1, "02k": 1, "04k": 1, "06k": 1, "08k": 1, "10k": 1, "provide": 1, "conservative": 1, "trainsteps": 1, "constraint": 1, "since": 1, "match": 2, "distributions": 1, "necessary": 1, "support": 1}, {"kldivergence": 1, "figure": 2, "4": 1, "performance": 1, "bearql": 1, "bcq": 1, "nave": 1, "rl": 1, "perform": 2, "well": 1, "case": 1, "opti": 1, "bc": 1, "mediumquality": 1, "leave": 1, "random": 1, "right": 1, "data": 2, "mal": 1, "show": 1, "6": 1, "humanoidv2": 1, "environment": 1}, {"note": 1, "bearql": 1, "worse": 1, "mmd": 1, "mediumquality": 1, "data": 1}, {"even": 1, "outperform": 1, "prior": 1, "methods": 1}, {"kldivergence": 1, "hand": 1, "tune": 2, "fully": 1, "prevent": 1, "instability": 1, "issue": 1, "still": 1, "perform": 1, "worse": 1, "notwell": 1, "mmd": 1, "constraint": 1}, {"provide": 1, "result": 1, "set": 1, "appendix": 1}, {"also": 1, "vary": 1, "number": 1, "sample": 1, "n": 1, "use": 1, "compute": 1, "mmd": 1, "constraint": 1}, {"find": 1, "smaller": 1, "n": 1, "": 1, "4": 1, "5": 1, "give": 1, "better": 1, "performance": 1}, {"although": 1, "difference": 1, "large": 1, "consistently": 1, "better": 1, "performance": 1, "4": 1, "sample": 2, "lean": 1, "favour": 1, "hypothesis": 1, "intermediate": 1, "number": 1, "work": 1, "well": 1, "support": 1, "match": 1, "hence": 1, "less": 1, "restrictive": 1}, {"700": 1, "": 12, "bcq": 1, "bearql": 1, "naiverl": 1, "600": 1, "bc": 1, "dqfd": 1, "klc": 1, "500": 1, "400": 1, "300": 1, "200": 1, "100": 1, "0": 1, "00k": 1, "7": 1, "02k": 1, "04k": 1, "06k": 1, "trainsteps": 1, "08k": 1, "10k": 1, "discussion": 1, "future": 1, "work": 2, "goal": 1, "study": 1, "offpolicy": 1, "reinforcement": 1, "learn": 1, "static": 1, "datasets": 1}, {"theoretically": 1, "empirically": 1, "analyze": 1, "error": 1, "propagate": 1, "offpolicy": 1, "rl": 1, "due": 1, "use": 1, "outofdistribution": 1, "action": 1, "compute": 1, "target": 1, "value": 1, "bellman": 1, "backup": 1}, {"experiment": 1, "suggest": 1, "source": 1, "error": 1, "one": 1, "primary": 1, "issue": 2, "afflict": 1, "offpolicy": 1, "rl": 2, "increase": 1, "number": 1, "8": 1, "": 1, "sample": 1, "appear": 1, "mitigate": 1, "degradation": 2, "figure": 2, "1": 1, "train": 2, "nave": 1, "data": 2, "random": 1, "policy": 1, "outofdistribution": 1, "action": 1, "show": 1, "much": 1, "less": 1, "focus": 1, "policies": 1, "5": 1}, {"arm": 1, "insight": 1, "develop": 1, "method": 1, "mitigate": 1, "effect": 1, "outofdistribution": 1, "action": 1, "call": 1, "bearql": 1}, {"bearql": 1, "constrain": 2, "backup": 1, "use": 1, "action": 1, "nonnegligible": 1, "support": 1, "data": 1, "distribution": 1, "without": 1, "overly": 1, "conservative": 1, "learn": 1, "policy": 1}, {"observe": 1, "experimentally": 1, "bearql": 1, "achieve": 1, "good": 1, "performance": 1, "across": 2, "range": 2, "task": 1, "dataset": 1, "compositions": 1, "learn": 1, "well": 1, "random": 1, "mediumquality": 1, "expert": 1, "data": 1}, {"bearql": 1, "substantially": 1, "stabilize": 1, "offpolicy": 1, "rl": 1, "believe": 1, "problem": 1, "merit": 1, "mmd": 2, "vs": 1, "kl": 1, "constraint": 1, "mediocre": 1, "data": 1, "number": 1, "sample": 1, "2500": 1, "3000": 1, "study": 1}, {"one": 1, "limitation": 1, "current": 1, "2500": 1, "method": 1, "although": 1, "learn": 2, "policies": 1, "2000": 2, "performant": 1, "acquire": 1, "1500": 2, "nave": 1, "rl": 1, "performance": 1, "sometimes": 1, "still": 1, "tend": 1, "1000": 2, "degrade": 1, "long": 1, "run": 1}, {"excite": 1, "hopper4": 1, "hopper10": 1, "500": 2, "direction": 1, "future": 1, "work": 1, "would": 1, "develop": 1, "kl": 1, "walker2d4": 1, "mmd": 1, "walker2d10": 1, "early": 1, "stop": 1, "condition": 1, "rl": 1, "perhaps": 1, "0": 2, "00k": 2, "01k": 2, "02k": 2, "03k": 2, "04k": 2, "05k": 1, "trainsteps": 2, "generalize": 1, "notion": 1, "validation": 1, "error": 1, "reinforcement": 1, "learn": 1}, {"limitation": 1, "approach": 1, "perform": 1, "constrainedaction": 1, "se": 1, "figure": 1, "6": 1, "average": 2, "return": 1, "hopperv2": 1, "lection": 1, "overly": 1, "conservative": 1, "walker2dv2": 1, "function": 1, "train": 1, "step": 1, "ablation": 1, "compare": 1, "methods": 1, "constrain": 1, "state": 1, "study": 1, "section": 1, "63": 1}, {"mmd": 2, "constrain": 1, "optimization": 1, "stable": 1, "lead": 1, "better": 1, "return": 1, "b": 1, "4": 1, "sample": 1, "distributions": 1, "directly": 1, "especially": 1, "datasets": 1, "performant": 1, "10": 1, "collect": 1, "mixtures": 1, "policies": 1}, {"leave": 1, "future": 1, "work": 1, "design": 1, "algorithms": 1, "directly": 1, "constrain": 1, "state": 1, "distributions": 1}, {"theoretically": 1, "robust": 1, "method": 1, "support": 1, "match": 1, "efficiently": 1, "highdimensional": 1, "continuous": 1, "action": 1, "space": 1, "question": 1, "future": 1, "research": 1}, {"perhaps": 1, "methods": 1, "outside": 1, "rl": 1, "predominantly": 1, "use": 3, "domain": 1, "adaptation": 1, "asymmetric": 1, "fdivergences": 1, "37": 1, "support": 1, "restriction": 1}, {"another": 1, "promise": 1, "future": 1, "direction": 1, "examine": 1, "well": 1, "bearql": 1, "work": 1, "largescale": 1, "offpolicy": 1, "learn": 1, "problems": 1, "sort": 1, "likely": 1, "arise": 1, "domains": 1, "robotics": 1, "autonomous": 1, "drive": 1, "operations": 1, "research": 1, "commerce": 1}, {"rl": 1, "algorithms": 1, "learn": 3, "effectively": 1, "largescale": 1, "offpolicy": 1, "datasets": 2, "reinforcement": 1, "become": 1, "truly": 1, "datadriven": 1, "discipline": 1, "benefit": 1, "advantage": 1, "generalization": 2, "see": 1, "recent": 1, "years": 1, "supervise": 1, "field": 1, "large": 1, "enable": 1, "rapid": 1, "progress": 1, "term": 1, "accuracy": 1, "7": 1}, {"acknowledgements": 1, "thank": 1, "kristian": 1, "hartikainen": 1, "share": 1, "implementations": 1, "rl": 1, "algorithms": 1, "help": 1, "debug": 1, "certain": 1, "issue": 1}, {"thank": 1, "matthew": 1, "soh": 1, "help": 1, "set": 1, "environments": 1}, {"thank": 1, "aurick": 1, "zhou": 1, "chelsea": 1, "finn": 1, "abhishek": 1, "gupta": 1, "kelvin": 1, "xu": 1, "informative": 1, "discussions": 1}, {"thank": 1, "ofir": 1, "nachum": 1, "comment": 1, "earlier": 1, "draft": 1, "paper": 1}, {"thank": 1, "google": 1, "nvidia": 1, "amazon": 1, "provide": 1, "computational": 1, "resources": 1}, {"research": 1, "support": 1, "berkeley": 1, "deepdrive": 1, "jpmorgan": 1, "chase": 1, "": 1, "co": 1, "nsf": 1, "iis1651843": 1, "iis1614653": 1, "darpa": 1, "assure": 1, "autonomy": 1, "program": 1, "arl": 1, "dcist": 1, "cra": 1, "w911nf1720181": 1}, {"reference": 1, "1": 1, "andrs": 1, "antos": 1, "csaa": 1, "szepesvari": 1, "remi": 1, "munos": 1}, {"valueiteration": 1, "base": 1, "fit": 1, "policy": 1, "iteration": 1, "learn": 1, "single": 1, "trajectory": 1}, {"2007": 2, "ieee": 1, "international": 1, "symposium": 1, "approximate": 1, "dynamic": 1, "program": 1, "reinforcement": 1, "learn": 1, "page": 1, "330337": 1, "april": 1, "doi": 1, "101109": 1, "adprl2007368207": 1}, {"2": 1, "andrs": 1, "antos": 1, "csaba": 1, "szepesvri": 1, "rmi": 1, "munos": 1}, {"fit": 1, "qiteration": 1, "continuous": 1, "actionspace": 1, "mdps": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "20": 1, "page": 1, "916": 1}, {"curran": 1, "associate": 1, "inc": 1, "2008": 1}, {"3": 1, "jam": 1, "bennett": 1, "stan": 1, "lanning": 1, "et": 1, "al": 1}, {"netflix": 1, "prize": 1}, {"2007": 1}, {"4": 1, "dimitri": 1, "p": 1, "bertsekas": 1, "john": 1, "n": 1, "tsitsiklis": 1}, {"neurodynamic": 1, "program": 1}, {"athena": 1, "scientific": 1, "1996": 1}, {"9": 1, "": 1, "5": 1, "jonathon": 1, "byrd": 1, "zachary": 1, "lipton": 1}, {"effect": 1, "importance": 1, "weight": 1, "deep": 1, "learn": 1}, {"icml": 1, "2019": 1}, {"6": 1, "tim": 1, "de": 1, "bruin": 1, "jens": 1, "kober": 1, "karl": 1, "tuyls": 1, "robert": 1, "babuska": 1}, {"importance": 1, "experience": 1, "replay": 1, "database": 1, "composition": 1, "deep": 1, "reinforcement": 1, "learn": 1}, {"01": 1, "2015": 1}, {"7": 1, "jia": 1, "deng": 1, "wei": 1, "dong": 1, "richard": 1, "socher": 1, "lijia": 1, "li": 3, "kai": 1, "feifei": 1}, {"imagenet": 1, "largescale": 1, "hierarchical": 1, "image": 1, "database": 1}, {"cvpr09": 1, "2009": 1}, {"8": 1, "jacob": 1, "devlin": 1, "mingwei": 1, "chang": 1, "kenton": 1, "lee": 1, "kristina": 1, "toutanova": 1}, {"bert": 1, "pretraining": 1, "deep": 1, "bidirectional": 1, "transformers": 1, "language": 1, "understand": 1}, {"arxiv": 1, "preprint": 1, "arxiv181004805": 1, "2018": 1}, {"9": 1, "lasse": 1, "espeholt": 1, "hubert": 1, "soyer": 1, "remi": 1, "munos": 1, "karen": 1, "simonyan": 1, "volodymir": 1, "mnih": 1, "tom": 1, "ward": 1, "yotam": 1, "doron": 1, "vlad": 1, "firoiu": 1, "tim": 1, "harley": 1, "iain": 1, "dun": 1, "et": 1, "al": 1}, {"impala": 1, "scalable": 1, "distribute": 1, "deeprl": 1, "importance": 1, "weight": 1, "actorlearner": 1, "architectures": 1}, {"proceed": 1, "international": 1, "conference": 1, "machine": 1, "learn": 1, "icml": 1, "2018": 1}, {"10": 1, "amirmassoud": 1, "farahmand": 1, "csaba": 1, "szepesvri": 1, "rmi": 1, "munos": 1}, {"error": 1, "propagation": 1, "approximate": 1, "policy": 1, "value": 1, "iteration": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "page": 1, "568576": 1, "2010": 1}, {"11": 1, "justin": 1, "fu": 1, "aviral": 1, "kumar": 1, "matthew": 1, "soh": 1, "sergey": 1, "levine": 1}, {"diagnose": 1, "bottleneck": 1, "deep": 1, "qlearning": 1, "algorithms": 1}, {"arxiv": 1, "preprint": 1, "arxiv190210250": 1, "2019": 1}, {"12": 1, "scott": 1, "fujimoto": 1, "david": 1, "meger": 1, "doina": 1, "precup": 1}, {"offpolicy": 1, "deep": 1, "reinforcement": 1, "learn": 1, "without": 1, "exploration": 1}, {"arxiv": 1, "preprint": 1, "arxiv181202900": 1, "2018": 1}, {"13": 1, "scott": 1, "fujimoto": 1, "herke": 1, "van": 1, "hoof": 1, "david": 1, "meger": 1}, {"address": 1, "function": 1, "approximation": 1, "error": 1, "actorcritic": 1, "methods": 1}, {"jennifer": 1, "dy": 1, "andreas": 1, "krause": 1, "editors": 1, "proceed": 2, "35th": 1, "international": 1, "conference": 1, "machine": 2, "learn": 2, "volume": 1, "80": 1, "research": 1, "page": 1, "15871596": 1}, {"pmlr": 1, "2018": 1}, {"14": 1, "yang": 1, "gao": 1, "huazhe": 1, "xu": 1, "ji": 1, "lin": 1, "fisher": 1, "yu": 1, "sergey": 1, "levine": 1, "trevor": 1, "darrell": 1}, {"reinforcement": 1, "learn": 1, "imperfect": 1, "demonstrations": 1}, {"iclr": 1, "workshop": 1}, {"openreviewnet": 1, "2018": 1}, {"15": 1, "carles": 1, "gelada": 1, "marc": 1, "g": 1, "bellemare": 1}, {"offpolicy": 1, "deep": 1, "reinforcement": 1, "learn": 1, "bootstrapping": 1, "covariate": 1, "shift": 1}, {"corr": 1, "abs190109455": 1, "2019": 1}, {"16": 1, "jordi": 1, "graumoya": 1, "felix": 1, "leibfried": 1, "peter": 1, "vrancx": 1}, {"soft": 1, "qlearning": 1, "mutualinformation": 1, "regularization": 1}, {"international": 1, "conference": 1, "learn": 1, "representations": 1, "2019": 1}, {"url": 1, "https": 1, "openreviewnetforumidhyetjocqfx": 1}, {"17": 1, "arthur": 1, "gretton": 1, "karsten": 1, "borgwardt": 1, "malte": 1, "j": 1, "rasch": 1, "bernhard": 1, "schlkopf": 1, "alexander": 1, "smola": 1}, {"kernel": 1, "twosample": 1, "test": 1}, {"j": 1, "mach": 1}, {"learn": 1}, {"res": 1, "13723773": 1, "march": 1, "2012": 1}, {"issn": 1, "15324435": 1}, {"url": 1, "httpdlacmorgcitationcfmid21883852188410": 1}, {"18": 1, "tuomas": 1, "haarnoja": 1, "aurick": 1, "zhou": 1, "pieter": 1, "abbeel": 1, "sergey": 1, "levine": 1}, {"soft": 1, "actorcritic": 1, "offpolicy": 1, "maximum": 1, "entropy": 1, "deep": 1, "reinforcement": 1, "learn": 1, "stochastic": 1, "actor": 1}, {"arxiv": 1, "preprint": 1, "arxiv180101290": 1, "2018": 1}, {"19": 1, "kaiming": 1, "xiangyu": 1, "zhang": 1, "shaoqing": 1, "ren": 1, "jian": 1, "sun": 1}, {"deep": 1, "residual": 1, "learn": 1, "image": 1, "recognition": 1}, {"2016": 2, "ieee": 1, "conference": 1, "computer": 1, "vision": 1, "pattern": 1, "recognition": 1, "cvpr": 1, "page": 1, "770778": 1}, {"20": 1, "todd": 1, "hester": 1, "matej": 1, "vecerik": 1, "olivier": 1, "pietquin": 1, "marc": 1, "lanctot": 1, "tom": 1, "schaul": 1, "bilal": 1, "piot": 1, "dan": 1, "horgan": 1, "john": 1, "quan": 1, "andrew": 1, "sendonaris": 1, "ian": 1, "osband": 1, "et": 1, "al": 1}, {"deep": 1, "qlearning": 1, "demonstrations": 1}, {"thirtysecond": 1, "aaai": 1, "conference": 1, "artificial": 1, "intelligence": 1, "2018": 1}, {"21": 1, "natasha": 1, "jaques": 1, "asma": 1, "ghandeharioun": 1, "judy": 1, "hanwen": 1, "shen": 1, "craig": 1, "ferguson": 1, "gata": 1, "lapedriza": 1, "noah": 1, "jones": 1, "shixiang": 1, "gu": 1, "rosalind": 1, "w": 1, "picard": 1}, {"way": 1, "offpolicy": 1, "batch": 1, "deep": 1, "reinforcement": 1, "learn": 1, "implicit": 1, "human": 1, "preferences": 1, "dialog": 1}, {"corr": 1, "abs190700456": 1, "2019": 1}, {"url": 1, "http": 1, "arxivorgabs190700456": 1}, {"22": 1, "sham": 1, "kakade": 1, "john": 1, "langford": 1}, {"approximately": 1, "optimal": 1, "approximate": 1, "reinforcement": 1, "learn": 1}, {"proceed": 1, "nineteenth": 1, "international": 1, "conference": 1, "machine": 1, "learn": 1, "page": 1, "267": 1, "274": 1}, {"morgan": 1, "kaufmann": 1, "publishers": 1, "inc": 1, "2002": 1}, {"23": 1, "dmitry": 1, "kalashnikov": 1, "alex": 1, "irpan": 1, "peter": 1, "pastor": 1, "julian": 1, "ibarz": 1, "alexander": 1, "herzog": 1, "eric": 1, "jang": 1, "deirdre": 1, "quillen": 1, "ethan": 1, "holly": 1, "mrinal": 1, "kalakrishnan": 1, "vincent": 1, "vanhoucke": 1, "sergey": 1, "levine": 1}, {"scalable": 1, "deep": 1, "reinforcement": 1, "learn": 1, "visionbased": 1, "robotic": 1, "manipulation": 1}, {"proceed": 2, "10": 1, "": 6, "24": 1, "25": 1, "26": 1, "27": 1, "28": 1, "29": 1, "30": 1, "31": 1, "32": 1, "33": 1, "34": 1, "35": 1, "36": 1, "37": 1, "38": 1, "2nd": 1, "conference": 1, "robot": 1, "learn": 2, "volume": 1, "87": 1, "machine": 1, "research": 1, "page": 1, "651673": 1}, {"pmlr": 1, "2018": 1}, {"romain": 1, "laroche": 1, "paul": 1, "trichelair": 1, "remi": 1, "tachet": 1, "des": 1, "comb": 1}, {"safe": 1, "policy": 1, "improvement": 1, "baseline": 1, "bootstrapping": 1}, {"international": 1, "conference": 1, "machine": 1, "learn": 1, "icml": 1, "2019": 1}, {"sergey": 1, "levine": 1}, {"reinforcement": 1, "learn": 1, "control": 1, "probabilistic": 1, "inference": 1, "tutorial": 1, "review": 1}, {"corr": 1, "abs180500909": 1, "2018": 1}, {"url": 1, "httparxivorgabs180500909": 1}, {"rupam": 1, "mahmood": 1, "huizhen": 1, "yu": 1, "martha": 1, "white": 1, "richard": 1, "sutton": 1}, {"emphatic": 1, "temporaldifference": 1, "learn": 1}, {"arxiv": 1, "preprint": 1, "arxiv150701569": 1, "2015": 1}, {"rmi": 1, "munos": 1}, {"error": 1, "bound": 1, "approximate": 1, "policy": 1, "iteration": 1}, {"proceed": 1, "twentieth": 1, "international": 2, "conference": 2, "machine": 1, "learn": 1, "page": 1, "560567": 1}, {"aaai": 1, "press": 1, "2003": 1}, {"rmi": 1, "munos": 1}, {"error": 1, "bound": 1, "approximate": 1, "value": 1, "iteration": 1}, {"proceed": 1, "national": 1, "conference": 1, "artificial": 1, "intelligence": 1, "2005": 1}, {"rmi": 1, "munos": 1, "tom": 1, "stepleton": 1, "anna": 1, "harutyunyan": 1, "marc": 1, "bellemare": 1}, {"safe": 1, "efficient": 1, "offpolicy": 1, "reinforcement": 1, "learn": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "page": 1, "10541062": 1, "2016": 1}, {"doina": 1, "precup": 1, "richard": 1, "sutton": 1, "sanjoy": 1, "dasgupta": 1}, {"offpolicy": 1, "temporaldifference": 1, "learn": 1, "function": 1, "approximation": 1}, {"international": 1, "conference": 1, "machine": 1, "learn": 1, "icml": 1, "2001": 1}, {"stefan": 1, "schaal": 1}, {"imitation": 1, "learn": 1, "route": 1, "humanoid": 1, "robots": 1, "1999": 1}, {"tom": 1, "schaul": 1, "john": 1, "quan": 1, "ioannis": 1, "antonoglou": 1, "david": 1, "silver": 1}, {"prioritize": 1, "experience": 1, "replay": 1}, {"corr": 1, "abs151105952": 1, "2016": 1}, {"bruno": 1, "scherrer": 1, "mohammad": 1, "ghavamzadeh": 1, "victor": 1, "gabillon": 1, "boris": 1, "lesner": 1, "matthieu": 1, "geist": 1}, {"approximate": 1, "modify": 1, "policy": 1, "iteration": 1, "application": 1, "game": 1, "tetris": 1}, {"journal": 1, "machine": 1, "learn": 1, "research": 1, "1616291676": 1, "2015": 1}, {"url": 1, "httpjmlrorgpapersv16": 1, "scherrer15ahtml": 1}, {"john": 1, "schulman": 1, "sergey": 1, "levine": 1, "pieter": 1, "abbeel": 1, "michael": 1, "jordan": 1, "philipp": 1, "moritz": 1}, {"trust": 1, "region": 1, "policy": 1, "optimization": 1}, {"francis": 1, "bach": 1, "david": 1, "blei": 1, "editors": 1, "proceed": 2, "32nd": 1, "international": 1, "conference": 1, "machine": 2, "learn": 2, "volume": 1, "37": 1, "research": 1, "page": 1, "18891897": 1, "lille": 1, "france": 1, "0709": 1, "jul": 1, "2015": 1}, {"pmlr": 1}, {"richard": 1, "sutton": 1, "andrew": 1, "g": 1, "barto": 1}, {"reinforcement": 1, "learn": 1, "introduction": 1}, {"second": 1, "edition": 1, "2018": 1}, {"emanuel": 1, "todorov": 1, "tom": 1, "erez": 1, "yuval": 1, "tassa": 1}, {"mujoco": 1, "physics": 1, "engine": 1, "modelbased": 1, "control": 1}, {"iros": 1, "page": 1, "50265033": 1, "2012": 1}, {"yifan": 1, "wu": 1, "ezra": 1, "winston": 1, "divyansh": 1, "kaushik": 1, "zachary": 1, "lipton": 1}, {"domain": 1, "adaptation": 1, "asymmetricallyrelaxed": 1, "distribution": 1, "alignment": 1}, {"icml": 1, "2019": 1}, {"fisher": 1, "yu": 1, "wenqi": 1, "xian": 1, "yingying": 1, "chen": 1, "fangchen": 1, "liu": 1, "mike": 1, "liao": 1, "vashisht": 1, "madhavan": 1, "trevor": 1, "darrell": 1}, {"bdd100k": 1, "diverse": 1, "drive": 1, "video": 1, "database": 1, "scalable": 1, "annotation": 1, "tool": 1}, {"corr": 1, "abs180504687": 1, "2018": 1}, {"url": 1, "httparxivorgabs180504687": 1}, {"11": 1}]
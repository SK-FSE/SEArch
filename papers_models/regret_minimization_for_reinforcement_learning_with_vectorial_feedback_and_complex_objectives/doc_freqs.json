[{"regret": 1, "minimization": 1, "reinforcement": 1, "learn": 1, "vectorial": 1, "feedback": 1, "complex": 1, "objectives": 1, "": 2, "wang": 1, "chi": 1, "cheung": 1, "department": 1, "industrial": 1, "systems": 1, "engineer": 1, "management": 1, "national": 1, "university": 1, "singapore": 1, "isecwcnusedusg": 1, "abstract": 1, "consider": 1, "agent": 1, "involve": 1, "online": 1, "markov": 1, "decision": 1, "process": 1, "receive": 1, "vector": 1, "outcomes": 1, "every": 1, "round": 1}, {"agent": 1, "optimize": 1, "aggregate": 1, "reward": 1, "function": 1, "multidimensional": 1, "outcomes": 1}, {"due": 1, "state": 1, "transition": 1, "challenge": 1, "balance": 1, "contribution": 1, "dimension": 1, "achieve": 1, "nearoptimality": 1}, {"contrary": 1, "single": 1, "objective": 1, "case": 1, "stationary": 1, "policies": 1, "generally": 1, "suboptimal": 1}, {"propose": 1, "noregret": 1, "algorithm": 2, "base": 1, "frankwolfe": 1, "frank": 1, "wolfe": 1, "1956": 1, "agrawal": 1, "devanur": 1, "2014": 1, "": 1, "ucrl2": 1, "jaksch": 1, "et": 1, "al": 1}, {"2010": 1, "well": 1, "crucial": 1, "novel": 1, "gradient": 1, "threshold": 1, "procedure": 1, "gtp": 1}, {"gtp": 1, "involve": 1, "carefully": 1, "delay": 1, "gradient": 1, "update": 1, "return": 1, "nonstationary": 1, "policy": 1, "diversify": 1, "outcomes": 1, "optimize": 1, "aggregate": 1, "reward": 1}, {"1": 1, "": 2, "introduction": 1, "markov": 1, "decision": 1, "process": 1, "mdps": 1, "model": 1, "sequential": 1, "optimization": 1, "problems": 1, "change": 1, "state": 1, "underlie": 1, "environment": 1}, {"time": 1, "agent": 1, "perform": 1, "action": 1, "contingent": 1, "upon": 1, "current": 1, "state": 1}, {"influence": 1, "present": 1, "state": 2, "action": 1, "agent": 1, "transit": 1, "another": 1, "receive": 1, "form": 1, "feedback": 1}, {"typically": 1, "feedback": 1, "scalar": 1, "reward": 2, "agent": 1, "aim": 1, "maximize": 1, "total": 1}, {"nevertheless": 1, "many": 1, "settings": 1, "feedback": 1, "vector": 1, "multiple": 1, "outcomes": 2, "agents": 1, "goal": 1, "depend": 1}, {"moreover": 1, "underlie": 1, "mdp": 1, "model": 1, "usually": 1, "know": 1, "agent": 1, "learn": 1, "onthefly": 1}, {"motivate": 1, "situations": 1, "consider": 1, "complexobjective": 1, "online": 1, "mdp": 1, "coomdp": 1, "problem": 1, "maximize": 1, "aggregate": 1, "function": 1, "average": 1, "vectorial": 1, "outcome": 1}, {"solve": 1, "coomdp": 1, "problem": 1, "require": 1, "overcome": 1, "follow": 1, "subtle": 1, "challenge": 1}, {"maximize": 1, "aggregate": 1, "function": 1, "agent": 1, "balance": 1, "contributions": 1, "outcomes": 1, "different": 3, "components": 1, "alternate": 1, "among": 1, "action": 1, "generally": 1, "associate": 1, "state": 1}, {"consequently": 1, "agent": 1, "traverse": 1, "state": 2, "space": 1, "could": 1, "require": 1, "visit": 1, "suboptimal": 1, "contribute": 1, "maximization": 1, "aggregate": 1, "function": 1}, {"altogether": 1, "maximization": 1, "hinder": 1, "undesirable": 1, "state": 1, "transition": 1, "worsen": 1, "agents": 1, "model": 1, "uncertainty": 1}, {"overcome": 1, "mention": 1, "challenge": 1, "propose": 1, "tfwucrl2": 1, "nearoptimal": 1, "online": 1, "algorithm": 1, "coomdp": 1, "problem": 1}, {"algorithm": 2, "build": 1, "upon": 1, "frankwolfe": 1, "fw": 1, "21": 1, "2": 1, "ucrl2": 1, "28": 1, "well": 1, "novel": 1, "gradient": 1, "threshold": 1, "procedure": 1, "gtp": 1}, {"fw": 1, "balance": 1, "objectives": 1, "scalarizing": 1, "outcomes": 1, "ucrl2": 1, "solve": 1, "scalarized": 1, "online": 1, "mdp": 1, "problems": 1, "model": 1, "uncertainty": 1}, {"however": 1, "fw": 1, "ucrl2": 1, "enough": 1, "overcome": 1, "challenge": 1, "balance": 1, "outcomes": 1, "avoid": 1, "suboptimal": 1, "state": 1}, {"gtp": 1, "overcome": 1, "challenge": 1, "judiciously": 1, "delay": 1, "gradient": 1, "update": 1, "fw": 1}, {"procedure": 1, "approximately": 1, "maintain": 1, "balance": 1, "effect": 1, "fw": 1, "limit": 1, "visit": 1, "suboptimal": 1, "state": 1, "switch": 1, "among": 1, "different": 1, "stationary": 1, "policies": 1, "adaptively": 1, "infrequently": 1, "despite": 1, "model": 1, "uncertainty": 1}, {"33rd": 1, "conference": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "neurips": 1, "2019": 1, "vancouver": 1, "canada": 1}, {"relate": 1, "literature": 1}, {"coomdp": 1, "problem": 3, "closely": 1, "relate": 1, "bandits": 1, "global": 1, "concave": 1, "reward": 1, "bwr": 1, "2": 1, "scalarobejctive": 1, "online": 1, "mdps": 1, "soomdp": 1, "28": 1}, {"bwr": 1, "concern": 1, "maximize": 1, "aggregate": 1, "function": 1, "vectorial": 1, "feedback": 1, "stochastic": 1, "bandit": 1, "settings": 1}, {"bwr": 2, "first": 1, "study": 1, "2": 1, "solve": 1, "synergy": 1, "online": 1, "convex": 1, "optimization": 1, "upper": 1, "confidence": 1, "bind": 1, "ucb": 1, "algorithms": 1}, {"subsequently": 1, "bwr": 1, "study": 1, "various": 1, "stochastic": 1, "bandit": 1, "settings": 1, "reward": 1, "function": 1, "4": 1, "17": 1, "14": 1}, {"bwr": 1, "closely": 1, "relate": 1, "bandit": 2, "knapsacks": 1, "problem": 1, "bwk": 1, "model": 1, "stochastic": 1, "problems": 1, "resource": 1, "constraints": 1}, {"bwk": 1, "first": 1, "study": 2, "10": 1, "subsequently": 1, "various": 1, "settings": 1, "2": 1, "11": 1, "3": 1, "20": 1}, {"model": 1, "bwr": 1, "bwk": 1, "assume": 1, "iid": 1}, {"outcomes": 1, "across": 1, "time": 1, "mdp": 1, "set": 1, "outcome": 1, "distribution": 1, "change": 1, "endogenously": 1, "accord": 1, "state": 1, "transition": 1}, {"adversarial": 1, "bwk": 1, "problem": 1, "recently": 1, "study": 1, "27": 1}, {"among": 1, "result": 1, "show": 1, "online": 1, "algorithm": 1, "achieve": 1, "expect": 1, "value": 1, "1": 1, "log": 1, "": 1, "time": 2, "offline": 1, "optimum": 1, "number": 1, "step": 1}, {"positive": 2, "result": 3, "coomdps": 1, "walk": 1, "fine": 1, "line": 1, "negative": 1, "adversarial": 1, "bwk": 2, "stochastic": 1, "bwr": 1}, {"online": 1, "optimization": 1, "problems": 1, "global": 1, "reward": 1, "function": 1, "study": 1, "adversarial": 1, "settings": 1, "full": 1, "feedback": 1, "19": 1, "9": 1}, {"soomdp": 1, "problem": 1, "first": 1, "study": 1, "8": 1, "28": 1, "communicate": 1, "mdps": 1}, {"subsequently": 1, "problem": 1, "study": 1, "general": 1, "case": 1, "weakly": 1, "communicate": 1, "mdps": 2, "13": 1, "23": 1, "noncommunicating": 1, "22": 1}, {"posterior": 1, "sample": 1, "algorithms": 1, "soomdp": 1, "problem": 1, "propose": 1, "analyze": 1, "5": 1, "37": 1}, {"soomdp": 1, "problem": 1, "also": 1, "study": 1, "certain": 1, "mix": 1, "time": 1, "assumptions": 1, "stationary": 1, "policies": 1, "36": 1}, {"soomdp": 1, "problem": 1, "assume": 1, "scalar": 1, "reward": 1, "incorporate": 1, "multiobjective": 1, "optimization": 1}, {"review": 1, "mdps": 1, "please": 1, "consult": 1, "38": 1, "15": 1}, {"reinforcement": 1, "learn": 1, "rl": 1, "vectorial": 1, "feedback": 1, "aggregate": 1, "function": 1, "study": 1, "discountedreward": 1, "set": 2, "24": 1, "6": 2, "12": 1, "43": 1, "1": 1, "25": 1, "42": 1, "29": 1, "30": 1, "33": 1, "averagereward": 1, "32": 1, "41": 1, "40": 1}, {"study": 1, "latter": 1, "online": 1, "model": 2, "uncertainty": 1}, {"work": 1, "show": 2, "nonasymptotic": 1, "convergence": 2, "optimum": 1, "differ": 1, "6": 1, "32": 1, "asymptotic": 1}, {"tarbouriech": 1, "lazaric": 1, "41": 1, "study": 1, "online": 1, "model": 1, "state": 1, "space": 1, "exploration": 1, "achieve": 1, "nonasymptotic": 1, "convergence": 1, "optimum": 1}, {"addition": 1, "choice": 1, "aggregate": 1, "function": 1, "work": 1, "differ": 1, "41": 1, "two": 1, "aspects": 1}, {"first": 1, "transition": 1, "kernel": 2, "assume": 1, "know": 2, "41": 1, "whereas": 1, "model": 1}, {"second": 1, "reachability": 1, "assumption": 2, "unichain": 1, "mdps": 2, "make": 2, "41": 1, "much": 1, "weaker": 1, "communicate": 1}, {"recently": 1, "online": 1, "mdps": 1, "adversarially": 1, "choose": 1, "aggregate": 1, "function": 1, "study": 1, "40": 1}, {"model": 1, "40": 1, "episodic": 1, "state": 2, "reset": 1, "fix": 2, "end": 1, "episode": 1, "involve": 1, "number": 1, "step": 1}, {"contrast": 1, "set": 1, "involve": 1, "state": 1, "reset": 1}, {"40": 1, "aggregate": 2, "function": 2, "apply": 2, "trajectory": 2, "episode": 1, "whereas": 1, "set": 1, "across": 1, "whole": 1, "horizon": 1}, {"finally": 1, "point": 1, "substantial": 1, "generalization": 1, "current": 1, "paper": 1, "put": 1, "forth": 1, "18": 1}, {"discount": 1, "reward": 1, "set": 1, "multiobjective": 1, "optimization": 1, "study": 1, "24": 1, "6": 1, "12": 1, "43": 1}, {"many": 1, "recent": 1, "work": 1, "study": 1, "discountedreward": 1, "set": 1, "resource": 1, "constraints": 1, "1": 1, "42": 1, "29": 1, "33": 1}, {"numerous": 1, "recent": 1, "research": 1, "work": 1, "focus": 1, "state": 1, "space": 1, "exploration": 1, "problems": 1, "25": 1, "30": 1, "discountedreward": 1, "set": 1}, {"constrain": 1, "mdps": 1, "review": 1, "6": 1, "multiobjective": 1, "rl": 1, "survey": 1, "39": 1, "31": 1}, {"2": 1, "": 3, "problem": 1, "definition": 1, "coomdp": 2, "instance": 1, "specify": 1, "s1": 1, "p": 1, "v": 1, "g": 1}, {"set": 1, "finite": 1, "state": 2, "space": 1, "s1": 1, "": 1, "start": 1}, {"collection": 1, "": 4, "ss": 1, "contain": 1, "finite": 1, "set": 1, "action": 1, "state": 1, "say": 1, "stateaction": 1, "pair": 1, "iff": 1}, {"collections": 1, "p": 1, "": 2, "ps": 1, "assaas": 2, "transition": 1, "kernel": 1, "collection": 1, "v": 1, "vs": 1, "govern": 1, "vectorial": 1, "outcomes": 1}, {"agent": 1, "choose": 1, "action": 1, "": 3, "state": 2, "subsequent": 1, "s0": 1, "distribute": 1, "ps": 1}, {"receive": 1, "stochastic": 1, "vectorial": 1, "outcome": 1, "v": 1, "": 5, "0": 2, "1k": 1, "distribute": 1, "vs": 2, "mean": 1, "ev": 1, "vk": 1, "ak": 1, "k1": 1}, {"emphasize": 1, "": 2, "v1": 1}, {"": 1}, {"": 1}, {"": 1, "vk": 1, "arbitrarily": 1, "correlate": 1}, {"focus": 1, "follow": 1, "reward": 1, "function": 1, "g": 1, "": 7, "0": 1, "1k": 1, "r0": 2, "parameterized": 1, "l0": 1, "l1": 1}, {"": 1}, {"": 1}, {"": 11, "lk": 2, "r": 1, "convex": 1, "compact": 1, "set": 1, "u": 1, "0": 1, "1k": 1, "k": 3, "x": 2, "1": 1, "l0": 1, "2": 1, "gw": 1, "w": 1, "min": 1, "wk": 1, "uk": 1}, {"1": 1, "k": 1, "2": 1, "uu": 1, "k1": 2, "": 2, "function": 1, "g": 1, "concave": 1, "see": 1, "appendix": 1, "b1": 1, "maximize": 1}, {"2": 1, "": 1, "dynamics": 1}, {"agent": 1, "face": 1, "coomdp": 1, "instance": 1, "": 3, "s1": 2, "p": 1, "v": 1, "g": 1, "start": 1, "state": 1, "time": 1, "three": 1, "events": 1, "happen": 1}, {"first": 1, "agent": 1, "observe": 1, "current": 1, "state": 1, "st": 1, "": 1}, {"second": 1, "take": 1, "action": 1, "": 2, "ast": 1}, {"third": 1, "transit": 1, "another": 1, "state": 1, "st1": 1, "": 8, "pst": 1, "receive": 1, "vectorial": 1, "outocme": 1, "vt": 1, "st": 1, "vst": 1}, {"st1": 1, "vt": 1, "st": 1, "": 2, "observe": 1, "agent": 1}, {"whole": 1, "dynamics": 1, "result": 1, "control": 1, "markov": 1, "process": 1, "st": 2, "": 5, "vt": 1, "t1": 1}, {"condition": 1, "st": 2, "": 6, "random": 1, "variable": 1, "pair": 1, "st1": 1, "vt": 1, "independent": 1, "ht1": 1}, {"second": 1, "event": 1, "choice": 1, "base": 1, "nonanticipatory": 1, "policy": 1}, {"choice": 1, "depend": 1, "current": 1, "state": 1, "st": 1, "previous": 1, "observations": 1, "ht1": 1, "": 5, "sq": 2, "aq": 2, "vq": 1, "t1": 1, "q1": 1}, {"depend": 1, "st": 1, "": 2, "ht1": 1, "correspond": 1, "nonanticipatory": 1, "policy": 1, "say": 1, "stationary": 1}, {"objective": 1}, {"coomdp": 1, "instance": 1, "latentp": 1, "agent": 1, "know": 2, "s1": 1, "": 4, "g": 1, "v": 1, "p": 1, "state": 1, "objective": 1, "define": 1, "v1t": 1, "1t": 1, "q1": 1, "vq": 1, "sq": 1, "aq": 1}, {"horizon": 1, "know": 1, "priori": 1, "agent": 1, "aim": 1, "maximize": 1, "gv1t": 1, "": 3, "select": 1, "action": 1, "a1": 1}, {"": 1}, {"": 1}, {"": 1, "nonanticipatory": 1, "policy": 1}, {"denote": 1, "v1tk": 1, "kcomponent": 1, "time": 1, "average": 1, "vector": 1, "v1t": 1, "": 1}, {"coomdps": 1, "capture": 1, "follow": 1, "problems": 1, "pk": 1, "multiobjective": 1, "optimization": 1}, {"consider": 1, "maximize": 1, "scalar": 1, "function": 1, "k1": 1, "lk": 1, "v1tk": 2, "": 4, "try": 1, "meet": 1, "key": 1, "performance": 1, "index": 1, "kpi": 1, "requirement": 1, "k": 2, "1": 1}, {"": 1}, {"": 1}, {"": 1, "k": 1}, {"k": 5, "vector": 1, "": 4, "k1": 2, "0": 1, "1": 1, "comprise": 1, "predetermine": 1, "kpi": 1, "target": 1, "objectives": 1, "v1tk": 1}, {"task": 1, "model": 1, "coomdp": 1, "problem": 1, "set": 1, "l0": 1, "": 6, "0": 1, "u": 1, "w": 1, "wk": 1, "k": 3, "1": 1}, {"put": 1, "k": 2, "": 5, "1": 1, "lk": 1, "0": 1, "maximizer": 1, "gv1t": 1, "paretooptimal": 1, "simultaneous": 1, "maximization": 1, "v1t1": 1}, {"": 1}, {"": 1}, {"": 2, "v1tk": 1}, {"pareto": 1, "optimality": 1, "still": 1, "ub": 2, "hold": 1, "replace": 1, "inequality": 1, "wk": 2, "": 3, "1": 1, "k": 2, "bound": 1, "average": 1, "v1tk": 1, "policy": 1}, {"state": 1, "space": 1, "exploration": 1}, {"consider": 1, "visit": 1, "state": 1, "empirical": 1, "frequency": 2, "close": 1, "possible": 1, "target": 1, "time": 1, "step": 1, "": 4, "ss": 1}, {"task": 1, "phrase": 1, "coomdp": 1, "problem": 1}, {"stateaction": 1, "pair": 1, "define": 1, "v": 1, "": 2, "0": 2, "1s": 1, "standard": 1, "basis": 1, "vector": 1, "rs": 1, "value": 2, "1": 1, "scoordinate": 1, "others": 1}, {"addition": 1, "set": 1, "l0": 1, "": 3, "1": 1, "l1": 1}, {"": 1}, {"": 1}, {"": 4, "lk": 1, "0": 1, "u": 1}, {"maximize": 1, "gv1t": 1, "": 3, "equivalent": 1, "minimize": 1, "mean": 1, "p": 1, "pt": 1, "square": 1, "error": 1, "ss": 1, "t1": 1, "1st": 1, "2": 1}, {"generalize": 1, "consider": 1, "visit": 1, "certain": 1, "subsets": 1, "necessarily": 1, "disjoint": 1, "cover": 1, "target": 1, "frequencies": 1}, {"finally": 1, "specialize": 1, "coomdp": 1, "problem": 2, "l0": 1, "": 1, "0": 1, "recover": 1, "soomdp": 1, "28": 1}, {"specialize": 1, "": 1, "recover": 1, "bwr": 1, "problem": 1, "2": 1, "reward": 1, "function": 1, "g": 1, "reachability": 1, "learn": 1, "latent": 1, "model": 1, "agent": 1, "travel": 1, "among": 1, "state": 1}, {"s0": 4, "": 14, "stationary": 1, "policy": 1, "define": 1, "travel": 1, "time": 1, "random": 1, "variable": 1, "mint": 1, "st1": 1, "s1": 1, "1": 1, "ps": 1}, {"assume": 1, "follow": 1, "assumption": 1, "21": 1}, {"latent": 1, "coomdp": 1, "instance": 1, "communicate": 1, "quantity": 1, "": 3, "maxss0": 1, "minstationary": 1, "es0": 1, "finite": 1}, {"call": 1, "diameter": 1, "reachability": 1, "assumption": 1, "make": 1, "28": 1}, {"since": 1, "instance": 1, "latent": 1, "correspond": 1, "diameter": 1, "also": 1, "know": 1, "agent": 1}, {"assumption": 2, "21": 1, "weaker": 1, "unichain": 1, "6": 1, "32": 1, "41": 1, "every": 1, "stationary": 1, "policy": 1, "induce": 1, "single": 1, "recurrent": 1, "class": 1, "offline": 1, "benchmark": 1, "regret": 1}, {"measure": 1, "effectiveness": 1, "policy": 1, "rephrase": 1, "agents": 1, "objective": 1, "minimization": 1, "regret": 1, "regt": 1, "": 5, "optpm": 1, "gv1t": 1}, {"offline": 1, "benchmark": 1, "optpm": 1, "": 1, "optimum": 1, "convex": 1, "optimization": 1, "problem": 2, "pg": 1, "serve": 1, "fluid": 1, "relaxation": 1, "38": 1, "6": 1, "coomdp": 1}, {"": 5, "x": 2, "pm": 1, "max": 1, "g": 1, "vs": 1, "ax": 1, "st": 1}, {"x": 3, "": 25, "ssaas": 2, "xs": 4, "pss0": 1, "a0": 2, "xs0": 1, "2a": 1, "s0": 1, "sa0": 1, "as0": 1, "aas": 1, "1": 1, "2b": 1, "0": 1, "3": 1, "2c": 1, "pm": 1, "variables": 1, "asa": 1, "form": 1, "probability": 1, "distribution": 1, "stateaction": 1, "pair": 1}, {"set": 1, "constraints": 1, "2a": 1, "require": 1, "rat": 1, "transit": 1, "state": 1, "equal": 1}, {"aim": 1, "design": 1, "nonanticipatory": 1, "policy": 1, "anytime": 1, "regret": 1, "bind": 1, "regt": 1, "": 6, "o1t": 1, "0": 1}, {"": 9, "0": 1, "exist": 1, "constants": 1, "c": 3, "depend": 1, "k": 1, "g": 1, "policy": 1, "satisfy": 1, "regt": 1, "ct": 1, "probability": 1, "least": 1, "1": 1}, {"achieve": 2, "regt": 1, "": 8, "o1t": 1, "0": 1, "imply": 1, "nearoptimality": 1, "since": 1, "optpm": 1, "differ": 1, "expect": 1, "optimum": 1, "additive": 1, "error": 1, "oldt": 1, "similar": 1, "reason": 1, "28": 1, "see": 1, "18": 1, "detail": 1}, {"3": 1, "": 2, "challenge": 2, "coomdp": 2, "algorithm": 2, "tfwucrl2": 2, "first": 1, "discuss": 2, "unique": 1, "present": 1, "1": 1}, {"finally": 1, "present": 1, "regret": 1, "bind": 1, "tfwucrl2": 1}, {"challenge": 1}, {"begin": 1, "describe": 1, "unique": 1, "challenge": 1, "coomdp": 1, "hint": 1, "introduction": 1}, {"consider": 1, "three": 1, "instance": 1, "fig": 1, "1": 1}, {"arc": 1, "state": 1, "s0": 1, "represent": 1, "action": 1, "ps0": 1, "": 1, "1": 1, "label": 1, "outcome": 1, "v": 1, "deterministic": 1}, {"let": 1, "focus": 1, "figs": 1, "1a": 1, "1b": 1}, {"common": 1, "objective": 1, "require": 1, "balance": 1, "2dimensional": 1, "outcomes": 1, "visit": 1, "leave": 1, "loop": 2, "right": 1, "rl": 1, "frequency": 1, "05": 1}, {"fig": 1, "1a": 1, "agent": 1, "incur": 1, "o1t": 1, "": 1, "regret": 1, "choose": 1, "rl": 1}, {"": 32, "0": 15, "1": 8, "bwr": 1, "s2": 2, "b": 1, "coomdp": 1, "figure": 1, "instance": 1, "opt": 1}, {"action": 1, "bolded": 1}, {"insts": 1, "1a": 1, "1b": 1, "gw": 1, "": 10, "s0": 1, "0": 2, "s1": 1, "1": 1, "c": 1, "soomdp": 1, "p2": 1, "k1": 1, "wk": 1, "052": 1, "2": 1}, {"however": 1, "agent": 1, "visit": 1, "rl": 1, "fig": 1, "1b": 1, "suffer": 1, "regt": 1, "": 2, "1": 1}, {"indeed": 1, "spend": 1, "two": 1, "third": 1, "time": 1, "action": 1, "suboptimal": 1, "state": 1, "s0": 1, "": 3, "result": 1, "v1t": 1, "16": 2, "large": 1}, {"agent": 1, "visit": 1, "loop": 3, "multiple": 1, "time": 1, "go": 1, "state": 1, "s0": 1, "another": 1, "length": 1, "stay": 1, "priori": 1, "clear": 1}, {"gradient": 1, "threshold": 1, "procedure": 1, "gtp": 2, "provide": 1, "principled": 1, "way": 1, "determine": 1, "lengths": 1, "generalize": 1, "communicate": 1, "mdps": 1}, {"finally": 1, "subtlety": 1, "state": 1, "transition": 1, "occur": 1, "fig": 1, "1c": 1, "generally": 1, "communicate": 1, "soomdp": 1, "instance": 1, "agent": 1, "achieve": 1, "nearoptimality": 1, "remain": 1, "single": 1, "recurrent": 1, "class": 1}, {"tfwucrl2": 1, "run": 1, "episodes": 1}, {"episode": 1, "start": 1, "begin": 1, "time": 2, "": 4, "end": 2, "1": 2}, {"episode": 1, "agent": 1, "follow": 1, "certain": 1, "stationary": 1, "policy": 1, "": 1}, {"start": 1, "": 2, "time": 1, "m1": 2, "policies": 1, "decide": 1, "adaptively": 1}, {"maintain": 1, "confidence": 1, "regions": 1, "v": 3, "p": 3, "asa": 2, "": 10, "hm": 4, "latent": 1, "across": 1, "episodes": 1, "first": 1, "define": 1, "m1": 1, "nm": 3, "x": 1, "max1": 1}, {"1st": 2, "sa": 2, "": 18, "3": 1, "t1": 2, "estimate": 1, "confidence": 1, "regions": 1, "v": 3, "m1": 1, "x": 1, "1": 1, "vm": 1, "vt": 1, "st": 1, "radvmk": 2, "nm": 1, "hm": 1, "0": 1, "1k": 1, "vk": 1, "vmk": 1, "k": 2}, {"": 4, "vmk": 1, "nm": 1}, {"": 17, "4": 1, "estimate": 1, "confidence": 1, "regions": 1, "p": 3, "m1": 1, "x": 1, "1": 1, "pm": 2, "1st": 1, "st1": 1, "sas0": 1, "radpm": 2, "s0": 4, "nm": 1, "t1": 1, "hm": 1, "ps0": 1}, {"0": 1, "": 6, "4": 1, "pm": 1, "s0": 1, "nm": 1}, {"": 2, "5": 1, "provide": 1, "complete": 1, "expressions": 1, "radvmk": 1, "radpm": 1, "s0": 1, "appendix": 1, "b2": 1}, {"explain": 1, "three": 1, "vital": 1, "components": 1, "tfwucrl2": 1, "frankwolfe": 1, "fw": 1, "21": 1, "adapt": 1, "relate": 1, "research": 1, "bwr": 1, "2": 1, "14": 1, "exploration": 1, "problems": 1, "mdps": 1, "25": 1, "41": 1, "ii": 1, "extend": 1, "value": 1, "iteration": 1, "evi": 1, "28": 1, "iii": 1, "crucial": 1, "novel": 1, "gradient": 1, "threshold": 1, "procedure": 1, "gtp": 1}, {"frank": 1, "wolfe": 1, "fw": 1, "21": 1, "provide": 1, "way": 1, "balance": 1, "vectorial": 1, "outcome": 1, "time": 1, "step": 1, "denote": 1, "k": 1, "": 4, "k2": 1, "euclidean": 1, "norm": 1, "define": 1, "u": 1, "w": 1, "argminuu": 1, "ku": 1, "wk2": 1}, {"time": 1, "fw": 1, "scalarizes": 1, "outcome": 1, "eqn": 1, "6": 1, "gradient": 1, "gv1t1": 1, "": 7, "1": 1, "l1": 1}, {"": 1}, {"": 1}, {"": 6, "lk": 1, "l0": 1, "v1t1": 2, "u": 1}, {"k": 1, "": 5, "gain": 1, "intuitions": 1, "consider": 1, "state": 1, "space": 1, "exploration": 1, "target": 1, "frequency": 1, "l0": 1, "pt1": 1, "1": 1, "l1": 1}, {"": 1}, {"": 1}, {"": 4, "lk": 1, "0": 1, "u": 1}, {"scomponent": 1, "gv1t1": 1, "": 4, "q1": 1, "1sq": 1, "1k": 1, "encourage": 1, "visit": 1, "state": 1, "empirical": 1, "frequency": 1, "target": 1}, {"similarly": 1, "multiobjective": 1, "optimization": 1, "kpi": 1, "target": 1, "": 5, "kcomponent": 1, "gv1t1": 1, "lk": 1, "l0": 1, "maxk": 1, "v1t1k": 1, "0k": 1}, {"agent": 1, "motivate": 1, "focus": 1, "kth": 1, "objective": 1, "v1t1k": 1, "": 2, "k": 1}, {"extend": 1, "value": 1, "iteration": 1, "evi": 1, "28": 1, "solve": 1, "optimistic": 1, "stationary": 1, "policy": 1, "soomdp": 1, "problem": 1, "v": 1, "p": 1, "know": 1}, {"extract": 1, "evi": 1, "28": 1, "appendix": 1, "b3": 1}, {"ideally": 1, "start": 1, "episode": 1, "agent": 2, "wish": 1, "compute": 2, "optimal": 2, "policy": 2, "scalarized": 1, "reward": 1, "gv1": 1, "m1": 1, "": 4, "v": 3, "transition": 1, "kernel": 1, "p": 3, "since": 1, "uncertain": 1, "use": 1, "evi": 1, "28": 1, "stationary": 1, "7": 1, "optimistic": 1, "choices": 1, "vm": 1, "hm": 2, "pm": 1}, {"optimistic": 1, "choices": 1, "vm": 3, "": 10, "pm": 2, "mean": 1, "result": 1, "single": 1, "objective": 1, "mdp": 1, "scalar": 1, "reward": 2, "rm": 3, "asa": 1, "gv1": 1, "m1": 1, "transition": 1, "kernel": 1, "v": 1, "p": 3, "highest": 1, "long": 1, "term": 1, "average": 1, "among": 1, "hm": 2}, {"last": 1, "argument": 1, "1": 1, "": 1, "evi": 2, "additive": 1, "error": 1, "term": 1, "allow": 1}, {"28": 1, "evi": 1, "converge": 1, "stationary": 1, "p": 1, "policy": 1, "finite": 1, "time": 1, "hm": 1, "contain": 1, "transition": 1, "kernel": 1, "communicate": 1, "mdp": 1}, {"algorithm": 1, "1": 3, "tfwucrl2": 1, "g": 1, "": 8, "input": 1, "parameter": 1, "0": 2, "gradient": 1, "threshold": 1, "q": 2, "default": 1, "l": 1, "k": 1, "initial": 1, "state": 1, "s1": 1}, {"2": 2, "initialize": 1, "": 3, "1": 2, "3": 1, "episode": 1}, {"": 1}, {"": 1}, {"": 4, "accord": 1, "eq": 1, "3": 1}, {"4": 2, "set": 1, "": 2, "initialize": 1, "nm": 1, "p": 2, "v": 2, "respectively": 1, "accord": 1, "eqs": 1, "5": 1}, {"": 10, "hm": 2, "5": 1, "compute": 3, "confidence": 1, "regions": 1, "6": 1, "optimistic": 1, "reward": 1, "rm": 3, "assaas": 1, "7": 1, "1": 1, "max": 1, "v": 1, "sa": 1, "vsahm": 1, "gv1": 1, "m1": 1, "vs": 1}, {"p": 3, "": 7, "moptimal": 1, "optimistic": 1, "policy": 1, "evirm": 1, "hm": 1, "1": 1}, {"6": 1, "": 9, "7": 1, "8": 1, "initialize": 1, "0": 2, "ref": 1, "gv1": 1, "m1": 1}, {"": 10, "9": 1, "q": 1, "st": 5, "nm": 1, "10": 1, "choose": 1, "action": 1}, {"11": 1, "observe": 1, "outcomes": 1, "vt": 1, "st": 1, "": 3, "next": 1, "state": 1, "st1": 1}, {"12": 1, "compute": 1, "gradient": 1, "t1": 1, "": 2, "gvt": 1}, {"": 1}, {"frankwolfe": 1, "13": 1, "update": 1, "": 6, "kt1": 1, "ref": 1, "k2": 1}, {"14": 1, "update": 1, "st": 2, "": 6, "1": 1}, {"15": 1, "update": 1, "": 2, "1": 1}, {"16": 1, "end": 2, "17": 1, "": 1, "gradient": 1, "threshold": 1, "procedure": 1, "gtp": 1, "maintain": 1, "fws": 1, "balance": 1, "effect": 1, "vectorial": 1, "outcomes": 1, "overcome": 1, "challenge": 1, "avoid": 1, "suboptimal": 1, "action": 1}, {"gtp": 1, "maintain": 1, "distance": 1, "measure": 2, "": 2, "gradients": 1, "generate": 1, "fw": 1, "episode": 2, "start": 1, "next": 1, "exceed": 1, "threshold": 1, "q": 1}, {"small": 1, "q": 2, "make": 1, "agent": 1, "alternate": 1, "among": 1, "different": 1, "stationary": 1, "5": 1, "": 1, "policies": 1, "frequently": 1, "balance": 1, "outcomes": 1, "large": 1, "facilitate": 1, "learn": 1, "avoid": 1, "visit": 1, "suboptimal": 1, "state": 1}, {"properly": 1, "tune": 1, "q": 1, "paths": 1, "way": 1, "solve": 1, "coomdp": 1, "problem": 1}, {"direct": 1, "combination": 1, "fw": 1, "evi": 1, "correspond": 1, "tfwucrl2": 1, "q": 1, "": 3, "0": 1, "silence": 1, "gtp": 1, "incur": 1, "regt": 1, "1": 1, "instance": 1, "fig": 1, "1b": 1}, {"let": 1, "assume": 1, "start": 1, "state": 1, "s0": 1, "": 1, "complete": 1, "knowledge": 1, "v": 1, "p": 1, "consistent": 1, "tie": 1, "break": 1}, {"agent": 1, "would": 1, "go": 2, "s2": 2, "": 2, "take": 3, "s1": 1, "rl": 1, "back": 1, "dynamics": 1, "challenge": 1}, {"indeed": 1, "pure": 1, "effect": 1, "fw": 1, "agent": 1, "obsess": 1, "balance": 1, "outcomes": 1}, {"v1t11": 1, "": 5, "v1t12": 1, "scalarized": 1, "reward": 1, "s2": 2, "higher": 1, "s1": 1, "rl": 1, "travel": 1}, {"similarly": 1, "v1t11": 1, "": 3, "v1t12": 1, "travel": 1, "s1": 1}, {"process": 1, "oblivious": 1, "fact": 1, "constantly": 2, "alternate": 1, "rl": 1, "penalize": 1, "objective": 1, "visit": 1, "s0": 1, "": 1}, {"contrast": 1, "": 4, "apply": 1, "tfwucrl2": 1, "0": 1, "q": 1, "lead": 1, "us": 1, "nearoptimality": 1}, {"example": 1, "0": 1, "q": 1, "": 4, "l": 1, "k": 1, "agent": 2, "follow": 1, "interest": 1, "trajectory": 1, "suppose": 1, "time": 2, "take": 1, "qt": 1, "head": 1, "back": 1, "s0": 1}, {"v1t11": 2, "": 14, "v1t12": 2, "would": 2, "travel": 2, "s2": 1, "0": 1, "otherwise": 1, "s1": 1, "take": 1, "rl": 1, "p": 3, "qt": 2, "time": 2, "head": 1, "back": 1, "0altogether": 1, "every": 1, "05": 1, "agent": 1, "visit": 1, "tq": 1, "lead": 1, "anytime": 1, "regret": 1, "bind": 1, "regt": 1, "q": 1, "1q": 1}, {"finally": 1, "another": 1, "extreme": 1, "case": 1, "q": 1, "": 5, "fact": 1, "regt": 1, "1sa": 1, "log": 1}, {"indeed": 1, "condition": 1, "": 2, "q": 1, "always": 1, "satisfy": 1}, {"apply": 1, "28": 1, "agent": 1, "alternate": 1, "among": 1, "rl": 1, "osa": 1, "log": 1, "": 1, "time": 2, "step": 1}, {"lead": 1, "imbalance": 1, "outcomes": 1, "since": 1, "agent": 1, "could": 1, "stay": 1, "loop": 1, "sa": 1, "log": 2, "": 4, "time": 1, "result": 1, "regt": 1, "1sa": 1}, {"p": 1, "main": 1, "result": 1}, {"establish": 1, "regret": 1, "bound": 1, "tfwucrl2": 1}, {"denote": 1, "": 3, "s1": 1, "ss": 1, "sa": 1, "number": 1, "stateaction": 1, "pair": 1}, {"denote": 1, "": 3, "maxssaas": 1, "kps": 1, "ak0": 1, "maximum": 1, "number": 1, "state": 1, "stateaction": 1, "pair": 1, "transit": 1}, {"employ": 1, "notation": 1, "hide": 1, "additive": 1, "term": 1, "scale": 1, "logt": 2, "well": 1, "multiplicative": 1, "": 1, "factor": 1}, {"theorem": 1, "31": 1}, {"consider": 1, "tfwucrl2": 1, "gradient": 1, "threshold": 1, "q": 2, "": 14, "0": 1, "apply": 1, "communicate": 1, "coomdp": 1, "instance": 1, "diameter": 1, "probability": 1, "1": 2, "anytime": 1, "regret": 1, "bind": 1, "hp": 1, "p": 2, "regt": 1, "l0": 2, "ld": 2, "kq": 1, "k": 1, "14": 1, "sa": 1}, {"": 8, "particular": 1, "set": 1, "q": 1, "l": 1, "k": 1, "give": 1, "regt": 1, "old": 1, "1": 1, "sa": 1}, {"": 1, "let": 1, "focus": 1, "first": 1, "term": 1, "bind": 1}, {"q": 1, "term": 1, "represent": 1, "regret": 1, "due": 1, "": 1, "delay": 1, "gradient": 1, "update": 1, "gtp": 1}, {"o1": 1, "q": 1, "term": 1, "represent": 1, "regret": 1, "due": 1, "interference": 1, "gtp": 1, "learn": 1, "v": 1, "p": 1, "b": 1, "switch": 1, "among": 1, "stationary": 1, "policies": 1, "could": 1, "require": 1, "visit": 1, "suboptimal": 1, "state": 1}, {"second": 1, "term": 1, "regret": 1, "due": 1, "simultaneous": 1, "explorationexploitation": 1, "evi": 1}, {"": 5, "specialize": 1, "l0": 1, "0": 1, "l1": 1}, {"": 1}, {"": 1}, {"": 7, "lk": 1, "1": 1, "tfwucrl2": 1, "incur": 1, "regt": 1, "od": 1, "sa": 1, "soomdp": 1, "whichessentially": 1, "match": 1, "281": 1}, {"specialize": 1, "": 4, "tfwucrl2": 2, "incur": 1, "regt": 1, "ol": 1, "match": 2, "2": 3, "bwr": 1, "g": 1, "regret": 1, "bound": 1, "28": 2, "special": 1, "case": 1, "design": 1, "analysis": 1, "require": 1, "novel": 1, "ideas": 1, "depart": 1}, {"design": 1, "novel": 1, "gtp": 1, "handle": 1, "state": 1, "transition": 1}, {"upcoming": 1, "analysis": 1, "show": 1, "gtp": 1, "streamline": 1, "achieve": 1, "regret": 1, "bound": 1, "without": 1, "excessively": 1, "interfere": 1, "balance": 1, "fw": 1, "learn": 1, "evi": 1}, {"finally": 1, "note": 1, "tfwucrl2": 1, "nonstationary": 1, "policy": 1, "diversify": 1, "across": 2, "different": 1, "stationary": 1, "policies": 1, "time": 1}, {"interestingly": 1, "nonstationary": 1, "policy": 1, "necessary": 1, "achieve": 1, "nearoptimality": 1, "even": 1, "model": 1, "parameters": 1, "unchanging": 1, "claim": 1, "32": 1}, {"every": 1, "stationary": 1, "policy": 1, "incur": 1, "1": 1, "anytime": 1, "regret": 1, "instance": 1, "fig": 1, "1b": 1}, {"claim": 1, "prove": 1, "appendix": 2, "b4": 2, "illustrate": 1, "profound": 1, "difference": 1, "communicate": 1, "coomdps": 2, "unichain": 1, "see": 1}, {"max": 2, "egv1t": 1, "": 2, "vs": 1, "gev1t": 1}, {"objective": 1, "maximize": 2, "gv1t": 1, "": 3, "also": 1, "lead": 1, "max": 1, "egv1t": 1, "crucially": 1, "different": 1, "gev1t": 1}, {"policy": 1, "hold": 1, "": 3, "jaksch": 1, "et": 1, "al": 1}, {"28": 1, "achieve": 1, "regret": 1, "bind": 1, "ods": 1, "": 1}, {"factor": 1, "improve": 1, "apply": 1, "empirical": 1, "bernstein": 1, "inequality7": 1, "instead": 1, "hoeffding": 1, "inequality": 1, "use": 1, "23": 1}, {"1": 1, "": 9, "6": 1, "egv1t": 1, "gev1t": 1, "optpm": 1, "oldt": 1}, {"second": 1, "inequality": 1, "formally": 1, "prove": 1, "18": 1, "demonstrate": 1, "show": 1, "policy": 1, "empirical": 1, "frequency": 1, "visit": 1, "stateaction": 1, "pair": 1, "nearly": 1, "feasible": 1, "solution": 1, "pm": 1, "": 2, "oldt": 1, "term": 1, "capture": 1, "error": 1, "due": 1, "near": 1, "feasibility": 1}, {"tfwucrl2": 1, "know": 1, "egv1t": 1, "": 2, "tend": 1, "optpm": 1, "grow": 1}, {"hence": 1, "also": 1, "gev1t": 1, "": 2, "tend": 1, "optpm": 1, "grow": 1}, {"nevertheless": 1, "converse": 1, "true": 1}, {"consider": 1, "instance": 1, "fig": 1, "1b": 1, "start": 1, "state": 1, "s0": 1, "": 1}, {"consider": 1, "follow": 1, "policy": 1, "start": 1, "agent": 1, "transit": 1, "either": 1, "s1": 1, "s2": 1, "probability": 1, "12": 1}, {"agent": 1, "loop": 1, "state": 1, "indefinitely": 1}, {"clear": 1, "prv1t": 2, "": 9, "1": 2, "1t": 2, "0": 2, "12": 1}, {"one": 1, "hand": 1, "gev1t": 1, "": 7, "18t": 1, "2": 1, "tend": 1, "optpm": 1, "0": 1}, {"hand": 1, "egv1t": 1, "": 8, "18": 1, "o1t": 1, "tend": 1, "optpm": 1, "0": 1}, {"altogether": 1, "solve": 1, "max": 2, "gev1t": 1, "": 2, "nearoptimality": 2, "lead": 1, "egv1t": 1}, {"end": 1, "worth": 1, "mention": 1, "relate": 1, "pworks": 1, "discount": 2, "settings": 1, "24": 1, "6": 1, "12": 1, "43": 1, "1": 2, "25": 1, "42": 1, "29": 1, "30": 1, "33": 1, "focus": 1, "maximize": 1, "ge": 1, "t1": 1, "": 5, "vt": 1, "st": 1, "g": 1, "certain": 1, "nonlinear": 1, "function": 1, "p": 1, "0": 1, "factor": 1}, {"envision": 1, "p": 1, "technique": 1, "could": 1, "useful": 1, "": 8, "maximize": 2, "eg": 1, "t1": 2, "vt": 2, "st": 2, "instead": 1, "ge": 1}, {"generalizations": 1}, {"theorem": 1, "31": 1, "concern": 1, "specialize": 1, "aggregate": 1, "function": 2, "1": 1, "cheung": 1, "18": 1, "recently": 1, "generalize": 1, "algorithmic": 1, "framework": 1, "lipschitz": 1, "continuous": 1, "smooth": 1}, {"adapt": 1, "online": 1, "mirror": 1, "descent": 1, "algorithm": 2, "34": 1, "cheung": 1, "18": 1, "propose": 1, "another": 1, "result": 1, "o1t": 1, "3": 1, "": 1, "regret": 1, "hide": 1, "dependence": 1, "lipschitz": 1, "continuous": 1, "concave": 1, "aggregate": 1, "function": 1, "necessarily": 1, "smooth": 1}, {"4": 1, "": 2, "analysis": 1, "tfwucrl2": 1, "section": 1, "prove": 1, "theorem": 1, "31": 1}, {"start": 1, "consider": 1, "events": 1, "e": 2, "v": 1, "": 1, "p": 1, "lemma": 1, "41": 1, "prove": 1, "appendix": 1, "c1": 1}, {"shorthand": 1, "mean": 1, "": 4, "n": 1}, {"v": 2, "e": 2, "": 7, "vs": 1, "hm": 2, "p": 2, "ps": 1}, {"lemma": 1, "41": 1}, {"hold": 1, "pe": 2, "v": 1, "": 6, "1": 2, "2": 2, "p": 1}, {"decompose": 1, "regt": 1, "p": 1, "": 1, "analytical": 1, "tool": 1, "fw": 1, "21": 1, "16": 1, "also": 1, "adapt": 1, "2": 1, "14": 1, "25": 1, "41": 1}, {"define": 1, "v": 1, "": 3, "sa": 1, "vs": 1, "ax": 1, "x": 1, "optimal": 1, "solution": 1, "pm": 1}, {"gv1t": 1, "": 52, "gv1t1": 10, "v1t": 1, "v1t1": 5, "l0": 4, "kv1t": 1, "k22": 2, "k": 1, "1": 5, "kvt": 1, "st": 4, "vt": 3, "kt2": 1, "v": 3, "2": 2, "optpm": 1}, {"": 3, "8": 2, "9": 1, "step": 1, "property": 1, "g": 1, "2l0": 1, "ksmooth": 1, "wrt": 1}, {"k": 1, "": 1, "k2": 1, "domain": 1, "0": 1, "1k": 1, "see": 1, "appendix": 1, "b1": 1}, {"rearrange": 1, "9": 1, "give": 1, "": 13, "regt": 2, "1": 2, "l0": 1, "gv1t1": 1, "v": 1, "vt": 1, "st": 1}, {"": 4, "10": 2, "apply": 1, "recursively": 1}, {"": 1}, {"": 1}, {"": 13, "1": 1, "obtain": 1, "recall": 1, "gv1t1": 1, "regt": 1, "2l0": 1, "log": 1, "1x": 1, "v": 1, "vt": 1, "st": 1}, {"t1": 1, "": 2, "11": 2, "main": 1, "analysis": 2, "second": 1, "term": 1, "require": 1, "novel": 1, "technical": 1, "regard": 1, "dynamics": 1, "gradient": 1, "threshold": 1, "procedure": 1}, {"start": 1, "follow": 1, "bind": 1}, {"7": 1, "": 1, "proposition": 1, "42": 1}, {"consider": 1, "execution": 1, "tfwucrl2": 1, "communicate": 1, "instance": 1, "diameter": 1, "": 2, "n": 1, "suppose": 1, "deterministic": 1, "constant": 1, "st": 1}, {"prmt": 1, "": 4, "1": 1}, {"condition": 1, "events": 1, "e": 2, "v": 2, "": 19, "p": 1, "probability": 1, "least": 1, "1": 2, "x": 1, "vt": 1, "st": 1, "q": 1, "k": 1, "ldm": 1, "ld": 1, "sit": 1}, {"t1": 1, "": 2, "proposition": 1, "42": 1, "bound": 1, "two": 1, "source": 1, "error": 2, "due": 1, "gtp": 1, "ii": 1, "estimation": 1, "errors": 1, "v": 1, "p": 1, "associate": 1, "hm": 2, "evi": 1}, {"error": 1, "ii": 1, "upper": 1, "bound": 1, "machinery": 1, "28": 1}, {"error": 1, "concern": 1, "follow": 1, "discrepancy": 1}, {"time": 1, "episode": 1, "action": 1, "": 5, "st": 1, "choose": 1, "base": 1, "policy": 1, "involve": 1, "scalarization": 1}, {"however": 1, "ideally": 1, "action": 1, "time": 1, "balance": 1, "current": 1, "vectorial": 1, "outcomes": 1, "scalarization": 1, "": 1}, {"proposition": 1, "bound": 1, "error": 1, "charge": 1, "discrepancy": 1, "threshold": 1, "q": 1, "upper": 1, "bind": 1, "": 1}, {"complete": 1, "proof": 1, "bound": 1, "regt": 1, "": 2, "establish": 1, "bind": 1, "small": 1, "enough": 1, "achieve": 1, "theorem": 1, "31": 1}, {"lemma": 1, "43": 1}, {"consider": 1, "execution": 1, "tfwucrl2": 1, "q": 2, "withgradient": 1, "threshold": 1, "": 1, "0": 1}, {"certainty": 1, "every": 1, "": 6, "n": 1, "mt": 1, "l0": 1, "kq": 1}, {"lemma": 1, "bound": 1, "error": 1, "gtp": 1, "balance": 1, "outcomes": 1}, {"fw": 1, "gradients": 1, "change": 1, "rate": 1, "o1t": 1, "slow": 1, "enough": 1, "agent": 1, "judiciously": 1, "delay": 1, "gradient": 1, "update": 1, "without": 1, "sacrifice": 1, "balance": 1, "effect": 1, "much": 1}, {"open": 1, "door": 1, "avoid": 1, "visit": 1, "suboptimal": 1, "state": 1, "frequently": 1}, {"sketch": 1, "proof": 1, "lemma": 1, "43": 1}, {"first": 1, "observe": 1, "1": 1, "": 1}, {"": 1}, {"": 1}, {"": 49, "mt": 1, "union": 1, "n": 2, "episode": 2, "1": 3, "start": 2, "due": 2, "q": 2, "st": 4, "nm": 1, "prove": 1, "lemma": 1, "suffice": 1, "show": 1, "kq2l0": 1, "4": 1, "2l0": 1, "kq": 1, "12": 1, "sa1": 1, "log2": 1}, {"13": 2, "bind": 1, "follow": 1, "28": 1}, {"thus": 1, "focus": 1, "show": 1, "bind": 1, "12": 1}, {"let": 1, "express": 1, "": 5, "m1": 1, "m2": 1}, {"": 1}, {"": 1}, {"": 5, "mn": 1, "m1": 1, "m2": 1}, {"": 1}, {"": 1}, {"": 2, "mn": 1}, {"also": 1, "define": 1, "m0": 1, "": 1, "0": 1}, {"focus": 1, "episode": 1, "index": 1, "mj": 2, "j": 1, "": 6, "1": 2, "consider": 1}, {"": 1}, {"": 1}, {"": 7, "mj": 2, "1": 1, "difference": 1, "kt": 1, "k2": 1}, {"follow": 1, "argue": 1, "gradients": 1, "fw": 1, "change": 1, "slowly": 1, "kt": 1, "": 63, "mj": 11, "k2": 2, "kgv1t1": 1, "gv1": 1, "1": 10, "l0": 2, "k": 2, "t1": 3, "x": 4, "vq": 4, "sq": 4, "aq": 4, "q1": 3, "q": 1, "2": 2, "2l0": 2}, {"t1": 1, "": 40, "mj": 13, "k": 2, "p": 1, "1": 5, "kt": 1, "k2": 1, "q": 1, "mean": 1, "since": 2, "know": 1, "j": 1, "mj1": 2, "12": 1, "2": 1, "x": 1, "kq": 1, "14": 2, "2l0": 1, "inequality": 1, "say": 1, "gradients": 1, "change": 1, "slowly": 1, "time": 1, "index": 1, "1nj1": 1, "far": 1, "apart": 1}, {"thus": 1, "n": 1, "bound": 1}, {"indeed": 1, "technical": 1, "arguments": 1, "see": 1, "": 5, "appendix": 1, "c2": 1, "inequality": 1, "14": 1, "turn": 1, "imply": 1, "mdq0": 1, "ej": 1, "1": 1, "q0": 2, "j": 1, "12": 1, "16": 1, "kq2l0": 1}, {"j": 1, "": 8, "n": 2, "1": 3, "get": 1, "q0": 1, "22": 1, "16": 1, "mdq0": 1, "en": 1, "lead": 1, "12": 1}, {"combine": 1, "bind": 1, "11": 1, "proposition": 1, "42": 1, "lemma": 1, "43": 1, "prove": 1, "theorem": 1, "31": 1}, {"8": 1, "": 3, "5": 1, "numerical": 1, "experiment": 1, "empirically": 1, "evaluate": 1, "tfwucrl2": 1, "state": 1, "space": 1, "exploration": 1, "3": 1, "instance": 1, "small": 1, "medium": 1, "large": 1}, {"instance": 1, "detail": 1, "appendix": 1, "a1": 1}, {"fig": 1, "2a": 1, "tfwucrl2": 1, "simulate": 1, "instance": 1, "": 1, "q": 1, "25": 1, "time": 1}, {"figs": 1, "2b": 1, "2c": 1, "tfwucrl2": 1, "simulate": 1, "instance": 1, "q": 1, "": 1, "l": 1, "k": 1, "25": 1, "time": 1}, {"curve": 1, "plot": 1, "average": 1, "across": 1, "25": 1, "trials": 1, "error": 2, "bar": 1, "quantify": 1, "": 1, "standard": 1, "deviation": 1, "region": 1}, {"fig": 1, "2a": 1, "depict": 1, "reg105": 1, "": 1, "different": 1, "qs": 1}, {"extremely": 1, "small": 1, "large": 2, "": 1, "q": 2, "lead": 1, "regret": 1, "tfwucrl2": 1, "seem": 1, "robust": 1, "middle": 1, "range": 1}, {"default": 1, "q": 1, "": 1, "l": 1, "k": 1, "green": 1, "dot": 1, "motivate": 1, "analysis": 1, "optimize": 1, "empirical": 1, "performance": 1}, {"tune": 1, "q": 1, "online": 1, "interest": 1, "research": 1, "direction": 1}, {"small": 3, "medium": 3, "large": 3, "": 40, "regt": 3, "reg105": 2, "10": 10, "104": 2, "2": 7, "1": 5, "0": 1, "q": 2, "different": 1, "obj": 6, "algo": 2, "pm": 2, "target": 2, "02": 1, "00": 1, "3": 2, "tfwucrl2": 3, "4": 2, "random": 3, "005": 1, "b": 1, "grow": 2, "000": 1, "103": 1, "c": 1, "simultaneous": 1, "convergence": 1, "figure": 1, "simulation": 1, "result": 1, "state": 1, "space": 1, "exploration": 1, "fig": 1, "2b": 1, "demonstrate": 1, "trend": 1, "loglog": 1, "scale": 1}, {"performance": 1, "tfwucrl2": 1, "contrast": 1, "random": 2, "policy": 1, "yellow": 1, "sample": 1, "action": 1, "uniformly": 1, "every": 1, "state": 1}, {"regt": 2, "": 2, "tfwucrl2": 1, "converge": 1, "0": 1, "grow": 1, "random": 1, "policy": 1, "constant": 1}, {"slight": 1, "wiggle": 1, "plot": 1, "tfwucrl2": 1, "due": 1, "gtp": 1, "could": 1, "deteriorate": 1, "objective": 1, "short": 1, "term": 1, "still": 1, "lead": 1, "nearoptimality": 1, "eventually": 1}, {"fig": 1}, {"2c": 1, "highlight": 1, "simultaneous": 1, "convergence": 1, "objective": 1, "target": 1, "large": 1, "instance": 1}, {"instance": 1, "involve": 1, "star": 1, "graph": 1, "center": 1, "state": 2, "12": 1, "branch": 1}, {"objectives": 1, "visit": 2, "center": 1, "state": 2, "frequency": 2, "0": 1, "obj": 1, "1": 1, "branch": 1, "112": 1, "": 2, "083": 1, "objs": 1, "2": 1}, {"": 1}, {"": 1}, {"13": 1}, {"target": 1, "frequencies": 2, "p": 1, "dash": 1, "black": 1, "realizable": 1, "plot": 1, "dot": 1, "cyan": 1, "indicate": 1, "v": 1, "": 3, "sa": 1, "vs": 1, "ax": 1, "x": 1, "optimal": 1, "solution": 1, "pm": 1}, {"along": 1, "complete": 1, "plot": 1, "fig": 1, "4": 1, "appendix": 1, "a2": 1, "see": 1, "output": 1, "v1tk": 1, "13": 2, "k1": 1, "tfwucrl2": 1, "solid": 1, "blue": 1, "simultaneously": 1, "converge": 1, "target": 1, "frequencies": 1}, {"reference": 1, "1": 1, "j": 1, "achiam": 1, "hold": 1, "tamar": 1, "p": 1, "abbeel": 1}, {"constrain": 1, "policy": 1, "optimization": 1}, {"proceed": 1, "34th": 1, "international": 1, "conference": 1, "machine": 1, "learn": 1, "": 1, "volume": 1, "70": 1, "icml17": 1, "page": 1, "2231": 1}, {"jmlrorg": 1, "2017": 1}, {"2": 1, "agrawal": 1, "n": 1, "r": 1, "devanur": 1}, {"bandits": 1, "concave": 1, "reward": 1, "convex": 1, "knapsacks": 1}, {"acm": 1, "conference": 1, "economics": 1, "computation": 1, "2014": 1}, {"3": 1, "agrawal": 1, "n": 1, "r": 1, "devanur": 1}, {"linear": 1, "contextual": 1, "bandits": 1, "knapsacks": 1}, {"advance": 1, "neural": 2, "information": 2, "process": 2, "systems": 2, "29": 1, "annual": 1, "conference": 1, "2016": 3, "december": 1, "510": 1, "barcelona": 1, "spain": 1, "page": 1, "34503458": 1}, {"4": 1, "agrawal": 1, "n": 1, "r": 1, "devanur": 1, "l": 1, "li": 1}, {"efficient": 1, "algorithm": 1, "contextual": 1, "bandits": 1, "knapsacks": 1, "extension": 1, "concave": 1, "objectives": 1}, {"proceed": 1, "29th": 1, "conference": 1, "learn": 1, "theory": 1, "colt": 1, "2016": 3, "new": 1, "york": 1, "usa": 1, "june": 1, "2326": 1, "page": 1, "418": 1}, {"5": 1, "agrawal": 1, "r": 1, "jia": 1}, {"optimistic": 1, "posterior": 1, "sample": 1, "reinforcement": 1, "learn": 1, "worstcase": 1, "regret": 1, "bound": 1}, {"guyon": 1, "u": 1, "v": 1, "luxburg": 1, "bengio": 1, "h": 1, "wallach": 1, "r": 2, "fergus": 1, "vishwanathan": 1, "garnett": 1, "editors": 1, "advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "30": 1, "page": 1, "1184": 1, "1194": 1}, {"curran": 1, "associate": 1, "inc": 1, "2017": 1}, {"6": 1, "e": 1, "altman": 1}, {"constrain": 1, "markov": 1, "decision": 1, "process": 1}, {"chapman": 1, "hall": 1, "1999": 1}, {"9": 1, "": 1, "7": 1, "j": 1, "audibert": 1, "r": 1, "munos": 1, "c": 1, "szepesvri": 1}, {"explorationexploitation": 1, "tradeoff": 1, "use": 1, "variance": 1, "estimate": 1, "multiarmed": 1, "bandits": 1}, {"theor": 1}, {"comput": 1}, {"sci": 1, "4101918761902": 1, "2009": 1}, {"8": 1, "p": 1, "auer": 1, "r": 1, "ortner": 1}, {"logarithmic": 1, "online": 1, "regret": 1, "bound": 1, "undiscounted": 1, "reinforcement": 1, "learn": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "page": 1, "4956": 1}, {"mit": 1, "press": 1, "2006": 1}, {"9": 1, "azar": 1, "u": 1, "felge": 1, "feldman": 1, "tennenholtz": 1}, {"sequential": 1, "decision": 1, "make": 1, "vector": 1, "outcomes": 1}, {"proceed": 1, "5th": 1, "conference": 1, "innovations": 1, "theoretical": 1, "computer": 1, "science": 1, "itcs": 1, "14": 1, "page": 1, "195206": 1, "new": 1, "york": 1, "ny": 1, "usa": 1, "2014": 1}, {"acm": 1}, {"10": 1, "badanidiyuru": 1, "r": 1, "kleinberg": 1, "slivkins": 1}, {"bandits": 1, "knapsacks": 1}, {"proceed": 1, "2013": 1, "ieee": 1, "54th": 1, "annual": 1, "symposium": 1, "foundations": 1, "computer": 1, "science": 1, "focs": 1, "13": 1, "page": 1, "207216": 1}, {"ieee": 1, "computer": 1, "society": 1, "2013": 1}, {"11": 1, "badanidiyuru": 1, "j": 1, "langford": 1, "slivkins": 1}, {"resourceful": 1, "contextual": 1, "bandits": 1}, {"proceed": 1, "27th": 1, "conference": 1, "learn": 1, "theory": 1, "colt": 1, "2014": 3, "barcelona": 1, "spain": 1, "june": 1, "1315": 1, "page": 1, "11091134": 1}, {"12": 1, "l": 1, "barrett": 1, "narayanan": 1}, {"learn": 1, "optimal": 1, "policies": 1, "multiple": 1, "criteria": 1}, {"proceed": 1, "25th": 1, "international": 1, "conference": 1, "machine": 1, "learn": 1, "icml": 1, "08": 1, "page": 1, "4147": 1, "new": 1, "york": 1, "ny": 1, "usa": 1, "2008": 1}, {"acm": 1}, {"13": 1, "p": 1, "l": 1, "bartlett": 1, "tewari": 1}, {"regal": 1, "regularization": 1, "base": 1, "algorithm": 1, "reinforcement": 1, "learn": 1, "weakly": 1, "communicate": 1, "mdps": 1}, {"uai": 1, "2009": 3, "proceed": 1, "twentyfifth": 1, "conference": 1, "uncertainty": 1, "artificial": 1, "intelligence": 1, "montreal": 1, "qc": 1, "canada": 1, "june": 1, "1821": 1, "page": 1, "3542": 1}, {"14": 1, "q": 1, "berthet": 1, "v": 1, "perchet": 1}, {"fast": 1, "rat": 1, "bandit": 1, "optimization": 1, "upperconfidence": 1, "frankwolfe": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "30": 1, "page": 1, "22252234": 1}, {"2017": 1}, {"15": 1, "p": 1, "bertsekas": 1}, {"dynamic": 1, "program": 1, "optimal": 1, "control": 1, "3rd": 1, "edition": 1}, {"athena": 1, "scientific": 1, "2005": 1}, {"16": 1, "bubeck": 1}, {"convex": 1, "optimization": 1, "algorithms": 1, "complexity": 1}, {"find": 1}, {"trend": 1, "mach": 1}, {"learn": 1, "834231357": 1, "nov": 1, "2015": 1}, {"17": 1, "r": 1, "busafekete": 1, "b": 1, "szrnyi": 1, "p": 1, "weng": 1, "mannor": 1}, {"multiobjective": 1, "bandits": 1, "optimize": 1, "generalize": 1, "gini": 1, "index": 1}, {"precup": 1, "w": 1, "teh": 1, "editors": 1, "proceed": 2, "34th": 1, "international": 2, "conference": 1, "machine": 2, "learn": 2, "volume": 1, "70": 1, "research": 1, "page": 1, "625634": 1, "convention": 1, "centre": 1, "sydney": 1, "australia": 1, "0611": 1, "aug": 1, "2017": 1}, {"pmlr": 1}, {"18": 1, "w": 1, "c": 1, "cheung": 1}, {"explorationexploitation": 1, "tradeoff": 1, "reinforcement": 1, "learn": 1, "online": 1, "markov": 1, "decision": 1, "process": 1, "global": 1, "concave": 1, "reward": 1}, {"corr": 1, "abs190506466": 1, "2019": 1}, {"19": 1, "e": 1, "evendar": 1, "r": 1, "kleinberg": 1, "mannor": 1, "mansour": 1}, {"online": 1, "learn": 1, "global": 1, "cost": 1, "function": 1}, {"22nd": 1, "annual": 1, "conference": 1, "learn": 1, "theory": 1, "colt": 1, "2009": 1}, {"20": 1, "k": 1, "j": 1, "ferreira": 1, "simchilevi": 1, "h": 1, "wang": 1}, {"online": 1, "network": 1, "revenue": 1, "management": 1, "use": 1, "thompson": 1, "sample": 1}, {"operations": 1, "research": 1, "66615861602": 1, "2018": 1}, {"21": 1, "frank": 1, "p": 1, "wolfe": 1}, {"algorithm": 1, "quadratic": 1, "program": 1}, {"naval": 1, "research": 1, "logistics": 1, "quarterly": 1, "31295110": 1, "march": 1, "": 1, "june": 1, "1956": 1}, {"22": 1, "r": 1, "fruit": 1, "pirotta": 1, "lazaric": 1}, {"near": 1, "optimal": 1, "explorationexploitation": 1, "noncommunicating": 1, "markov": 1, "decision": 1, "process": 1}, {"bengio": 1, "h": 2, "wallach": 1, "larochelle": 1, "k": 1, "grauman": 1, "n": 1, "cesabianchi": 1, "r": 1, "garnett": 1, "editors": 1, "advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "31": 1, "page": 1, "29983008": 1}, {"curran": 1, "associate": 1, "inc": 1, "2018": 1}, {"23": 1, "r": 2, "fruit": 1, "pirotta": 1, "lazaric": 1, "ortner": 1}, {"efficient": 1, "biasspanconstrained": 1, "explorationexploitation": 1, "reinforcement": 1, "learn": 1}, {"j": 1, "dy": 1, "krause": 1, "editors": 1, "proceed": 2, "35th": 1, "international": 1, "conference": 1, "machine": 2, "learn": 2, "volume": 1, "80": 1, "research": 1, "page": 1, "15781586": 1, "stockholmsmssan": 1, "stockholm": 1, "sweden": 1, "1015": 1, "jul": 1, "2018": 1}, {"pmlr": 1}, {"10": 1, "": 1, "24": 1, "z": 2, "gbor": 1, "kalmr": 1, "c": 1, "szepesvri": 1}, {"multicriteria": 1, "reinforcement": 1, "learn": 1}, {"proceed": 1, "fifteenth": 1, "international": 1, "conference": 1, "machine": 1, "learn": 1, "icml": 1, "98": 1, "page": 1, "197205": 1, "san": 1, "francisco": 1, "ca": 1, "usa": 1, "1998": 1}, {"morgan": 1, "kaufmann": 1, "publishers": 1, "inc": 1, "25": 1, "e": 1, "hazan": 1, "kakade": 1, "k": 1, "singh": 1, "v": 1, "soest": 1}, {"provably": 1, "efficient": 1, "maximum": 1, "entropy": 1, "exploration": 1}, {"corr": 1, "2018": 1}, {"26": 1, "w": 1, "hoeffding": 1}, {"probability": 1, "inequalities": 1, "sum": 1, "bound": 1, "random": 1, "variables": 1}, {"journal": 1, "american": 1, "statistical": 1, "association": 1, "583011330": 1, "1963": 1}, {"27": 1, "n": 1, "immorlica": 1, "k": 1, "sankararaman": 1, "r": 1, "e": 1, "schapire": 1, "slivkins": 1}, {"adversarial": 1, "bandits": 1, "knapsacks": 1}, {"corr": 1, "2018": 1}, {"28": 1, "jaksch": 1, "r": 1, "ortner": 1, "p": 1, "auer": 1}, {"nearoptimal": 1, "regret": 1, "bound": 1, "reinforcement": 1, "learn": 1}, {"j": 1, "mach": 1}, {"learn": 1}, {"res": 1, "1115631600": 1, "aug": 1, "2010": 1}, {"29": 1, "h": 1, "le": 1, "c": 1, "voloshin": 1, "yue": 1}, {"batch": 1, "policy": 1, "learn": 1, "constraints": 1}, {"k": 1, "chaudhuri": 1, "r": 1, "salakhutdinov": 1, "editors": 1, "proceed": 2, "36th": 1, "international": 1, "conference": 1, "machine": 2, "learn": 2, "volume": 1, "97": 1, "research": 1, "page": 1, "37033712": 1, "long": 1, "beach": 1, "california": 1, "usa": 1, "0915": 1, "jun": 1, "2019": 1}, {"pmlr": 1}, {"30": 1, "l": 1, "lee": 1, "b": 1, "eysenbach": 1, "e": 2, "parisotto": 1, "p": 1, "xing": 1, "levine": 1, "r": 1, "salakhutdinov": 1}, {"efficient": 1, "exploration": 1, "via": 1, "state": 1, "marginal": 1, "match": 1}, {"corr": 1, "abs190605274": 1, "2019": 1}, {"31": 1, "c": 1, "liu": 1, "x": 1, "xu": 1, "hu": 1}, {"multiobjective": 1, "reinforcement": 1, "learn": 1, "comprehensive": 1, "overview": 1}, {"ieee": 1, "transactions": 1, "systems": 2, "man": 1, "cybernetics": 1, "453385398": 1, "march": 1, "2015": 1}, {"32": 1, "mannor": 1, "n": 1, "shimkin": 1}, {"geometric": 1, "approach": 1, "multicriterion": 1, "reinforcement": 1, "learn": 1}, {"j": 1, "mach": 1}, {"learn": 1}, {"res": 1, "5325360": 1, "dec": 1, "2004": 1}, {"33": 1, "miryoosefi": 1, "k": 1, "brantley": 1, "h": 1, "iii": 1, "dudk": 1, "r": 1, "e": 1, "schapire": 1}, {"reinforcement": 1, "learn": 1, "convex": 1, "constraints": 1}, {"appear": 1, "neurips": 1, "2019": 2}, {"34": 1, "nemirovski": 1, "b": 1, "yudin": 1}, {"problem": 1, "complexity": 1, "method": 1, "efficiency": 1, "optimization": 1}, {"wiley": 1, "interscience": 1, "series": 1, "discrete": 1, "mathematics": 1, "1983": 1}, {"35": 1, "j": 1, "r": 1, "norris": 1}, {"markov": 1, "chain": 1}, {"cambridge": 1, "series": 1, "statistical": 1, "probabilistic": 1, "mathematics": 1}, {"cambridge": 1, "university": 1, "press": 1, "1998": 1}, {"36": 1, "r": 1, "ortner": 1}, {"regret": 1, "bound": 1, "reinforcement": 1, "learn": 1, "via": 1, "markov": 1, "chain": 1, "concentration": 1}, {"corr": 1, "2018": 1}, {"37": 1, "ouyang": 1, "gagrani": 1, "nayyar": 1, "r": 1, "jain": 1}, {"learn": 1, "unknown": 1, "markov": 1, "decision": 1, "process": 1, "thompson": 1, "sample": 1, "approach": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "30": 1, "page": 1, "13331342": 1}, {"2017": 1}, {"38": 1, "l": 1, "puterman": 1}, {"markov": 1, "decision": 1, "process": 1, "discrete": 1, "stochastic": 1, "dynamic": 1, "program": 1}, {"john": 1, "wiley": 1, "": 1, "sons": 1, "inc": 1, "new": 1, "york": 1, "ny": 1, "usa": 1, "1st": 1, "edition": 1, "1994": 1}, {"39": 1, "roijers": 1, "p": 1, "vamplew": 1, "whiteson": 1, "r": 1, "dazeley": 1}, {"survey": 1, "multiobjective": 1, "sequential": 1, "decisionmaking": 1}, {"j": 1, "artif": 1}, {"int": 1}, {"res": 1, "48167113": 1, "oct": 1, "2013": 1}, {"40": 1, "rosenberg": 1, "mansour": 1}, {"online": 1, "convex": 1, "optimization": 1, "adversarial": 1, "markov": 1, "decision": 1, "process": 1}, {"k": 1, "chaudhuri": 1, "r": 1, "salakhutdinov": 1, "editors": 1, "proceed": 2, "36th": 1, "international": 1, "conference": 1, "machine": 2, "learn": 2, "volume": 1, "97": 1, "research": 1, "page": 1, "54785486": 1, "long": 1, "beach": 1, "california": 1, "usa": 1, "0915": 1, "jun": 1, "2019": 1}, {"pmlr": 1}, {"41": 1, "j": 1, "tarbouriech": 1, "lazaric": 1}, {"active": 1, "exploration": 1, "markov": 1, "decision": 1, "process": 1}, {"k": 1, "chaudhuri": 1, "sugiyama": 1, "editors": 1, "proceed": 2, "machine": 2, "learn": 2, "research": 2, "volume": 1, "89": 1, "page": 1, "974982": 1}, {"pmlr": 1, "1618": 1, "apr": 1, "2019": 1}, {"42": 1, "c": 1, "tessler": 1, "j": 1, "mankowitz": 1, "mannor": 1}, {"reward": 1, "constrain": 1, "policy": 1, "optimization": 1}, {"international": 1, "conference": 1, "learn": 1, "representations": 1, "2019": 1}, {"43": 1, "k": 1, "van": 1, "moffaert": 1}, {"multiobjective": 1, "reinforcement": 1, "learn": 1, "use": 1, "set": 1, "pareto": 1, "dominate": 1, "policies": 1}, {"j": 1, "mach": 1}, {"learn": 1}, {"res": 1, "15134833512": 1, "jan": 1, "2014": 1}, {"11": 1}]
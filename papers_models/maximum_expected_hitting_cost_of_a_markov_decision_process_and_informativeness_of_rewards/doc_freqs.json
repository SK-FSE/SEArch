[{"maximum": 2, "expect": 2, "hit": 2, "cost": 2, "markov": 2, "decision": 2, "process": 2, "informativeness": 1, "reward": 1, "falcon": 1, "z": 1, "dai": 1, "toyota": 2, "technological": 2, "institute": 2, "chicago": 4, "il": 2, "usa": 2, "60637": 2, "daitticedu": 1, "": 2, "matthew": 1, "r": 1, "walter": 1, "mwaltertticedu": 1, "abstract": 1, "propose": 1, "new": 1, "complexity": 1, "measure": 1, "mdps": 1, "mehc": 1}, {"measure": 1, "tighten": 1, "closely": 1, "relate": 1, "notion": 1, "diameter": 1, "joa10": 1, "account": 1, "reward": 1, "structure": 1}, {"show": 1, "parameter": 1, "replace": 1, "diameter": 1, "upper": 2, "bind": 1, "optimal": 1, "value": 1, "span": 1, "extend": 1, "mdp": 1, "thus": 1, "refine": 1, "associate": 1, "bound": 1, "regret": 1, "several": 1, "ucrl2like": 1, "algorithms": 1}, {"furthermore": 1, "show": 1, "potentialbased": 1, "reward": 2, "shape": 1, "nhr99": 1, "induce": 1, "equivalent": 1, "function": 1, "vary": 1, "informativeness": 1, "measure": 1, "mehc": 1}, {"establish": 1, "shape": 1, "reduce": 1, "increase": 1, "mehc": 2, "factor": 1, "two": 1, "large": 1, "class": 1, "mdps": 1, "finite": 1, "unsaturated": 1, "optimal": 1, "average": 1, "reward": 1}, {"1": 1, "": 2, "introduction": 1, "average": 2, "reward": 2, "set": 1, "reinforcement": 1, "learn": 2, "rl": 1, "put94": 1, "sb98": 1, "algorithm": 1, "maximize": 1, "interact": 1, "unknown": 1, "markov": 1, "decision": 1, "process": 1, "mdp": 1}, {"similar": 1, "analysis": 1, "multiarmed": 1, "bandits": 1, "online": 1, "machine": 1, "learn": 2, "problems": 1, "cumulative": 1, "regret": 1, "provide": 1, "natural": 1, "model": 1, "evaluate": 1, "efficiency": 1, "algorithm": 1}, {"p": 1, "ucrl2": 1, "algorithm": 1, "jaksch": 1, "ortner": 1, "auer": 1, "joa10": 1, "show": 1, "problemdependent": 1, "bind": 2, "ods": 1, "": 1, "regret": 2, "associate": 1, "logarithmic": 1, "expect": 1, "diameter": 1, "actual": 1, "mdp": 1, "definition": 1, "1": 1, "size": 2, "state": 1, "space": 2, "action": 1}, {"many": 1, "subsequent": 1, "algorithms": 1, "flp19": 1, "enjoy": 1, "similar": 1, "diameterdependent": 1, "bound": 1}, {"establish": 1, "diameter": 1, "important": 1, "measure": 1, "complexity": 1, "mdp": 1}, {"however": 1, "strikingly": 1, "measure": 1, "independent": 1, "reward": 1, "function": 1, "transition": 1}, {"obviously": 1, "peculiar": 1, "two": 1, "mdps": 1, "differ": 1, "reward": 2, "would": 1, "regret": 1, "bound": 1, "even": 1, "one": 1, "give": 1, "maximum": 1, "transition": 1}, {"review": 1, "relate": 1, "key": 1, "observation": 1, "jaksch": 1, "ortner": 1, "auer": 1, "joa10": 1, "refine": 1, "new": 1, "lemma": 2, "1": 2, "establish": 1, "rewardsensitive": 1, "complexity": 1, "measure": 1, "refer": 1, "maximum": 1, "expect": 1, "hit": 1, "cost": 1, "mehc": 1, "definition": 1, "2": 1, "tighten": 1, "regret": 1, "bound": 1, "ucrl2": 1, "similar": 1, "algorithms": 1, "replace": 1, "diameter": 1, "theorem": 1}, {"next": 1, "respect": 1, "new": 1, "complexity": 1, "measure": 1, "describe": 1, "notion": 1, "reward": 1, "informativeness": 1, "section": 1, "24": 1}, {"intuitively": 1, "speak": 1, "environment": 1, "desire": 1, "policies": 1, "motivate": 1, "different": 1, "immediate": 1, "reward": 1}, {"differ": 1, "definitions": 1, "reward": 2, "less": 1, "informative": 1, "useful": 1, "action": 1, "ie": 1, "yield": 1, "high": 1, "longterm": 1}, {"formalize": 1, "intuition": 1, "study": 1, "way": 1, "reparametrize": 1, "reward": 3, "via": 1, "potentialbased": 1, "shape": 1, "pbrs": 1, "nhr99": 1, "produce": 1, "different": 1, "nearoptimal": 1, "policies": 1, "section": 1, "25": 1}, {"show": 1, "mehc": 1, "change": 1, "reparametrization": 1, "pbrs": 1, "turn": 1, "regret": 1, "sample": 1, "complexity": 1, "substantiate": 1, "notion": 1, "informativeness": 1}, {"lastly": 1, "study": 1, "extent": 1, "impact": 1}, {"particular": 1, "show": 1, "factoroftwo": 1, "limit": 1, "impact": 1, "mehc": 1, "large": 1, "class": 1, "mdps": 1, "theorem": 1, "2": 1}, {"result": 1, "concept": 1, "reward": 2, "informativeness": 1, "may": 1, "useful": 1, "task": 1, "designer": 1, "craft": 1, "function": 1, "section": 1, "3": 1}, {"detail": 1, "proof": 1, "deffered": 1, "appendix": 1}, {"33rd": 1, "conference": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "neurips": 1, "2019": 1, "vancouver": 1, "canada": 1}, {"main": 1, "contributions": 1, "work": 1, "twofold": 1, "": 1, "propose": 1, "new": 1, "mdp": 1, "structural": 1, "parameter": 1, "maximum": 1, "expect": 1, "hit": 1, "cost": 1, "mehc": 1, "account": 1, "transition": 1, "reward": 1}, {"parameter": 1, "replace": 1, "diameter": 1, "regret": 1, "bound": 1, "several": 1, "modelbased": 1, "rl": 1, "algorithms": 1}, {"": 1, "show": 1, "potentialbased": 1, "reward": 1, "shape": 1, "change": 1, "maximum": 1, "expect": 1, "hit": 1, "cost": 1, "mdp": 1, "thus": 1, "regret": 1, "bind": 1}, {"result": 1, "set": 1, "equivalent": 1, "mdps": 1, "different": 1, "learn": 1, "difficulties": 1, "measure": 1, "regret": 1}, {"moreover": 1, "show": 1, "mehcs": 1, "differ": 1, "factor": 1, "two": 1, "large": 1, "class": 1, "mdps": 1}, {"11": 1, "": 2, "relate": 2, "work": 2, "closely": 1, "study": 1, "diameter": 1, "mdp": 1, "complexity": 1, "measure": 1, "joa10": 1, "prevalent": 1, "regret": 1, "bound": 1, "rl": 1, "algorithms": 1, "average": 1, "reward": 1, "set": 1, "flp19": 1}, {"note": 1, "jaksch": 1, "ortner": 1, "auer": 1, "joa10": 1, "unlike": 1, "previous": 1, "measure": 1, "mdp": 1, "complexity": 1, "return": 1, "mix": 1, "time": 1, "ks02": 1, "bt02": 1, "diameter": 1, "depend": 1, "transition": 1, "reward": 1}, {"core": 1, "reason": 1, "presence": 1, "diameter": 1, "regret": 1, "analysis": 1, "upper": 1, "bound": 1, "optimal": 1, "value": 1, "span": 1, "extend": 1, "mdp": 1, "summarize": 1, "observations": 1, "section": 1, "23": 1, "equation": 1, "8": 1}, {"review": 1, "update": 1, "observation": 1, "rewarddependent": 1, "parameter": 1, "call": 1, "maximum": 1, "expect": 1, "hit": 1, "cost": 1, "lemma": 1, "1": 1}, {"interestingly": 1, "gap": 1, "diameter": 2, "mehc": 2, "arbitrarily": 1, "large": 1, "": 3, "rmax": 1, "dm": 1, "mdps": 1, "finite": 1, "infinite": 1}, {"mdps": 1, "noncommunicating": 1, "saturate": 1, "optimal": 1, "average": 1, "reward": 1, "": 4, "rmax": 1}, {"intuitively": 1, "state": 3, "mdps": 1, "learner": 1, "cannot": 1, "visit": 1, "s0": 1, "": 1, "nonetheless": 1, "achieve": 1, "maximum": 1, "possible": 1, "average": 1, "reward": 1, "thus": 1, "allow": 1, "good": 1, "regret": 1, "guarantee": 1, "unreachable": 1, "seem": 1, "better": 1, "reachable": 1, "ones": 1, "principle": 1, "optimism": 1, "face": 1, "uncertainty": 1, "ofu": 1}, {"use": 1, "ucrl2": 1, "joa10": 1, "example": 1, "algorithm": 1, "throughout": 1, "rest": 1, "article": 1, "however": 1, "main": 1, "result": 1, "depend": 1}, {"particular": 1, "mehc": 1, "regret": 1, "bound": 1, "update": 1, "theorem": 1, "1": 1}, {"another": 1, "important": 1, "comparison": 1, "optimal": 1, "bias": 1, "span": 1, "put94": 1, "bt09": 1, "fru18": 1, "rewarddependent": 1, "parameter": 1, "mdps": 1}, {"find": 1, "gap": 1, "arbitrarily": 1, "large": 1, "spm": 1, "": 6, "1": 1, "noncommunicating": 1, "mdps": 1, "would": 1, "unsaturated": 1, "optimal": 1, "average": 1, "reward": 1, "rmax": 1}, {"show": 1, "elsewhere": 1, "fpl18": 1, "fru18": 1, "extra": 1, "knowledge": 1, "upper": 1, "bind": 1, "optimal": 1, "bias": 1, "span": 1, "necessary": 1, "algorithm": 1, "enjoy": 1, "regret": 1, "scale": 1, "smaller": 1, "parameter": 1}, {"contrast": 1, "ucrl2": 1, "scale": 1, "mehc": 2, "need": 1, "know": 1, "diameter": 1, "actual": 1, "mdp": 1}, {"potentialbased": 1, "reward": 1, "shape": 1, "nhr99": 1, "originally": 1, "propose": 1, "solution": 1, "technique": 1, "programmer": 1, "influence": 1, "sample": 1, "complexity": 1, "reinforcement": 1, "learn": 1, "algorithm": 1, "without": 1, "change": 1, "nearoptimal": 1, "policies": 1, "episodic": 1, "discount": 1, "settings": 1}, {"prior": 1, "theoretical": 1, "analysis": 1, "involve": 1, "pbrs": 1, "nhr99": 1, "wie03": 1, "wce03": 1, "alz08": 1, "grz17": 1, "mostly": 1, "focus": 1, "consistency": 1, "rl": 1, "shape": 1, "reward": 1, "ie": 1, "result": 1, "learn": 1, "behavior": 1, "also": 1, "nearoptimal": 1, "original": 1, "mdp": 1, "suggest": 1, "empirically": 1, "sample": 1, "complexity": 1, "change": 1, "well": 1, "specify": 1, "potential": 1}, {"work": 1, "use": 1, "pbrs": 1, "construct": 1, "equivalent": 1, "reward": 3, "function": 2, "average": 1, "set": 1, "section": 2, "24": 1, "show": 1, "two": 1, "relate": 1, "shape": 1, "potential": 1, "different": 2, "mehcs": 1, "thus": 1, "regret": 1, "sample": 1, "complexities": 1, "25": 1}, {"however": 1, "subtle": 1, "important": 1, "technical": 1, "requirement": 1, "0": 2, "rmax": 2, "boundedness": 2, "mdps": 1, "make": 1, "difficult": 1, "immediately": 1, "apply": 1, "result": 1, "section": 1, "25": 1, "theorem": 1, "2": 1, "treatment": 1, "pbrs": 1, "solution": 1, "technique": 1, "arbitrary": 1, "potential": 1, "function": 1, "pick": 1, "without": 1, "knowledge": 1, "original": 1, "mdp": 1, "may": 1, "preserve": 1}, {"nevertheless": 1, "think": 1, "work": 1, "may": 1, "bring": 1, "new": 1, "perspectives": 1, "topic": 1}, {"1": 2, "inequality": 1, "derive": 1, "consequence": 1, "lemma": 1, "n": 1, "": 1}, {"1": 1, "": 2, "tight": 1, "confidence": 1, "intervals": 1, "around": 1, "actual": 1, "transition": 1, "mean": 1, "reward": 1}, {"observe": 1, "span": 1, "ui": 1, "equal": 1, "spm": 1, "": 2, "limit": 1}, {"1": 1, "joa10": 1, "remark": 1, "8": 1}, {"2": 2, "": 7, "21": 1, "result": 1, "markov": 2, "decision": 2, "process": 2, "define": 1, "tuple": 1, "p": 2, "r": 1, "state": 1, "space": 2, "action": 1}, {"ps": 1, "transition": 1, "function": 1, "r": 1, "": 3}, {"p0": 1, "rmax": 1, "": 2, "reward": 1, "function": 1, "mean": 1, "rs": 1, "ers": 1}, {"assume": 1, "state": 1, "action": 1, "space": 1, "finite": 1, "size": 1, "": 2, "respectively": 1}, {"time": 1, "step": 1, "": 2, "0": 1, "1": 1, "2": 1}, {"": 1}, {"": 1, "algorithm": 1, "l": 1, "choose": 1, "action": 1, "2": 1, "base": 1, "observations": 1, "point": 1}, {"state": 1, "transition": 2, "st1": 1, "probability": 1, "pst1": 1, "st": 1, "": 4, "reward": 2, "rt": 1, "2": 2, "0": 1, "rmax": 1, "draw": 1, "accord": 1, "distribution": 1, "rst": 1, "probabilities": 1, "function": 1, "mdp": 1, "unknown": 1, "learner": 1}, {"sequence": 1, "random": 1, "variables": 1, "st": 1, "": 2, "rt": 1, "0": 1, "form": 1, "stochastic": 1, "process": 1}, {"note": 1, "stationary": 1, "deterministic": 1, "policy": 1, "": 3}, {"restrictive": 1, "type": 1, "algorithm": 1, "whose": 1, "action": 1, "depend": 1, "st": 1, "": 1}, {"refer": 1, "stationary": 1, "deterministic": 1, "policies": 2, "rest": 1, "paper": 1}, {"recall": 1, "markov": 1, "chain": 1, "hit": 1, "time": 1, "state": 2, "s0": 3, "start": 1, "random": 1, "variable": 1, "hss0": 1, "": 3, "inft": 1, "2": 1, "n": 1, "0": 1, "st": 1, "s3": 1, "lpw08": 1}, {"definition": 1, "1": 1, "diameter": 1, "joa10": 1}, {"suppose": 1, "stochastic": 1, "process": 1, "induce": 1, "follow": 1, "policy": 1, "": 3, "mdp": 1, "time": 1, "hit": 1, "state": 2, "s0": 1, "start": 1, "hss0": 1}, {"define": 1, "diameter": 1, "dm": 1, "": 4, "max": 1, "min": 1, "e": 1, "hss0": 1}, {"0": 1, "ss": 1, "2s": 1, "sa": 1, "": 1, "incorporate": 1, "reward": 1, "diameter": 1, "introduce": 1, "novel": 1, "mdp": 1, "parameter": 1}, {"definition": 1, "2": 1, "maximum": 1, "expect": 1, "hit": 1, "cost": 1}, {"define": 1, "maximum": 1, "expect": 1, "hit": 1, "cost": 1, "markov": 1, "decision": 1, "process": 1, "2": 1, "3": 1, "hss0": 1, "1": 1, "x": 1, "": 3, "max": 1, "min": 1, "e": 1, "4": 1, "rmax": 1, "rt": 1, "5": 1}, {"0": 1, "ss": 1, "2s": 1, "sa": 1, "": 9, "t0": 1, "observe": 1, "mehc": 1, "smaller": 1, "parameter": 1, "rmax": 3, "dm": 1, "since": 1, "s0": 1, "rt": 1}, {"22": 1, "": 5, "average": 1, "reward": 2, "criterion": 1, "regret": 1, "accumulate": 1, "algorithm": 1, "l": 2, "time": 1, "step": 1, "mdp": 1, "start": 1, "state": 1, "random": 1, "variable": 1, "x1": 1, "rm": 1, "rt": 1}, {"t0": 1, "": 3, "define": 1, "average": 1, "reward": 1, "gain": 1, "put94": 1, "1": 1, "e": 1, "rm": 1, "l": 1}, {"1": 2, "": 4, "l": 1, "lim": 1, "evaluate": 1, "policies": 1, "average": 1, "reward": 1}, {"maximize": 1, "stationary": 1, "deterministic": 1, "policy": 1, "define": 1, "optimal": 1, "average": 1, "reward": 1, "start": 1, "state": 1, "": 3, "max": 1}, {"sa": 1, "": 6, "2": 1, "furthermore": 1, "assume": 1, "optimal": 1, "average": 1, "reward": 1, "start": 1, "state": 2, "ie": 1, "maxs0": 1, "s0": 1, "natural": 1, "requirement": 1, "mdp": 1, "online": 1, "set": 1, "allow": 1, "hope": 1, "vanish": 1, "regret": 1}, {"otherwise": 1, "learner": 1, "may": 1, "take": 1, "action": 1, "lead": 1, "state": 1, "lower": 1, "average": 1, "optimal": 1, "reward": 2, "due": 1, "ignorance": 1, "incur": 1, "linear": 1, "regret": 1, "2": 1, "important": 1, "assume": 1, "support": 1, "lie": 1, "know": 1, "bound": 1, "interval": 1, "often": 1, "0": 1, "1": 1, "convention": 1}, {"sometimes": 1, "refer": 1, "bound": 1, "mdp": 1, "literature": 1}, {"analogous": 1, "bandits": 1, "detail": 1, "reward": 2, "distribution": 1, "often": 1, "unimportant": 1, "suffice": 1, "specify": 1, "mdp": 1, "mean": 1, "r": 1}, {"3": 1, "0indexing": 1, "ensure": 1, "hss": 1, "": 1, "0": 1}, {"note": 1, "also": 1, "convention": 1, "inf": 1, "": 1}, {"": 1, "1": 1}, {"3": 1, "": 1, "compare": 1, "optimal": 1, "policy": 1, "start": 1, "initial": 1, "state": 1}, {"particular": 1, "condition": 1, "true": 1, "communicate": 1, "mdps": 2, "put94": 1, "virtue": 1, "transition": 1, "also": 1, "possible": 1, "noncommunicating": 1, "appropriate": 1, "reward": 1}, {"write": 1, "": 5, "maxs0": 1, "s0": 1}, {"compete": 1, "expect": 1, "cumulative": 1, "reward": 1, "optimal": 1, "policy": 1, "trajectory": 1, "define": 1, "regret": 1, "learn": 1, "algorithm": 1, "l": 3, "start": 1, "state": 1, "time": 1, "step": 1, "": 6, "23": 1, "rm": 1}, {"3": 1, "": 2, "optimism": 2, "face": 2, "uncertainty": 2, "extend": 1, "mdp": 1, "ucrl2": 1, "principle": 1, "ofu": 1, "sb98": 1, "state": 1, "uncertain": 1, "stateaction": 1, "pair": 1, "ie": 1, "visit": 1, "enough": 1, "point": 1, "optimistic": 1, "outcome": 1}, {"intuition": 1, "take": 1, "rewardmaximizing": 1, "action": 1, "respect": 1, "optimistic": 1, "model": 1, "term": 1, "transition": 1, "immediate": 1, "reward": 1, "uncertain": 1, "stateaction": 2, "pair": 2, "regret": 1, "optimism": 1, "well": 1, "place": 1, "otherwise": 1, "quickly": 1, "learn": 1, "suboptimal": 1, "avoid": 1, "future": 1}, {"fruitful": 1, "idea": 1, "basis": 1, "many": 1, "modelbased": 1, "rl": 1, "algorithms": 1, "flp19": 1, "particular": 1, "ucrl2": 1, "joa10": 1, "keep": 1, "track": 1, "statistical": 1, "uncertainty": 1, "via": 1, "upper": 1, "confidence": 1, "bound": 1}, {"suppose": 1, "visit": 1, "particular": 1, "stateaction": 1, "pair": 1, "n": 1, "amany": 1, "time": 1}, {"confidence": 2, "least": 1, "1": 1, "": 1, "establish": 1, "interval": 1, "mean": 1, "reward": 1, "rs": 1, "transition": 1, "ps": 1, "chernoffhoeffding": 1, "inequality": 1, "bernstein": 1, "fpl18": 1}, {"let": 1, "b": 1, "": 1, "n": 2, "2": 1, "r": 1, "confidence": 1, "bind": 1, "observe": 1, "iid": 1}, {"sample": 1, "0": 1, "1bounded": 1, "random": 1, "variable": 1, "rs": 2, "empirical": 2, "mean": 1, "ps": 2, "transition": 1}, {"statistically": 2, "plausible": 2, "mean": 1, "reward": 1, "b": 3, "": 15, "r0": 2, "2": 2, "r": 1, "rs": 1, "rmax": 2, "n": 2, "0": 1, "transition": 1, "c": 1, "p0": 2, "ps": 2, "a1": 1}, {"define": 1, "extend": 1, "mdp": 1, "": 25, "p": 5, "r": 5, "summarize": 1, "statistics": 1, "gld00": 1, "sl05": 1, "tb07": 1, "joa10": 1, "state": 1, "space": 3, "action": 3, "union": 1, "statespecific": 1, "0": 4, "2": 3, "c": 1, "b": 1, "4": 1, "transition": 1, "accord": 2, "select": 2, "distribution": 1, "p0": 4, "r0": 4, "5": 1, "reward": 2, "mean": 1}, {"6": 1, "": 2, "hard": 1, "see": 1, "indeed": 1, "mdp": 1, "infinite": 1, "compact": 1, "action": 1, "space": 1}, {"ofu": 1, "want": 1, "find": 1, "optimal": 1, "policy": 1, "optimistic": 1, "mdp": 1, "within": 1, "set": 1, "statistically": 1, "plausible": 1, "mdps": 1}, {"observe": 1, "joa10": 1, "equivalent": 1, "find": 1, "optimal": 1, "policy": 1, "": 4}, {"extend": 1, "mdp": 2, "": 14, "specify": 1, "policy": 1, "via": 2, "1": 1, "f": 1, "pe": 1, "transition": 1, "projection": 1, "map": 1, "onto": 1, "ith": 1, "coordinate": 1, "optimistic": 1, "pes": 1, "2": 1, "mean": 1, "reward": 1, "res": 1, "3": 1, "action": 1, "select": 1, "4": 1}, {"construction": 1, "extend": 1, "mdp": 1, "": 3, "high": 1, "confidence": 1, "ie": 1, "rs": 1, "2": 4, "b": 1, "ps": 1, "c": 1}, {"heart": 1, "ucrl2type": 1, "regret": 1, "analysis": 1, "key": 1, "observation": 1, "joa10": 1, "equation": 1, "11": 1, "bind": 1, "span": 1, "optimal": 1, "value": 1, "extend": 1, "mdp": 2, "": 3, "diameter": 1, "actual": 1, "condition": 1}, {"observation": 1, "need": 1, "characterize": 1, "good": 1, "follow": 1, "optimistic": 1, "policy": 1, "1": 1, "": 3, "actual": 1, "mdp": 1}, {"0": 1, "istep": 1, "optimal": 1, "value": 1, "ui": 1, "": 2, "expect": 1, "total": 1, "reward": 2, "4": 1, "set": 1, "transition": 1, "mean": 1, "action": 1, "6": 1, "p": 1, "r": 1, "respectively": 1}, {"4": 2, "": 62, "follow": 1, "optimal": 1, "nonstationary": 1, "istep": 1, "policy": 1, "start": 1, "state": 1, "2": 1, "also": 1, "define": 1, "recursively": 1, "via": 1, "dynamic": 1, "programming5": 1, "u0": 1, "0": 12, "ui1": 1, "max": 5, "ap0": 2, "r": 6, "2a": 2, "5": 1, "6": 1, "a2a": 1, "p0": 3, "r0": 2, "s0": 5, "x": 3, "2b": 1, "sa": 2, "p": 3, "ui": 3, "2c": 1, "7": 1, "ready": 1, "restate": 1, "observation": 1}, {"": 5, "happen": 1, "high": 1, "probability": 1, "jaksch": 1, "ortner": 1, "auer": 1, "joa10": 1, "observe": 1, "min": 1, "ui": 1, "s0": 1, "rmax": 1, "dm": 1}, {"0": 1, "": 4, "max": 1, "ui": 1, "8": 1, "however": 1, "bind": 1, "conservative": 1, "fail": 1, "account": 1, "reward": 1, "collect": 1}, {"patch": 1, "tighten": 1, "upper": 1, "bind": 1, "mehc": 1}, {"lemma": 1, "1": 1, "mehc": 1, "upper": 1, "bound": 1, "span": 1, "value": 1}, {"assume": 1, "actual": 1, "mdp": 2, "extend": 1, "": 7, "ie": 1, "rs": 1, "2": 5, "b": 1, "ps": 1, "c": 1, "max": 1, "ui": 3, "min": 1, "s0": 1, "0": 1, "istep": 1, "optimal": 1, "undiscounted": 1, "value": 1, "state": 1, "refine": 1, "upper": 1, "bind": 1, "immediately": 1, "plug": 1, "main": 1, "theorems": 1, "joa10": 1, "equations": 1, "19": 1, "22": 1, "theorem": 1}, {"theorem": 1, "1": 1, "rewardsensitive": 1, "regret": 1, "bind": 1, "ucrl2": 1}, {"probability": 1, "least": 1, "1": 2, "initial": 1, "state": 1, "": 19, "regret": 1, "ucrl2": 2, "bound": 1, "p": 1, "5": 2, "8t": 3, "log": 2, "sa": 2, "log2": 1, "8": 1, "2": 1}, {"": 18, "p": 2, "2at": 1, "2sat": 1, "14s": 1, "log": 3, "14": 1, "2": 2, "1": 1, "sit": 1, "34": 1, "max1": 1}, {"": 5, "2": 2, "corollary": 2, "theorem": 1, "1": 2, "imply": 1, "ucrl2": 1, "offer": 1, "s2": 1, "log": 1, "sa": 1, "sample": 1, "complexity": 1, "kak03": 1, "invert": 1, "regret": 2, "bind": 1, "demand": 1, "perstep": 1, "probability": 1, "least": 1, "joa10": 1, "3": 1}, {"similarly": 1, "update": 1, "logarithmic": 1, "bind": 1, "expect": 1, "2": 2, "regret": 1, "joa10": 1, "theorem": 1, "4": 1, "e": 1, "ucrl2": 1, "": 4, "ag": 1, "log": 1, "g": 1, "gap": 1, "average": 1, "reward": 1, "best": 2, "policy": 2, "second": 1}, {"5": 2, "": 5, "fact": 1, "exact": 1, "maximization": 1, "equation": 1, "7": 1, "find": 1, "via": 1, "extend": 1, "value": 2, "iteration": 1, "joa10": 1, "section": 1, "31": 1, "24": 1, "informativeness": 1, "reward": 2, "informally": 1, "hard": 1, "appreciate": 1, "challenge": 1, "impose": 1, "delay": 1, "feedback": 1, "inherent": 1, "mdps": 1, "action": 1, "high": 2, "immediate": 1, "necessarily": 1, "lead": 1, "optimal": 1}, {"different": 1, "equivalent": 1, "reward": 1, "function": 1, "differ": 1, "informativeness": 1, "informative": 1, "ones": 1, "easier": 1, "reinforcement": 1, "learn": 1}, {"suppose": 1, "two": 1, "mdps": 1, "differ": 1, "reward": 1, "m1": 1, "": 7, "p": 2, "r1": 1, "m2": 1, "r2": 1, "diameters": 1, "dm1": 1, "dm2": 1, "thus": 1, "diameterdependent": 1, "regret": 1, "bound": 1, "previous": 1, "work": 1}, {"mehc": 1, "however": 1, "may": 1, "get": 1, "meaningful": 1, "answer": 1}, {"firstly": 1, "let": 1, "us": 1, "make": 1, "precise": 1, "notion": 1, "equivalence": 1}, {"say": 1, "r1": 1, "r2": 1, "equivalent": 1, "policy": 1, "": 3}, {"average": 1, "reward": 2, "two": 1, "function": 1, "m1": 1, "": 5, "m2": 1}, {"formally": 1, "study": 1, "mehc": 1, "class": 1, "equivalent": 1, "reward": 1, "function": 1, "relate": 1, "via": 1, "potential": 1}, {"25": 1, "": 5, "potentialbased": 2, "reward": 2, "shape": 2, "originally": 1, "introduce": 1, "ng": 1, "harada": 1, "russell": 1, "nhr99": 1, "pbrs": 1, "take": 1, "potential": 1}, {"r": 1, "define": 1, "shape": 1, "reward": 1, "rt": 2, "": 4, "st": 1, "st1": 1}, {"9": 1, "think": 1, "stochastic": 1, "process": 1, "st": 1, "": 8, "rt": 1, "0": 1, "generate": 1, "mdp": 1, "p": 1, "r": 2, "reward": 1, "function": 1}, {"p0": 1, "rmax": 1, "6": 1, "whose": 1, "mean": 1, "reward": 1, "r": 1, "": 4, "rs": 1, "es0": 1, "psa": 1, "s0": 1}, {"easy": 1, "check": 1, "r": 2, "indeed": 1, "equivalent": 1}, {"policy": 1, "": 37, "1": 14, "lim": 6, "e": 6, "rm": 3, "x": 2, "rt": 3, "t0": 3, "st": 2, "st1": 1, "telescope": 1, "sum": 1, "potential": 1, "term": 2, "consecutive": 1, "x1": 1, "s0": 1, "est": 1, "first": 1, "two": 1, "vanish": 1, "limit": 1}, {"10": 1, "": 1, "get": 1, "intuition": 1, "instructive": 1, "consider": 1, "toy": 1, "example": 1, "figure": 1, "1": 1}, {"suppose": 1, "0": 2, "": 14, "2": 1, "1": 2, "optimal": 2, "average": 2, "reward": 2, "mdp": 1, "stationary": 1, "deterministic": 1, "policy": 1, "s1": 1, "a2": 1, "s2": 2, "a1": 1, "stay": 1, "state": 1, "yield": 1, "highest": 1}, {"expect": 1, "number": 1, "step": 1, "need": 1, "transition": 1, "state": 1, "s1": 1, "s2": 1, "vice": 1, "versa": 1, "1": 1, "via": 1, "action": 1, "a2": 1, "": 8, "conclude": 1, "max": 1}, {"furthermore": 1, "notice": 1, "take": 2, "action": 2, "a2": 1, "either": 1, "state": 2, "transition": 1, "probability": 1, "": 1, "however": 1, "immediate": 2, "reward": 2, "alternative": 1, "a1": 1, "stay": 1, "current": 1, "statethe": 1, "informative": 1}, {"differentiate": 1, "action": 1, "better": 1, "shape": 1, "potential": 1, "s1": 1, "": 5, "0": 1, "s2": 1, "2": 1}, {"shape": 1, "mean": 1, "reward": 1, "become": 1, "s1": 5, "": 20, "r": 2, "a2": 1, "1": 4, "s2": 1, "2": 1, "a1": 1, "6": 1, "one": 1, "need": 1, "ensure": 1, "respect": 1, "0": 1, "rmax": 1, "boundedness": 1}, {"6": 1, "": 21, "a2": 2, "1": 9, "a1": 2, "s1": 1, "s2": 1, "figure": 1, "circular": 1, "nod": 2, "represent": 2, "state": 1, "square": 1, "action": 1}, {"solid": 1, "edge": 2, "label": 2, "transition": 1, "probabilities": 1, "dash": 1, "mean": 1, "reward": 1}, {"furthermore": 1, "rmax": 1, "": 1, "1": 1}, {"concreteness": 1, "one": 1, "consider": 1, "set": 1, "": 24, "011": 1, "01": 1, "005": 1, "s2": 5, "r": 2, "a2": 1, "1": 4, "s1": 1, "2": 1, "a1": 1}, {"encourage": 1, "take": 2, "action": 2, "a2": 1, "state": 2, "s1": 1, "discourage": 1, "a1": 1, "s2": 1, "simultaneously": 1}, {"maximum": 1, "expect": 1, "hit": 1, "cost": 1, "become": 1, "smaller": 1, "": 29, "max": 2, "s1": 2, "s2": 2, "2": 3}, {"": 1, "example": 1, "mehc": 1, "halve": 1, "best": 1, "make": 1, "arbitrarily": 1, "close": 1, "zero": 1}, {"note": 1, "original": 1, "mdp": 1, "equivalent": 1, "": 2, "shape": 1, "potential": 1, "ie": 1}, {"": 4, "9": 1, "see": 1, "mehc": 1, "almost": 1, "double": 1}, {"turn": 1, "halve": 1, "double": 1, "mehc": 1, "pbrs": 1, "large": 1, "class": 1, "mdps": 1}, {"theorem": 1, "2": 1, "mehc": 1, "pbrs": 1}, {"give": 1, "mdp": 2, "finite": 1, "maximum": 2, "expect": 2, "hit": 2, "cost": 2, "": 13, "1": 2, "unsaturated": 1, "optimal": 1, "average": 1, "reward": 1, "rmax": 1, "pbrsparameterized": 1, "bound": 1, "multiplicative": 1, "factor": 1, "two": 1, "2m": 1}, {"2": 1, "key": 1, "observation": 1, "expect": 1, "total": 1, "reward": 1, "along": 1, "loop": 1, "remain": 1, "unchanged": 1, "shape": 1, "originally": 1, "motivate": 1, "pbrs": 1, "nhr99": 1}, {"see": 1, "consider": 1, "loop": 1, "concatenation": 1, "two": 1, "paths": 1, "one": 1, "s0": 3, "shape": 1, "potential": 1, "": 2, "expect": 1, "total": 1, "reward": 1, "former": 1, "increase": 1, "latter": 1, "decrease": 1, "amount": 1}, {"detail": 1, "see": 1, "appendix": 1, "a2": 1}, {"3": 1, "": 2, "discussion": 1, "view": 1, "rl": 1, "engineer": 1, "tool": 1, "compile": 1, "arbitrary": 1, "reward": 2, "function": 2, "behavior": 1, "represent": 1, "policy": 1, "environment": 1, "programmers": 1, "primary": 1, "responsibility": 1, "would": 1, "craft": 1, "faithfully": 1, "express": 1, "intend": 1, "goal": 1}, {"however": 1, "problem": 1, "reward": 1, "design": 1, "complicate": 1, "practical": 1, "concern": 1, "difficulty": 1, "learn": 1}, {"recognize": 1, "kober": 1, "bagnell": 1, "peters": 1, "kbp13": 1, "section": 1, "34": 1, "also": 1, "tradeoff": 1, "complexity": 2, "reward": 3, "function": 1, "learn": 2, "problem": 1, "7": 1, "": 1, "accurate": 1, "often": 1, "easy": 1, "specify": 2, "sparse": 1, "manner": 1, "reach": 1, "position": 1, "capture": 1, "king": 1, "etc": 1, "thus": 1, "hard": 1, "whereas": 1, "dense": 1, "provide": 1, "feedback": 1, "harder": 1, "accurately": 1, "lead": 1, "incorrect": 1, "train": 1, "behaviors": 1}, {"recent": 1, "rise": 1, "deep": 1, "rl": 1, "also": 1, "expose": 1, "bug": 1, "design": 1, "reward": 1, "ca16": 1}, {"result": 1, "show": 1, "informativeness": 1, "reward": 2, "aspect": 1, "complexity": 1, "learn": 1, "problem": 1, "control": 1, "extent": 1, "well": 1, "specify": 1, "potential": 1, "without": 1, "inadvertently": 1, "change": 1, "intend": 1, "behaviors": 1, "original": 1}, {"therefore": 1, "propose": 1, "separate": 1, "definitional": 1, "concern": 2, "train": 1}, {"reward": 1, "first": 1, "define": 1, "faithfully": 1, "express": 1, "intend": 1, "task": 1, "extra": 1, "knowledge": 1, "incorporate": 1, "via": 1, "shape": 1, "potential": 1, "reduce": 1, "sample": 1, "complexity": 1, "train": 1, "obtain": 1, "desire": 1, "behaviors": 1}, {"say": 1, "generally": 1, "easy": 1, "find": 1, "helpful": 1, "potential": 1, "make": 1, "reward": 1, "informative": 1}, {"though": 1, "theorem": 1, "2": 1, "might": 1, "disappoint": 1, "result": 2, "pbrs": 1, "wish": 1, "emphasize": 1, "directly": 1, "concern": 1, "algorithms": 1, "whose": 1, "regret": 1, "scale": 1, "mehc": 1, "ucrl2": 1}, {"conceivable": 1, "different": 2, "set": 1, "discount": 1, "total": 1, "reward": 1, "rl": 1, "algorithm": 1, "sarsa": 1, "epsilongreedy": 1, "exploration": 1, "nhr99": 1, "footnote": 1, "4": 1, "pbrs": 1, "might": 1, "greater": 1, "impact": 1, "learn": 1, "efficiency": 1}, {"acknowledgments": 1, "work": 1, "support": 1, "part": 1, "national": 1, "science": 1, "foundation": 1, "grant": 1}, {"1830660": 1}, {"thank": 1, "avrim": 1, "blum": 1, "many": 1, "insightful": 1, "comment": 1}, {"particular": 1, "challenge": 1, "find": 1, "better": 1, "example": 1, "lead": 1, "theorem": 1, "2": 1}, {"also": 1, "thank": 1, "ronan": 1, "fruit": 1, "discussion": 1, "concept": 1, "similar": 1, "propose": 1, "maximum": 1, "expect": 1, "hit": 1, "cost": 1, "independently": 1, "develop": 1, "thesis": 1, "draft": 1}, {"reference": 1, "alz08": 1, "bt02": 1, "bt09": 1, "ca16": 1, "flp19": 1, "fpl18": 1, "fru18": 1, "gld00": 1, "grz17": 1, "": 2, "joa10": 1, "kak03": 1, "john": 1, "asmuth": 1, "michael": 1, "l": 1, "littman": 1, "robert": 1, "zinkov": 1}, {"potentialbased": 1, "shape": 1, "modelbased": 1, "reinforcement": 1, "learn": 1, "proceed": 1, "national": 1, "conference": 1, "artificial": 1, "intelligence": 1, "aaai": 1}, {"2008": 1, "pp": 1}, {"604609": 1}, {"ronen": 1, "brafman": 1, "moshe": 1, "tennenholtz": 1}, {"rmaxa": 1, "general": 1, "polynomial": 1, "time": 1, "algorithm": 1, "nearoptimal": 1, "reinforcement": 1, "learn": 1}, {"journal": 1, "machine": 1, "learn": 1, "research": 1, "3oct": 1, "2002": 1, "pp": 1}, {"213231": 1}, {"peter": 1, "l": 1, "bartlett": 1, "ambuj": 1, "tewari": 1}, {"regal": 1, "regularization": 1, "base": 1, "algorithm": 1, "reinforcement": 1, "learn": 1, "weakly": 1, "communicate": 1, "mdps": 1}, {"proceed": 1, "conference": 1, "uncertainty": 1, "artificial": 1, "intelligence": 1, "uai": 1}, {"auai": 1, "press": 1}, {"2009": 1, "pp": 1}, {"3542": 1}, {"jack": 1, "clark": 1, "dario": 1, "amodei": 1}, {"faulty": 1, "reward": 1, "function": 1, "wild": 1}, {"2016": 1}, {"url": 1, "https": 1, "openaicomblogfaultyrewardfunctions": 1, "visit": 1, "10272019": 1}, {"ronan": 1, "fruit": 1, "alessandro": 1, "lazaric": 1, "matteo": 1, "pirotta": 1}, {"explorationexploitation": 1, "reinforcement": 1, "learn": 1}, {"tutorial": 1, "algorithmic": 1, "learn": 1, "theory": 1, "conference": 1}, {"2019": 1}, {"url": 1, "httpsrlgammazerogithubiodocs2019altexptutorialpdf": 1}, {"ronan": 1, "fruit": 1, "matteo": 1, "pirotta": 1, "alessandro": 1, "lazaric": 1}, {"near": 1, "optimal": 1, "explorationexploitation": 1, "noncommunicating": 1, "markov": 1, "decision": 1, "process": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "neurips": 1}, {"2018": 1, "pp": 1}, {"29943004": 1}, {"ronan": 1, "fruit": 1, "matteo": 1, "pirotta": 1, "alessandro": 1, "lazaric": 1, "ronald": 1, "ortner": 1}, {"efficient": 1, "biasspanconstrained": 1, "explorationexploitation": 1, "reinforcement": 1, "learn": 1}, {"proceed": 1, "international": 1, "conference": 1, "machine": 1, "learn": 1, "icml": 1}, {"2018": 1, "pp": 1}, {"15731581": 1}, {"robert": 1, "givan": 1, "sonia": 1, "leach": 1, "thomas": 1, "dean": 1}, {"boundedparameter": 1, "markov": 1, "decision": 1, "process": 1}, {"artificial": 1, "intelligence": 1, "12212": 1, "2000": 1, "pp": 1}, {"71109": 1}, {"marek": 1, "grzes": 1}, {"reward": 1, "shape": 1, "episodic": 1, "reinforcement": 1, "learn": 1}, {"proceed": 1, "international": 1, "conference": 1, "autonomous": 1, "agents": 1, "multiagent": 1, "systems": 1, "aamas": 1}, {"international": 1, "foundation": 1, "autonomous": 1, "agents": 1, "multiagent": 1, "systems": 1}, {"2017": 1, "pp": 1}, {"565": 1, "573": 1}, {"thomas": 1, "jaksch": 1, "ronald": 1, "ortner": 1, "peter": 1, "auer": 1}, {"nearoptimal": 1, "regret": 1, "bound": 1, "reinforcement": 1, "learn": 1}, {"journal": 1, "machine": 1, "learn": 1, "research": 1, "11apr": 1, "2010": 1, "pp": 1}, {"1563": 1, "1600": 1}, {"sham": 1, "machandranath": 1, "kakade": 1}, {"sample": 1, "complexity": 1, "reinforcement": 1, "learn": 1}, {"phd": 1, "thesis": 1}, {"2003": 1}, {"8": 1, "": 2, "kbp13": 1, "jens": 1, "kober": 1, "j": 1, "andrew": 1, "bagnell": 1, "jan": 1, "peters": 1}, {"reinforcement": 1, "learn": 1, "robotics": 1, "survey": 1}, {"international": 1, "journal": 1, "robotics": 1, "research": 1, "3211": 1, "2013": 1, "pp": 1}, {"1238": 1, "1274": 1}, {"ks02": 1, "michael": 1, "kearns": 1, "satinder": 1, "singh": 1}, {"nearoptimal": 1, "reinforcement": 1, "learn": 1, "polynomial": 1, "time": 1}, {"machine": 1, "learn": 1, "4923": 1, "2002": 1, "pp": 1}, {"209232": 1}, {"lpw08": 1, "david": 1, "levin": 1, "yuval": 1, "peres": 1, "elizabeth": 1, "l": 1, "wilmer": 1}, {"markov": 1, "chain": 1, "mix": 1, "time": 1}, {"american": 1, "mathematical": 1, "soc": 1, "2008": 1}, {"nhr99": 1, "andrew": 1, "ng": 1, "daishi": 1, "harada": 1, "stuart": 1, "russell": 1}, {"policy": 1, "invariance": 1, "reward": 2, "transformations": 1, "theory": 1, "application": 1, "shape": 1}, {"proceed": 1, "international": 1, "conference": 1, "machine": 1, "learn": 1, "icml": 1}, {"vol": 1}, {"99": 1}, {"1999": 1, "pp": 1}, {"278287": 1}, {"put94": 1, "martin": 1, "l": 1, "puterman": 1}, {"markov": 1, "decision": 1, "process": 1, "discrete": 1, "stochastic": 1, "dynamic": 1, "program": 1}, {"john": 1, "wiley": 1, "": 1, "sons": 1, "inc": 1, "1994": 1}, {"sb98": 1, "richard": 1, "sutton": 1, "andrew": 1, "g": 1, "barto": 1}, {"reinforcement": 1, "learn": 1, "introduction": 1}, {"mit": 1, "press": 1, "1998": 1}, {"sl05": 1, "alexander": 1, "l": 2, "strehl": 1, "michael": 1, "littman": 1}, {"theoretical": 1, "analysis": 1, "modelbased": 1, "interval": 1, "estimation": 1}, {"proceed": 1, "international": 1, "conference": 1, "machine": 1, "learn": 1, "icml": 1}, {"acm": 1}, {"2005": 1, "pp": 1}, {"856863": 1}, {"tb07": 1, "ambuj": 1, "tewari": 1, "peter": 1, "l": 1, "bartlett": 1}, {"bound": 1, "parameter": 1, "markov": 1, "decision": 1, "process": 1, "average": 1, "reward": 1, "criterion": 1}, {"international": 1, "conference": 1, "computational": 1, "learn": 1, "theory": 1, "colt": 1}, {"springer": 1}, {"2007": 1, "pp": 1}, {"263277": 1}, {"wce03": 1, "eric": 1, "wiewiora": 1, "garrison": 1, "w": 1, "cottrell": 1, "charles": 1, "elkan": 1}, {"principled": 1, "methods": 1, "advise": 1, "reinforcement": 1, "learn": 1, "agents": 1}, {"proceed": 1, "international": 1, "conference": 1, "machine": 1, "learn": 1, "icml": 1}, {"2003": 1, "pp": 1}, {"792799": 1}, {"wie03": 1, "eric": 1, "wiewiora": 1}, {"potentialbased": 1, "shape": 1, "qvalue": 1, "initialization": 1, "equivalent": 1}, {"journal": 1, "artificial": 1, "intelligence": 1, "research": 1, "19": 1, "2003": 1, "pp": 1}, {"205208": 1}, {"9": 1}]
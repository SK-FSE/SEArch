[{"policy": 1, "continuation": 1, "hindsight": 1, "inverse": 1, "dynamics": 1, "": 6, "hao": 1, "sun1": 1, "zhizhong": 1, "li1": 1, "xiaotong": 1, "liu2": 1, "dahua": 1, "lin1": 1, "bolei": 1, "zhou1": 1, "1": 1, "chinese": 1, "university": 2, "hong": 1, "kong": 1, "2": 1, "peking": 1, "abstract": 1, "solve": 1, "goaloriented": 1, "task": 1, "important": 1, "challenge": 1, "problem": 1, "reinforcement": 1, "learn": 1, "rl": 1}, {"task": 1, "reward": 1, "often": 1, "sparse": 1, "make": 1, "difficult": 1, "learn": 1, "policy": 1, "effectively": 1}, {"tackle": 1, "difficulty": 1, "propose": 1, "new": 1, "approach": 1, "call": 1, "policy": 1, "continuation": 1, "hindsight": 1, "inverse": 1, "dynamics": 1, "pchid": 1}, {"approach": 1, "learn": 1, "hindsight": 2, "inverse": 1, "dynamics": 1, "base": 1, "experience": 1, "replay": 1}, {"enable": 1, "learn": 2, "process": 1, "selfimitated": 1, "manner": 1, "thus": 1, "train": 1, "supervise": 1}, {"work": 1, "also": 1, "extend": 1, "multistep": 1, "settings": 1, "policy": 1, "continuation": 1}, {"propose": 1, "method": 1, "general": 1, "work": 1, "isolation": 1, "combine": 1, "onpolicy": 1, "offpolicy": 1, "algorithms": 1}, {"two": 1, "multigoal": 1, "task": 1, "gridworld": 1, "fetchreach": 1, "pchid": 1, "significantly": 1, "improve": 1, "sample": 1, "efficiency": 1, "well": 1, "final": 1, "performance1": 1, "": 1}, {"1": 1, "": 2, "introduction": 1, "imagine": 1, "give": 1, "task": 1, "tower": 1, "hanoi": 1, "ten": 1, "disk": 1, "would": 1, "probably": 1, "solve": 1, "complex": 1, "problem": 1}, {"game": 1, "look": 1, "daunt": 1, "first": 1, "glance": 1}, {"however": 1, "trials": 1, "errors": 1, "one": 3, "may": 1, "discover": 1, "key": 1, "recursively": 1, "relocate": 1, "disk": 1, "top": 1, "stack": 1, "pod": 1, "another": 1, "assist": 1, "intermediate": 1}, {"case": 1, "actually": 1, "learn": 2, "skills": 2, "easier": 1, "subtasks": 1, "help": 1}, {"case": 1, "exemplify": 1, "procedure": 1, "selfimitated": 1, "curriculum": 1, "learn": 1, "recursively": 1, "develop": 1, "skills": 1, "solve": 1, "complex": 1, "problems": 1}, {"tower": 1, "hanoi": 1, "belong": 1, "important": 1, "kind": 1, "challenge": 1, "problems": 1, "reinforcement": 1, "learn": 1, "rl": 1, "namely": 1, "solve": 1, "goaloriented": 1, "task": 1}, {"task": 1, "reward": 1, "usually": 1, "sparse": 1}, {"example": 1, "many": 1, "goaloriented": 1, "task": 2, "single": 1, "binary": 1, "reward": 1, "provide": 1, "complete": 1, "1": 1, "2": 1, "3": 1}, {"previous": 1, "work": 1, "attribute": 1, "difficulty": 1, "sparse": 1, "reward": 1, "problems": 1, "low": 1, "efficiency": 1, "experience": 1, "collection": 1, "4": 1}, {"thus": 1, "many": 1, "approach": 1, "propose": 1, "tackle": 1, "problem": 1, "include": 1, "automatic": 1, "goal": 1, "generation": 1, "5": 1, "selfimitation": 1, "learn": 3, "6": 1, "hierarchical": 1, "reinforcement": 1, "7": 1, "curiosity": 1, "drive": 1, "methods": 1, "8": 1, "9": 1, "curriculum": 1, "1": 1, "10": 1, "hindsight": 1, "experience": 1, "replay": 1, "11": 1}, {"work": 1, "guide": 1, "agent": 1, "demonstrate": 1, "successful": 1, "choices": 1, "base": 1, "sufficient": 1, "exploration": 1, "improve": 1, "learn": 1, "efficiency": 1}, {"differently": 1, "open": 1, "new": 1, "way": 1, "learn": 1, "failures": 1, "assign": 1, "hindsight": 1, "credit": 1, "primal": 1, "experience": 1}, {"however": 1, "limit": 1, "applicable": 1, "combine": 1, "offpolicy": 1, "algorithms3": 1}, {"paper": 1, "propose": 1, "approach": 1, "goaloriented": 1, "rl": 1, "call": 1, "policy": 1, "continuation": 1, "hindsight": 1, "inverse": 1, "dynamics": 1, "pchid": 1, "leverage": 1, "key": 1, "idea": 1, "selfimitate": 1, "learn": 1}, {"contrast": 1, "method": 1, "work": 1, "auxiliary": 1, "module": 1, "onpolicy": 1, "offpolicy": 1, "algorithms": 1, "isolate": 1, "controller": 1}, {"moreover": 1, "learn": 1, "predict": 1, "action": 1, "directly": 1, "backpropagation": 1, "selfimitation": 1, "12": 1, "instead": 1, "temporal": 1, "difference": 1, "13": 1, "policy": 1, "gradient": 1, "14": 1, "15": 1, "16": 1, "17": 1, "data": 1, "efficiency": 1, "greatly": 1, "improve": 1}, {"contributions": 1, "work": 1, "lie": 1, "three": 1, "aspects": 1, "1": 1, "introduce": 1, "stategoal": 1, "space": 1, "partition": 1, "multigoal": 1, "rl": 1, "thereon": 1, "define": 1, "policy": 1, "continuation": 1, "pc": 1, "new": 1, "approach": 1, "task": 1}, {"1": 1, "": 2, "code": 1, "relate": 1, "materials": 1, "available": 1, "httpssitesgooglecomviewneurips2019pchid": 1, "33rd": 1, "conference": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "neurips": 1, "2019": 1, "vancouver": 1, "canada": 1}, {"2": 1, "propose": 1, "hindsight": 1, "inverse": 2, "dynamics": 2, "hide": 1, "extend": 1, "vanilla": 1, "method": 1, "goaloriented": 1, "set": 1}, {"3": 1, "integrate": 1, "pc": 1, "hide": 1, "pchid": 1, "effectively": 1, "leverage": 1, "selfsupervised": 1, "learn": 2, "accelerate": 1, "process": 1, "reinforcement": 1}, {"note": 1, "pchid": 1, "general": 1, "method": 1}, {"onpolicy": 1, "offpolicy": 1, "algorithms": 1, "benefit": 1, "therefrom": 1}, {"test": 1, "method": 1, "challenge": 1, "rl": 1, "problems": 1, "achieve": 1, "considerably": 1, "higher": 1, "sample": 1, "efficiency": 1, "performance": 1}, {"2": 1, "": 2, "relate": 1, "work": 1, "hindsight": 1, "experience": 1, "replay": 1, "learn": 1, "sparse": 1, "reward": 2, "rl": 1, "problems": 1, "always": 1, "lead": 1, "challenge": 1, "usually": 1, "uneasy": 1, "reach": 1, "random": 1, "explorations": 1}, {"hindsight": 1, "experience": 1, "replay": 1, "relabels": 1, "fail": 1, "rollouts": 1, "successful": 1, "ones": 1, "propose": 1, "andrychowicz": 1, "et": 1, "al": 1}, {"11": 1, "method": 1, "deal": 1, "problem": 1}, {"agent": 1, "receive": 1, "reward": 1, "reach": 1, "either": 1, "original": 2, "goal": 2, "relabeled": 2, "episode": 1, "store": 1, "transition": 2, "pair": 1, "st": 2, "": 6, "g": 2, "r": 2, "replay": 1, "buffer": 1}, {"later": 1, "extend": 1, "work": 1, "demonstration": 1, "data": 1, "4": 1, "boost": 1, "multiprocessing": 1, "train": 1, "3": 1}, {"work": 1, "rauber": 1, "et": 1, "al": 1}, {"18": 1, "extend": 1, "hindsight": 1, "knowledge": 1, "policy": 1, "gradient": 1, "methods": 1, "use": 1, "importance": 1, "sample": 1}, {"inverse": 2, "dynamics": 2, "give": 1, "state": 1, "transition": 1, "pair": 1, "st": 2, "": 6, "st1": 2, "19": 1, "take": 1, "input": 1, "output": 1, "correspond": 1, "action": 1}, {"previous": 1, "work": 1, "use": 1, "inverse": 1, "dynamics": 1, "perform": 1, "feature": 1, "extraction": 1, "20": 1, "9": 1, "21": 1, "policy": 1, "network": 1, "optimization": 1}, {"action": 1, "store": 1, "transition": 1, "pair": 1, "always": 1, "collect": 1, "random": 1, "policy": 2, "barely": 1, "use": 1, "optimize": 1, "network": 1, "directly": 1}, {"work": 1, "use": 1, "hindsight": 2, "experience": 1, "revise": 1, "original": 1, "transition": 1, "pair": 1, "inverse": 2, "dynamics": 2, "call": 1, "approach": 1}, {"detail": 1, "elucidate": 1, "next": 1, "section": 1}, {"auxiliary": 1, "task": 1, "curiosity": 1, "drive": 1, "method": 1, "mirowski": 1, "et": 1, "al": 1}, {"22": 1, "propose": 1, "jointly": 1, "learn": 2, "goaldriven": 1, "reinforcement": 1, "problems": 1, "unsupervised": 1, "depth": 1, "prediction": 1, "task": 3, "selfsupervised": 1, "loop": 1, "closure": 1, "classification": 1, "achieve": 1, "data": 1, "efficiency": 1, "performance": 1, "improvement": 1}, {"method": 1, "require": 1, "extra": 1, "supervision": 1, "like": 1, "depth": 1, "input": 1}, {"shelhamer": 1, "et": 1, "al": 1}, {"21": 1, "introduce": 1, "several": 1, "selfsupervised": 1, "auxiliary": 1, "task": 1, "perform": 1, "feature": 2, "extraction": 1, "adopt": 1, "learn": 3, "reinforcement": 1, "improve": 1, "data": 1, "efficiency": 1, "return": 1, "endtoend": 1}, {"pathak": 1, "et": 1, "al": 1}, {"20": 1, "propose": 1, "learn": 2, "intrinsic": 1, "curiosity": 1, "reward": 2, "besides": 1, "normal": 1, "extrinsic": 1, "formulate": 1, "prediction": 1, "error": 1, "visual": 1, "feature": 1, "space": 1, "improve": 1, "efficiency": 1}, {"approach": 1, "belong": 1, "selfsupervision": 1, "utilize": 1, "inverse": 1, "dynamics": 1, "train": 1}, {"although": 1, "method": 1, "use": 1, "auxiliary": 1, "task": 1, "train": 2, "selfsupervised": 2, "way": 1, "improve": 1, "vanilla": 1, "inverse": 1, "dynamics": 1, "hindsight": 1, "enable": 1, "direct": 1, "joint": 1, "policy": 1, "network": 1, "temporal": 1, "difference": 1, "learn": 1}, {"3": 1, "": 2, "policy": 1, "continuation": 1, "hindsight": 1, "inverse": 1, "dynamics": 1, "section": 1, "first": 1, "briefly": 1, "go": 1, "preliminaries": 1, "sec31": 1}, {"sec32": 1, "retrospect": 1, "toy": 1, "example": 2, "introduce": 1, "motivate": 1}, {"sec33": 1, "36": 1, "describe": 1, "method": 1, "detail": 1}, {"31": 1, "": 6, "preliminaries": 1, "markov": 2, "decision": 2, "process": 2, "consider": 1, "mdp": 1, "denote": 1, "tuple": 1, "p": 2, "r": 1, "finite": 1, "state": 1, "action": 1, "space": 1, "describe": 1, "transition": 1, "probability": 1, "0": 1, "1": 1}, {"r": 2, "": 4, "reward": 1, "function": 1, "0": 1, "1": 1, "discount": 1, "factor": 1}, {"p": 1, "": 17, "0": 1, "1": 1, "denote": 1, "policy": 2, "optimal": 1, "satisfy": 1, "arg": 1, "max": 1, "esa": 1, "t0": 1, "rst": 1, "st": 2, "st1": 1, "pst1": 1, "s0": 1, "give": 1, "pan": 1, "start": 1, "state": 1}, {"transition": 2, "policy": 1, "deterministic": 3, "": 14, "arg": 1, "max": 1, "es0": 1, "t0": 1, "rst": 1, "st": 2, "st1": 1, "model": 1, "dynamics": 1}, {"expectation": 1, "possible": 1, "start": 1, "state": 1}, {"universal": 2, "value": 2, "function": 2, "approximators": 1, "multigoal": 1, "rl": 1, "approximator": 1, "uvfa": 1, "23": 1, "extend": 1, "state": 1, "space": 1, "deep": 1, "qnetworks": 1, "dqn": 1, "24": 1, "include": 1, "goal": 1, "2": 1, "": 5, "combine": 1, "inverse": 1, "dynamics": 1}, {"6": 1, "": 2}, {"reward": 1, "": 16, "8": 1, "10": 1, "010id": 1, "005id": 1, "003id": 1, "001id": 1, "id": 1, "12": 1, "14": 1, "0": 2, "20": 1, "40": 1, "60": 1, "number": 1, "frames1e4": 1, "80": 1, "1": 1}, {"100": 1, "": 2, "21": 1, "figure": 1, "1": 1, "result": 1, "bitflipping": 1, "problem": 1}, {"b": 1, "illustration": 1, "flat": 1, "state": 1, "space": 1}, {"c": 1, "example": 1, "gridworld": 1, "domain": 1, "nonflat": 1, "case": 1}, {"state": 1, "g": 5, "": 9, "part": 1, "input": 1, "ie": 1, "st": 2, "extend": 1, "policy": 1, "become": 1, "pretty": 1, "useful": 1, "set": 1, "multiple": 1, "goals": 1, "achieve": 1}, {"moreover": 1, "schaul": 1, "etal": 1}, {"23": 1, "show": 1, "set": 1, "learn": 1, "policy": 1, "generalize": 1, "previous": 1, "unseen": 1, "stategoal": 1, "pair": 1}, {"application": 1, "uvfa": 1, "proximal": 1, "policy": 1, "optimization": 1, "algorithm": 1, "ppo": 1, "25": 1, "straightforward": 1}, {"follow": 1, "work": 1, "use": 1, "stategoal": 1, "pair": 1, "denote": 1, "extend": 1, "state": 1, "space": 1, "g": 4, "": 8, "st": 2, "st1": 1}, {"goal": 1, "g": 1, "fix": 1, "within": 1, "episode": 1, "change": 1, "across": 1, "different": 1, "episodes": 1}, {"32": 1, "": 5, "revisit": 1, "bitflipping": 2, "problem": 2, "provide": 1, "motivate": 1, "example": 1, "11": 1, "n": 2, "bits": 1, "state": 1, "space": 2, "0": 3, "1n": 1, "action": 1, "1": 2}, {"action": 1, "correspond": 1, "turn": 1, "ath": 1, "bite": 1, "state": 1}, {"episode": 1, "start": 1, "randomly": 1, "generate": 1, "state": 3, "s0": 1, "random": 1, "goal": 2, "g": 2, "reach": 1, "agent": 1, "receive": 1, "reward": 1}, {"propose": 1, "relabel": 1, "fail": 1, "trajectories": 1, "receive": 1, "reward": 1, "signal": 1, "thus": 1, "enable": 1, "policy": 1, "learn": 1, "failures": 1}, {"however": 1, "method": 1, "base": 1, "temporal": 1, "difference": 1, "thus": 1, "efficiency": 1, "data": 1, "limit": 1}, {"learn": 3, "failures": 1, "come": 1, "question": 1, "policy": 1, "supervise": 1, "data": 1, "generate": 1, "use": 1, "hindsight": 1, "experience": 1}, {"inspire": 1, "selfimitate": 1, "learn": 2, "ability": 1, "human": 1, "aim": 1, "employ": 1, "selfimitation": 1, "get": 1, "success": 1, "rl": 1, "even": 1, "original": 1, "goal": 1, "yet": 1, "achieve": 1}, {"straightforward": 1, "way": 1, "utilize": 1, "selfimitate": 1, "learn": 1, "adopt": 1, "inverse": 1, "dynamics": 1}, {"however": 1, "case": 1, "action": 1, "store": 1, "inverse": 1, "dynamics": 1, "irrelevant": 1, "goals": 1}, {"specifically": 1, "transition": 1, "tuples": 1, "like": 1, "st": 1, "": 3, "g": 2, "st1": 1, "save": 1, "learn": 1, "inverse": 1, "dynamics": 1, "goaloriented": 1, "task": 1}, {"learn": 1, "process": 1, "execute": 1, "simply": 1, "classification": 1, "action": 2, "space": 2, "discrete": 1, "regression": 1, "continuous": 1}, {"give": 1, "neural": 1, "network": 1, "parameterized": 1, "": 5, "objective": 1, "learn": 1, "inverse": 1, "dynamics": 1, "follow": 1, "x": 1, "f": 1, "st": 1, "g": 2, "st1": 1, "2": 1}, {"1": 1, "": 5, "arg": 1, "min": 1, "st": 1, "st1": 1, "due": 1, "unawareness": 1, "goals": 2, "agent": 1, "take": 1, "action": 1, "g": 1, "eq": 1}, {"1": 1, "placeholders": 1}, {"thus": 1, "cost": 1, "nothing": 1, "replace": 1, "g": 2, "": 3, "mst1": 1, "result": 1, "meaningful": 1, "form": 1, "ie": 1, "encode": 1, "follow": 1, "state": 1, "hindsight": 1, "goal": 1}, {"say": 1, "agent": 1, "want": 1, "reach": 1, "g": 1, "": 3, "st": 1, "take": 1, "action": 1, "thus": 1, "decision": 1, "make": 1, "process": 1, "aware": 1, "hindsight": 1, "goal": 1}, {"adopt": 1, "f": 1, "train": 1, "eq": 1}, {"1": 1, "additional": 1, "module": 1, "incorporate": 1, "bitflipping": 1, "environment": 1, "simply": 1, "add": 1, "logit": 1, "output": 1}, {"show": 1, "fig1a": 1, "additional": 1, "module": 1, "lead": 1, "significant": 1, "improvement": 1}, {"attribute": 1, "success": 1, "flatness": 1, "state": 1, "space": 1}, {"fig1b": 1, "illustrate": 1, "flatness": 1, "case": 1, "agent": 2, "grid": 1, "map": 1, "require": 1, "reach": 3, "goal": 1, "g3": 2, "start": 1, "s0": 1, "": 1, "already": 1, "know": 1, "s1": 1, "east": 2, "intuitively": 1, "problem": 1, "extrapolate": 1, "policy": 1, "farther": 1}, {"nevertheless": 1, "success": 1, "always": 1, "within": 1, "effortless": 1, "single": 1, "step": 1, "reach": 1}, {"reach": 1, "goals": 1, "g1": 1, "g2": 1, "relatively": 1, "harder": 1, "task": 1, "navigate": 1, "start": 1, "point": 2, "goal": 1, "gridworld": 1, "domain": 1, "show": 1, "fig1c": 1, "even": 1, "challenge": 1}, {"employ": 1, "selfimitate": 1, "learn": 1, "overcome": 1, "single": 1, "step": 1, "limitation": 1, "inverse": 2, "dynamics": 2, "come": 1, "new": 1, "approach": 1, "call": 1, "policy": 1, "continuation": 1, "hindsight": 1}, {"3": 1, "": 3, "33": 1, "perspective": 1, "policy": 2, "continuation": 2, "multigoal": 1, "rl": 1, "task": 1, "approach": 1, "mainly": 1, "base": 1, "subpolicies": 1, "view": 1, "emendation": 1, "spontaneous": 1, "extrapolation": 1, "bitflipping": 1, "case": 1}, {"definition": 2, "1": 3, "policy": 7, "continuationpc": 1, "suppose": 1, "": 35, "function": 3, "define": 2, "nonempty": 1, "substatespace": 1, "su": 6, "state": 2, "space": 1, "ie": 3, "sv": 3, "larger": 1, "subset": 1, "contain": 1, "call": 2, "continuation": 1, "say": 1, "restriction": 1, "denote": 1, "optimal": 2, "st": 1, "gt": 1, "introduce": 1, "concept": 1, "kstep": 4, "solvability": 3, "2": 1, "give": 1, "stategoal": 1, "pair": 2, "g": 6, "task": 1, "certain": 1, "system": 1, "deterministic": 1, "dynamics": 1, "reach": 1, "goal": 1, "need": 1, "least": 1, "k": 2, "step": 1, "start": 2, "s0": 1, "execute": 1, "ai": 1, "si": 1, "0": 1, "sk": 1, "sk1": 1, "ak1": 1, "satisfy": 1, "msk": 1, "solvable": 1}, {"ideally": 1, "kstep": 1, "solvability": 1, "mean": 1, "number": 1, "step": 1, "take": 1, "g": 1, "give": 1, "maximum": 1, "permit": 1, "action": 1, "value": 1}, {"practice": 1, "kstep": 1, "solvability": 1, "evolve": 1, "concept": 1, "gradually": 1, "change": 1, "learn": 1, "process": 1, "thus": 1, "define": 1, "whether": 1, "solve": 1, "k1": 2, "within": 1, "k": 1, "step": 1, "convergence": 1, "train": 1, "k1step": 1, "hids": 1}, {"follow": 1, "assume": 1, "map": 1, "": 2, "g": 1, "st": 1}, {"": 3, "reward": 1, "function": 1, "rs": 1, "ms": 1, "1": 1, "thus": 1, "information": 1, "goal": 2, "g": 3, "encode": 1, "state": 2, "simplest": 1, "case": 1, "identical": 1, "map": 1, "consider": 1, "certain": 1, "system": 1}, {"follow": 1, "idea": 1, "recursion": 1, "curriculum": 1, "learn": 1, "divide": 1, "finite": 2, "stategoal": 3, "space": 1, "": 28, "2": 4, "part": 1, "accord": 1, "kstep": 2, "solvability": 1, "g": 6, "g0": 2, "g1": 1, "gt": 1, "gu": 2, "timestep": 1, "horizon": 1, "suppose": 1, "task": 1, "solve": 1, "within": 1, "gi": 1, "0": 2, "1": 2, "denote": 2, "set": 1, "istep": 1, "solvable": 2, "pair": 2, "unsolvable": 1, "ie": 1, "k": 1, "trivial": 1, "case": 1, "ms0": 1}, {"optimal": 1, "policy": 1, "aim": 1, "solve": 1, "solvable": 1, "stategoal": 1, "pair": 1, "take": 1, "": 1, "gu": 1, "consideration": 1}, {"clear": 1, "define": 3, "disjoint": 1, "substategoal": 3, "space": 6, "union": 1, "solvable": 4, "stategoal": 4, "pair": 2, "definition": 3, "3": 2, "partition": 2, "give": 1, "certain": 1, "environment": 1, "categorize": 1, "one": 1, "sub": 3, "follow": 2, "": 17, "gs": 1, "gu": 1, "gj": 2, "j0": 2, "set": 1, "subpolicies": 1, "0": 1, "1": 1, "2": 1, "si": 1, "respectively": 1, "4": 1, "policy": 1, "subpolicy": 1, "gi": 1}, {"say": 1, "optimal": 1, "subpolicy": 1, "able": 1, "solve": 1, "istep": 1, "solvable": 1, "stategoal": 1, "pair": 1, "task": 1, "step": 1}, {"": 11, "corollary": 1, "1": 2, "restrict": 1, "policy": 5, "continuation": 2, "i1": 1, "2": 1, "k": 2, "able": 1, "solve": 1, "istep": 1, "solvable": 1, "problem": 1, "definition": 1, "optimal": 2, "sub": 1, "already": 1, "substitute": 1}, {"recursively": 1, "approximate": 1, "": 3, "expand": 1, "domain": 1, "substategoal": 1, "space": 1, "policy": 1, "continuation": 1, "optimal": 1, "subpolicy": 1, "0": 1}, {"practice": 1, "use": 1, "neural": 1, "network": 1, "approximate": 1, "4": 1, "": 1, "subpolicies": 1, "policy": 1, "continuation": 1}, {"propose": 1, "parameterize": 1, "policy": 1, "function": 1, "": 3, "f": 2, "neural": 1, "network": 1, "optimize": 2, "selfsupervised": 1, "learn": 1, "data": 1, "collect": 1, "hindsight": 1, "inverse": 1, "dynamics": 1, "hide": 1, "recursively": 1, "joint": 1, "optimization": 1}, {"34": 1, "": 2, "hindsight": 2, "inverse": 2, "dynamics": 2, "onestep": 1, "one": 1, "step": 1, "hide": 1, "data": 1, "collect": 1, "easily": 1}, {"n": 3, "randomly": 1, "rollout": 1, "trajectories": 1, "s0": 2, "": 26, "g": 5, "a0": 2, "r0": 1, "s1": 2, "a1": 2, "st": 3, "rt": 1, "1": 4, "2": 2, "use": 1, "modify": 1, "inverse": 1, "dynamics": 1, "substitute": 1, "original": 1, "goal": 2, "hindsight": 1, "mst1": 1, "every": 1, "result": 1, "ms1": 1, "ms2": 1, "mst": 1}, {"fit": 1, "f1": 1, "1": 1, "": 12, "arg": 1, "min": 1, "x": 1, "f": 2, "st": 2, "mst1": 2, "st1": 2, "2": 1, "4": 1, "collect": 1, "enough": 1, "trajectories": 1, "optimize": 1, "implement": 1, "neural": 1, "network": 1, "stochastic": 1, "gradient": 1, "descent": 1, "26": 1}, {"identical": 1, "map": 1, "function": 1, "f1": 1, "good": 1, "enough": 1, "approximator": 1, "1": 1, "": 1, "guarantee": 1, "approximation": 1, "ability": 1, "neural": 1, "network": 1, "27": 1, "28": 1, "29": 1}, {"p": 1, "otherwise": 1, "adapt": 1, "eq": 1}, {"4": 1, "1": 1, "": 5, "arg": 1, "min": 1, "st": 2, "st1": 2, "f": 1, "mst1": 2, "2": 1, "ie": 1, "omit": 1, "state": 2, "information": 1, "future": 1, "regard": 1, "f1": 1, "policy": 1}, {"p": 1, "practice": 1, "become": 1, "1": 1, "": 5, "arg": 1, "min": 1, "st": 2, "st1": 1, "f": 1, "mst1": 1, "2": 1}, {"": 3, "kmultistep": 1, "hindsight": 1, "inverse": 1, "dynamics": 1, "fk1": 1, "approximator": 1, "k1": 1, "step": 1, "hide": 1, "ready": 1, "get": 1}, {"collect": 1, "valid": 1, "kstep": 4, "hide": 2, "data": 1, "recursively": 1, "test": 1, "whether": 1, "stategoal": 2, "pair": 3, "indeed": 1, "need": 1, "k": 5, "step": 2, "solve": 1, "ie": 1, "transition": 1, "": 17, "st": 4, "g": 2, "rt": 1, "stk": 2, "atk": 1, "rtk": 1, "policy": 1, "k1": 1, "hand": 1, "provide": 1, "another": 1, "solution": 1, "mstk": 4, "less": 1, "must": 1, "solvable": 1, "together": 1, "action": 1, "mark": 1}, {"fig2": 1, "illustrate": 1, "process": 1}, {"test": 3, "process": 1, "base": 1, "function": 1, "focus": 1, "selection": 1, "sec36": 1}, {"transition": 1, "pair": 1, "like": 1, "collect": 1, "optimize": 1, "k": 1, "": 1}, {"practice": 1, "leverage": 1, "joint": 1, "train": 1, "ensure": 1, "fk": 1, "policy": 2, "continuation": 1, "": 18, "1": 1, "k": 2, "ie": 1, "arg": 1, "min": 1, "35": 1, "x": 1, "f": 1, "st": 2, "msti": 2, "sti": 2, "2": 1, "5": 1, "i1k": 1, "dynamic": 1, "program": 1, "formulation": 1, "goaloriented": 1, "task": 1, "learn": 1, "objective": 1, "find": 1, "reach": 1, "goal": 1, "soon": 1, "possible": 1}, {"circumstances": 1, "l": 6, "st": 4, "": 53, "g": 9, "st1": 4, "1": 5, "6": 1, "define": 1, "number": 1, "step": 1, "execute": 1, "policy": 1, "additional": 1, "7": 1, "arg": 1, "min": 1, "learn": 1, "process": 1, "impossible": 1, "enumerate": 1, "possible": 1, "intermediate": 1, "state": 2, "continuous": 1, "space": 1, "suppose": 1, "optimal": 1, "subpolicy": 1, "k1": 1, "istep": 1, "solvable": 1, "problems": 1, "k": 1, "lk": 1, "lk1": 1, "5": 1, "8": 1, "less": 1}, {"": 4, "less": 1}, {"": 5, "less": 1}, {"0": 2, "": 6}, {"": 1}, {"figure": 1, "2": 1, "test": 1, "whether": 1, "transition": 1, "2step": 1, "leave": 1, "kstep": 1, "right": 1, "solvable": 1}, {"test": 1, "function": 1, "return": 1, "true": 1, "transition": 1, "st": 1, "": 1, "stk": 1, "need": 1, "least": 1, "k": 1, "step": 1}, {"algorithm": 1, "1": 8, "policy": 4, "continuation": 1, "hindsight": 1, "inverse": 1, "dynamics": 1, "pchid": 2, "require": 1, "b": 8, "g": 13, "reward": 1, "function": 1, "rs": 1, "equal": 1, "": 39, "ms": 1, "else": 1, "0": 3, "buffer": 3, "b1": 1, "b2": 1, "bt": 1, "list": 1, "k": 5, "initialize": 1, "episode": 2, "generate": 1, "s0": 1, "system": 1, "select": 1, "action": 2, "behavior": 2, "st": 6, "execute": 1, "get": 1, "next": 1, "state": 1, "st1": 2, "store": 2, "transition": 1, "temporary": 1, "end": 6, "calculate": 1, "additional": 1, "goal": 1, "accord": 1, "stk": 1, "mstk": 1, "testk": 1, "true": 1, "bk": 1, "sample": 1, "minibatch": 1, "optimize": 1, "predict": 1, "supervise": 1, "learn": 1, "converge": 1, "add": 1, "maxk": 1, "hold": 1, "gk": 1}, {"sample": 1, "trajectories": 1, "random": 1, "rollout": 1, "unbiased": 1, "policies": 1, "choose": 1, "feasible": 1, "st": 6, "": 16, "g": 1, "pair": 2, "ie": 1, "stk": 3, "trajectory": 1, "solve": 1, "k1": 1, "k": 1, "st1": 3, "9": 1, "recursive": 1, "approach": 1, "start": 1, "1": 1, "easily": 1, "approximate": 1, "train": 1, "self": 1, "supervise": 1, "learn": 1, "give": 1, "g1": 1, "definition": 1}, {"combination": 1, "pc": 1, "multistep": 1, "hide": 1, "lead": 1, "algorithm": 1, "pchid": 1}, {"pchid": 1, "work": 1, "alone": 1, "auxiliary": 1, "module": 1, "rl": 1, "algorithms": 1}, {"discuss": 1, "three": 1, "different": 1, "combination": 1, "methods": 1, "pchid": 1, "algorithms": 1, "sec43": 1}, {"full": 1, "algorithm": 2, "pchid": 1, "present": 1, "1": 1}, {"36": 1, "": 5, "selection": 1, "test": 2, "function": 1, "algorithm": 1, "1": 1, "crucial": 1, "step": 1, "extend": 1, "k": 1, "1step": 1, "sub": 2, "policy": 2, "kstep": 3, "whether": 1, "transition": 1, "st": 2, "stk": 1, "trajectory": 1, "indeed": 1, "solvable": 1, "problem": 1, "regard": 1, "start": 1, "state": 1, "s0": 1, "mstk": 1, "goal": 1, "g": 1, "propose": 1, "two": 1, "approach": 1, "evaluate": 1, "sec4": 1}, {"interaction": 1, "straightforward": 1, "idea": 1, "reset": 1, "environment": 1, "st": 1, "execute": 1, "action": 1, "policy": 1, "k1": 1, "": 4, "follow": 1, "execution": 1, "at1": 1, "at2": 1, "record": 1, "achieve": 1, "goal": 1, "less": 1, "k": 1, "step": 1}, {"6": 1, "": 1, "call": 1, "approach": 1, "interaction": 1, "require": 1, "environment": 2, "resettable": 1, "interact": 1}, {"approach": 1, "portable": 1, "transition": 1, "dynamics": 1, "know": 1, "approximate": 1, "without": 1, "heavy": 1, "computation": 1, "expense": 1}, {"random": 1, "network": 3, "distillation": 1, "rnd": 2, "give": 1, "state": 2, "input": 1, "30": 1, "propose": 1, "provide": 1, "exploration": 1, "bonus": 1, "compare": 1, "output": 2, "difference": 2, "fix": 1, "randomly": 1, "initialize": 1, "neural": 2, "na": 2, "another": 1, "nb": 2, "": 1, "train": 1, "minimize": 1, "previous": 1}, {"train": 1, "nb": 3, "1": 2, "2": 1, "": 3, "k": 1, "step": 1, "transition": 2, "pair": 3, "minimize": 1, "output": 2, "difference": 1, "na": 1, "since": 1, "never": 1, "see": 1, "kstep": 1, "solvable": 1, "differentiate": 1, "lead": 1, "larger": 1, "differences": 1}, {"37": 1, "": 2, "synchronous": 1, "improvement": 1, "pchid": 1, "learn": 3, "scheme": 1, "set": 1, "curriculum": 1, "ie": 1, "agent": 1, "must": 1, "master": 1, "easy": 1, "skills": 1, "complex": 1, "ones": 1}, {"however": 1, "general": 1, "efficiency": 1, "find": 1, "transition": 1, "sequence": 1, "istep": 1, "solvable": 1, "decrease": 1, "increase": 1}, {"size": 1, "buffer": 1, "bi": 1, "thus": 1, "decrease": 1, "": 2, "1": 1, "2": 1, "3": 1, "learn": 1, "might": 1, "restrict": 1, "due": 1, "limit": 1, "experience": 1}, {"besides": 1, "continuous": 1, "control": 1, "task": 1, "kstep": 1, "solvability": 1, "mean": 1, "number": 1, "step": 1, "take": 1, "g": 1, "give": 1, "maximum": 1, "permit": 1, "action": 1, "value": 1}, {"practice": 1, "kstep": 1, "solvability": 1, "treat": 1, "evolve": 1, "concept": 1, "change": 1, "gradually": 1, "learn": 1, "go": 1}, {"specifically": 1, "begin": 1, "agent": 1, "walk": 1, "small": 1, "pace": 1, "learn": 1, "experience": 1, "collect": 1, "random": 1, "movements": 1}, {"train": 1, "continue": 1, "agent": 1, "confident": 1, "move": 1, "larger": 1, "pace": 1, "may": 1, "change": 1, "distribution": 1, "select": 1, "action": 1}, {"consequently": 1, "previous": 1, "kstep": 1, "solvable": 1, "state": 1, "goal": 1, "pair": 1, "may": 1, "solve": 1, "less": 1, "k": 1, "step": 1}, {"base": 1, "efficiency": 1, "limitation": 1, "progressive": 1, "definition": 1, "kstep": 1, "solvatbility": 1, "propose": 1, "synchronous": 1, "version": 1, "pchid": 1}, {"readers": 1, "please": 1, "refer": 1, "supplementary": 1, "material": 1, "detail": 1, "discussion": 1, "intuitive": 1, "interpretation": 1, "empirical": 1, "result": 1}, {"4": 1, "": 4, "experiment": 1, "policy": 1, "g": 2, "aim": 1, "reach": 1, "state": 1, "ms": 1, "intuition": 1, "difficulty": 1, "solve": 1, "goaloriented": 1, "task": 1, "depend": 1, "complexity": 1, "sec41": 1, "start": 1, "simple": 1, "case": 1, "identical": 1, "map": 2, "environment": 1, "gridworld": 1, "show": 1, "agent": 1, "fully": 1, "observable": 1}, {"moreover": 1, "gridworld": 1, "environment": 1, "permit": 1, "us": 1, "use": 1, "prior": 1, "knowledge": 1, "calculate": 1, "accuracy": 1, "test": 1, "function": 1}, {"show": 1, "pchid": 1, "work": 1, "independently": 1, "augment": 2, "dqn": 3, "discrete": 1, "action": 1, "space": 1, "set": 1, "outperform": 1, "well": 1}, {"gridworld": 1, "environment": 2, "correspond": 1, "identical": 1, "map": 1, "case": 1, "g": 1, "": 1, "sec42": 1, "test": 1, "method": 1, "continuous": 1, "control": 1, "problem": 1, "fetchreach": 1, "provide": 1, "plappert": 1, "et": 1, "al": 1}, {"3": 1}, {"method": 1, "outperform": 1, "ppo": 1, "achieve": 1, "100": 2, "successful": 1, "rate": 1, "episodes": 1}, {"compare": 1, "sensitivity": 1, "ppo": 1, "reward": 1, "value": 1, "robustness": 1, "pchid": 1, "own": 1}, {"stategoal": 1, "map": 2, "fetchreach": 1, "environment": 1, "g": 1, "": 3, "41": 1, "gridworld": 2, "navigation": 2, "use": 1, "task": 1, "value": 1, "iteration": 1, "network": 1, "vin": 1, "31": 1, "state": 1, "information": 1, "include": 1, "position": 2, "agent": 1, "image": 1, "obstacles": 1, "goal": 1}, {"experiment": 1, "use": 1, "16": 2, "": 1, "domains": 1, "navigation": 1, "effortless": 1, "task": 1}, {"fig1c": 1, "show": 1, "example": 1, "domains": 1}, {"action": 2, "space": 1, "discrete": 1, "contain": 1, "8": 2, "lead": 1, "agent": 1, "neighbour": 1, "position": 1, "respectively": 1}, {"reward": 2, "10": 1, "provide": 1, "agent": 2, "reach": 1, "goal": 1, "within": 1, "50": 1, "timesteps": 1, "otherwise": 1, "receive": 1, "002": 1}, {"action": 1, "lead": 1, "agent": 2, "obstacle": 1, "execute": 1, "thus": 1, "stay": 1}, {"episode": 1, "new": 1, "map": 1, "randomly": 1, "select": 1, "start": 1, "goal": 1, "g": 1, "point": 1, "generate": 1}, {"train": 1, "agent": 2, "500": 2, "episodes": 1, "total": 1, "need": 1, "learn": 1, "navigate": 1, "within": 1, "trials": 1, "much": 1, "less": 1, "number": 1, "use": 1, "vin": 1, "312": 1, "thus": 1, "demonstrate": 1, "high": 1, "data": 1, "efficiency": 1, "pchid": 1, "2": 1, "": 1, "tarmar": 1, "et": 1, "al": 1}, {"train": 1, "vin": 1, "imitation": 1, "learn": 1, "il": 1, "groundtruth": 1, "shortest": 1, "paths": 1, "start": 1, "goal": 1, "position": 1}, {"although": 1, "approach": 1, "base": 1, "il": 1, "need": 1, "groundtruth": 1, "data": 1, "": 65, "7": 1, "comparison": 1, "05": 2, "accuracy": 4, "test": 4, "function": 3, "recall": 2, "100": 5, "099": 1, "04": 3, "098": 1, "10": 2, "01": 2, "0": 4, "200": 4, "episode": 4, "300": 4, "400": 4, "500": 4, "03": 2, "02": 3, "grountruth": 1, "interaction": 3, "rnd": 4, "1": 1, "step": 1, "00": 2, "dqn": 2, "pchid1": 2, "pchid5": 1, "08": 1, "different": 2, "06": 3, "097": 1, "096": 1, "025": 2, "0275": 2, "030": 2, "095": 1, "094": 1, "figure": 1, "3": 1, "rollout": 1, "success": 1, "rate": 1, "map": 1, "experiment": 1, "random": 1, "seed": 1}, {"outperform": 1, "vin": 1, "difference": 1, "disappear": 1, "combine": 1, "pchid": 1}, {"pchid1": 1, "pchid5": 1, "represent": 1, "1step": 1, "5step": 1, "pchid": 1}, {"b": 1, "performance": 1, "pchid": 1, "module": 1, "alone": 1, "different": 1, "test": 1, "function": 1}, {"blue": 1, "line": 4, "grind": 1, "truth": 1, "test": 1, "result": 2, "orange": 1, "green": 1, "interaction": 1, "rnd": 1, "respectively": 1, "red": 1, "1step": 1, "baseline": 1}, {"cd": 1, "test": 1, "accuracy": 1, "recall": 1, "interaction": 1, "rnd": 1, "method": 1, "different": 1, "threshold": 1}, {"reward": 2, "obtain": 1, "": 40, "success": 2, "rate": 2, "10": 1, "15": 1, "ppo": 6, "20": 1, "r10": 2, "08": 1, "pchid": 6, "25": 1, "30": 1, "35": 1, "06": 1, "04": 1, "40": 1, "02": 1, "45": 1, "50": 1, "00": 1, "0": 2, "100": 2, "200": 2, "300": 2, "episode": 2, "400": 2, "500": 2, "600": 2, "figure": 1, "4": 1, "fetchreach": 1, "environment": 1}, {"b": 1, "reward": 1, "obtain": 1, "process": 1, "method": 1}, {"ppo": 1, "r10": 1, "reward": 2, "achieve": 1, "goal": 1, "become": 1, "10": 1, "instead": 1, "0": 1, "default": 1, "rescale": 1, "comparable": 1, "approach": 1}, {"show": 1, "sensitivity": 1, "ppo": 1, "reward": 1, "value": 1}, {"contrast": 1, "performance": 1, "pchid": 1, "unrelated": 1, "reward": 1, "value": 1}, {"c": 1, "success": 1, "rate": 1, "method": 1}, {"combine": 2, "ppo": 1, "pchid": 3, "bring": 1, "little": 1, "improvement": 1, "improve": 1, "performance": 1, "significantly": 1}, {"test": 1, "learn": 1, "agent": 1, "1000": 1, "unseen": 1, "map": 1}, {"work": 1, "follow": 1, "vin": 1, "use": 1, "rollout": 1, "success": 1, "rate": 1, "evaluation": 1, "metric": 1}, {"empirical": 1, "result": 1, "show": 1, "fig3": 1}, {"method": 1, "compare": 1, "dqn": 1, "equip": 1, "vin": 1, "policy": 1, "network": 1}, {"also": 1, "apply": 1, "dqn": 1, "result": 1, "little": 1, "improvement": 1}, {"pc": 2, "1step": 1, "hide": 2, "denote": 2, "pchid": 2, "1": 1, "achieve": 1, "similar": 1, "accuracy": 1, "dqn": 1, "much": 2, "less": 1, "episodes": 1, "combine": 1, "5step": 1, "5": 1, "result": 1, "distinctive": 1, "improvement": 1}, {"42": 1, "": 2, "openai": 1, "fetch": 3, "env": 1, "environments": 1, "several": 1, "task": 1, "base": 1, "7dof": 1, "robotics": 1, "arm": 1, "twofingered": 1, "parallel": 1, "gripper": 1}, {"four": 1, "task": 1, "fetchreach": 1, "fetchpush": 1, "fetchslide": 1, "fetchpickandplace": 1}, {"task": 1, "state": 1, "include": 1, "cartesian": 1, "position": 2, "linear": 1, "velocity": 2, "gripper": 1, "information": 2, "well": 1, "object": 1, "present": 1}, {"goal": 1, "present": 1, "3dimentional": 1, "vector": 1, "describe": 1, "target": 1, "location": 1, "object": 1, "move": 1}, {"agent": 1, "get": 1, "reward": 1, "0": 1, "object": 1, "target": 1, "location": 1, "within": 1, "tolerance": 1, "1": 1, "otherwise": 1}, {"action": 1, "continuous": 1, "4dimentional": 1, "vector": 1, "first": 1, "three": 1, "control": 2, "movement": 1, "gripper": 2, "last": 1, "one": 1, "open": 1, "close": 1}, {"fetchreach": 2, "demonstrate": 1, "pchid": 1, "task": 1}, {"compare": 1, "pchid": 1, "ppo": 2, "base": 1}, {"work": 1, "first": 1, "extend": 1, "hindsight": 1, "knowledge": 1, "onpolicy": 1, "algorithms": 1, "3": 1}, {"fig4": 1, "show": 1, "result": 1}, {"pchid": 1, "greatly": 1, "improve": 1, "learn": 1, "efficiency": 1, "ppo": 1}, {"although": 1, "design": 1, "onpolicy": 1, "algorithms": 1, "combination": 1, "pchid": 1, "ppobased": 1, "result": 1, "best": 1, "performance": 1}, {"8": 1, "": 49, "combination": 2, "methods": 1, "average": 3, "output": 3, "different": 2, "weight": 1, "reward": 2, "obtain": 1, "15": 1, "05": 2, "20": 1, "04": 2, "03": 2, "02": 2, "joint": 2, "train": 2, "binary": 1, "ir": 2, "continuous": 1, "01": 2, "00": 2, "0": 3, "100": 3, "200": 3, "episode": 3, "300": 3, "400": 3, "500": 3, "pchid": 1, "25": 1, "01pchid": 1, "04pchid": 1, "07pchid": 1, "10pchid": 1, "13pchid": 1, "16pchid": 1, "accuracy": 3, "06": 2, "30": 1, "35": 1, "40": 1, "45": 1, "50": 1, "600": 1, "figure": 1, "5": 1, "gridworld": 1, "strategies": 1}, {"b": 1, "average": 1, "output": 1, "different": 1, "weight": 1}, {"c": 1, "obtain": 1, "reward": 1, "fetchreach": 1, "different": 1, "strategies": 1}, {"43": 1, "": 2, "comb": 1, "pchid": 2, "rl": 2, "algorithms": 4, "require": 1, "sufficient": 1, "exploration": 1, "environment": 1, "approximate": 1, "optimal": 1, "subpolicies": 1, "progressively": 1, "easily": 1, "plug": 1, "include": 1, "onpolicy": 1, "offpolicy": 1}, {"point": 1, "pchid": 1, "module": 1, "regard": 1, "extension": 1, "offpolicy": 1, "algorithms": 1}, {"put": 1, "forward": 1, "three": 1, "combination": 1, "strategies": 1, "evaluate": 1, "gridworld": 1, "fetchreach": 1, "environment": 1}, {"joint": 1, "train": 1, "first": 1, "strategy": 1, "combine": 1, "pchid": 1, "normal": 1, "rl": 1, "algorithm": 1, "adopt": 1, "share": 1, "policy": 1}, {"share": 1, "network": 1, "train": 1, "temporal": 1, "difference": 1, "learn": 2, "rl": 1, "selfsupervised": 1, "pchid": 1}, {"pchid": 1, "module": 1, "joint": 1, "train": 1, "view": 1, "regularizer": 1}, {"average": 1, "output": 1, "another": 1, "strategy": 1, "combination": 1, "train": 1, "two": 1, "policy": 1, "network": 1, "separately": 1, "data": 1, "collect": 1, "set": 1, "trajectories": 1}, {"action": 1, "space": 1, "discrete": 1, "simply": 1, "average": 1, "two": 1, "output": 1, "vectors": 1, "policy": 1, "network": 1, "eg": 1}, {"qvalue": 1, "vector": 2, "logprobability": 1, "pchid": 1}, {"action": 3, "space": 1, "continuous": 1, "average": 1, "two": 1, "predict": 1, "vectors": 1, "perform": 1, "interpolate": 1}, {"perspective": 1, "rl": 1, "agent": 1, "actually": 1, "learn": 1, "work": 1, "base": 1, "pchid": 1, "parallel": 1, "key": 1, "insight": 1, "resnet": 1, "32": 1}, {"pchid": 2, "solve": 1, "task": 1, "perfectly": 1, "rl": 1, "agent": 1, "need": 1, "follow": 1, "advice": 1}, {"otherwise": 1, "come": 1, "complex": 1, "task": 1, "pchid": 1, "provide": 1, "basic": 1, "proposals": 1, "decision": 1, "make": 1}, {"rl": 1, "agent": 1, "receive": 1, "hint": 1, "proposals": 1, "thus": 1, "learn": 1, "become": 1, "easier": 1}, {"intrinsic": 1, "reward": 1, "ir": 1, "approach": 1, "quite": 1, "similar": 1, "curiosity": 1, "drive": 1, "methods": 1}, {"instead": 1, "use": 2, "inverse": 1, "dynamics": 1, "define": 1, "curiosity": 1, "prediction": 1, "difference": 1, "pchid": 2, "module": 1, "rl": 2, "agent": 2, "intrinsic": 1, "reward": 1, "motivate": 1, "act": 1}, {"maximize": 1, "intrinsic": 1, "reward": 1, "help": 1, "rl": 1, "agent": 1, "avoid": 1, "aimless": 1, "explorations": 1, "hence": 1, "speed": 1, "learn": 1, "process": 1}, {"fig5": 1, "show": 1, "result": 1, "gridworld": 1, "fetchreach": 1, "different": 1, "combination": 1, "strategies": 1}, {"joint": 1, "train": 1, "perform": 1, "best": 1, "need": 1, "hyperparameter": 1, "tune": 1}, {"contrary": 1, "average": 1, "output": 1, "require": 2, "determine": 1, "weight": 1, "intrinsic": 1, "reward": 2, "adjust": 1, "scale": 1, "regard": 1, "external": 1}, {"5": 1, "": 2, "conclusion": 1, "work": 1, "propose": 1, "policy": 1, "continuation": 1, "hindsight": 1, "inverse": 1, "dynamics": 1, "pchid": 1, "solve": 1, "goaloriented": 1, "reward": 1, "sparse": 1, "task": 1, "new": 1, "perspective": 1}, {"experiment": 1, "show": 1, "pchid": 1, "able": 1, "improve": 1, "data": 1, "efficiency": 1, "remarkably": 1, "discrete": 1, "continuous": 1, "control": 1, "task": 1}, {"moreover": 1, "method": 1, "incorporate": 1, "onpolicy": 1, "offpolicy": 1, "rl": 1, "algorithms": 1, "flexibly": 1}, {"acknowledgement": 1, "acknowledge": 1, "discussions": 1, "yuhang": 1, "song": 1, "chuheng": 1, "zhang": 1}, {"work": 1, "partially": 1, "support": 1, "sensetime": 1, "group": 1, "cuhk": 2, "agreement": 1, "no7051699": 1, "direct": 1, "fund": 1, "no4055098": 1}, {"9": 1, "": 1, "reference": 1, "1": 1, "carlos": 1, "florensa": 1, "david": 1, "hold": 1, "markus": 1, "wulfmeier": 1, "michael": 1, "zhang": 1, "pieter": 1, "abbeel": 1}, {"reverse": 1, "curriculum": 1, "generation": 1, "reinforcement": 1, "learn": 1}, {"arxiv": 1, "preprint": 1, "arxiv170705300": 1, "2017": 1}, {"2": 1, "yan": 1, "duan": 1, "xi": 1, "chen": 1, "rein": 1, "houthooft": 1, "john": 1, "schulman": 1, "pieter": 1, "abbeel": 1}, {"benchmarking": 1, "deep": 1, "reinforcement": 1, "learn": 1, "continuous": 1, "control": 1}, {"international": 1, "conference": 1, "machine": 1, "learn": 1, "page": 1, "13291338": 1, "2016": 1}, {"3": 1, "matthias": 1, "plappert": 1, "marcin": 1, "andrychowicz": 1, "alex": 1, "ray": 1, "bob": 1, "mcgrew": 1, "bowen": 1, "baker": 1, "glenn": 1, "powell": 1, "jonas": 1, "schneider": 1, "josh": 1, "tobin": 1, "maciek": 1, "chociej": 1, "peter": 1, "welinder": 1, "et": 1, "al": 1}, {"multigoal": 1, "reinforcement": 1, "learn": 1, "challenge": 1, "robotics": 1, "environments": 1, "request": 1, "research": 1}, {"arxiv": 1, "preprint": 1, "arxiv180209464": 1, "2018": 1}, {"4": 1, "ashvin": 1, "nair": 1, "bob": 1, "mcgrew": 1, "marcin": 1, "andrychowicz": 1, "wojciech": 1, "zaremba": 1, "pieter": 1, "abbeel": 1}, {"overcome": 1, "exploration": 1, "reinforcement": 1, "learn": 1, "demonstrations": 1}, {"2018": 1, "ieee": 1, "international": 1, "conference": 1, "robotics": 1, "automation": 1, "icra": 1, "page": 1, "62926299": 1}, {"ieee": 1, "2018": 1}, {"5": 1, "david": 1, "hold": 1, "xinyang": 1, "geng": 1, "carlos": 1, "florensa": 1, "pieter": 1, "abbeel": 1}, {"automatic": 1, "goal": 1, "generation": 1, "reinforcement": 1, "learn": 1, "agents": 1}, {"arxiv": 1, "preprint": 1, "arxiv170506366": 1, "2017": 1}, {"6": 1, "junhyuk": 1, "oh": 1, "yijie": 1, "guo": 1, "satinder": 1, "singh": 1, "honglak": 1, "lee": 1}, {"selfimitation": 1, "learn": 1}, {"arxiv": 1, "preprint": 1, "arxiv180605635": 1, "2018": 1}, {"7": 1, "andrew": 1, "levy": 1, "robert": 1, "platt": 1, "kate": 1, "saenko": 1}, {"hierarchical": 1, "reinforcement": 1, "learn": 1, "hindsight": 1}, {"international": 1, "conference": 1, "learn": 1, "representations": 1, "2019": 1}, {"8": 1, "deepak": 1, "pathak": 1, "pulkit": 1, "agrawal": 1, "alexei": 1, "efros": 1, "trevor": 1, "darrell": 1}, {"curiositydriven": 1, "exploration": 1, "selfsupervised": 1, "prediction": 1}, {"ieee": 1, "conference": 1, "computer": 1, "vision": 1, "": 1, "pattern": 1, "recognition": 1, "workshops": 1, "2017": 1}, {"9": 1, "yuri": 1, "burda": 1, "harri": 1, "edwards": 1, "deepak": 1, "pathak": 1, "amos": 1, "storkey": 1, "trevor": 1, "darrell": 1, "alexei": 1, "efros": 1}, {"largescale": 1, "study": 1, "curiositydriven": 1, "learn": 1}, {"arxiv": 1, "preprint": 1, "arxiv180804355": 1, "2018": 1}, {"10": 1, "yuxin": 1, "wu": 1, "yuandong": 1, "tian": 1}, {"train": 1, "agent": 1, "firstperson": 1, "shooter": 1, "game": 1, "actorcritic": 1, "curriculum": 1, "learn": 1}, {"international": 1, "conference": 1, "learn": 1, "representations": 1, "2017": 1}, {"11": 1, "marcin": 1, "andrychowicz": 1, "filip": 1, "wolski": 1, "alex": 1, "ray": 1, "jonas": 1, "schneider": 1, "rachel": 1, "fong": 1, "peter": 1, "welinder": 1, "bob": 1, "mcgrew": 1, "josh": 1, "tobin": 1, "openai": 1, "pieter": 1, "abbeel": 1, "wojciech": 1, "zaremba": 1}, {"hindsight": 1, "experience": 1, "replay": 1}, {"guyon": 1, "u": 1, "v": 1, "luxburg": 1, "bengio": 1, "h": 1, "wallach": 1, "r": 2, "fergus": 1, "vishwanathan": 1, "garnett": 1, "editors": 1, "advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "30": 1, "page": 1, "50485058": 1}, {"curran": 1, "associate": 1, "inc": 1, "2017": 1}, {"12": 1, "david": 1, "e": 2, "rumelhart": 1, "geoffrey": 1, "hinton": 1, "ronald": 1, "j": 1, "williams": 1, "et": 1, "al": 1}, {"learn": 1, "representations": 1, "backpropagating": 1, "errors": 1}, {"cognitive": 1, "model": 1, "531": 1, "1988": 1}, {"13": 1, "richard": 1, "sutton": 1}, {"learn": 1, "predict": 1, "methods": 1, "temporal": 1, "differences": 1}, {"machine": 1, "learn": 1, "31944": 1, "1988": 1}, {"14": 1, "david": 1, "silver": 1, "guy": 1, "lever": 1, "nicolas": 1, "heess": 1, "thomas": 1, "degris": 1, "daan": 1, "wierstra": 1, "martin": 1, "riedmiller": 1}, {"deterministic": 1, "policy": 1, "gradient": 1, "algorithms": 1}, {"icml": 1, "2014": 1}, {"15": 1, "sham": 1, "kakade": 1}, {"natural": 1, "policy": 1, "gradient": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "page": 1, "15311538": 1, "2002": 1}, {"16": 1, "richard": 1, "sutton": 1, "david": 1, "mcallester": 1, "satinder": 1, "p": 1, "singh": 1, "yishay": 1, "mansour": 1}, {"policy": 1, "gradient": 1, "methods": 1, "reinforcement": 1, "learn": 1, "function": 1, "approximation": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "page": 1, "10571063": 1, "2000": 1}, {"17": 1, "timothy": 1, "p": 1, "lillicrap": 1, "jonathan": 1, "j": 1, "hunt": 1, "alexander": 1, "pritzel": 1, "nicolas": 1, "heess": 1, "tom": 1, "erez": 1, "yuval": 1, "tassa": 1, "david": 1, "silver": 1, "daan": 1, "wierstra": 1}, {"continuous": 1, "control": 1, "deep": 1, "reinforcement": 1, "learn": 1}, {"arxiv": 1, "preprint": 1, "arxiv150902971": 1, "2015": 1}, {"18": 1, "paulo": 1, "rauber": 1, "avinash": 1, "ummadisingu": 1, "filipe": 1, "mutz": 1, "juergen": 1, "schmidhuber": 1}, {"hindsight": 1, "policy": 1, "gradients": 1}, {"arxiv": 1, "preprint": 1, "arxiv171106006": 1, "2017": 1}, {"10": 1, "": 1, "19": 1, "michael": 1, "jordan": 1, "david": 1, "e": 1, "rumelhart": 1}, {"forward": 1, "model": 1, "supervise": 1, "learn": 1, "distal": 1, "teacher": 1}, {"cognitive": 1, "science": 1, "163307354": 1, "1992": 1}, {"20": 1, "deepak": 1, "pathak": 1, "pulkit": 1, "agrawal": 1, "alexei": 1, "efros": 1, "trevor": 1, "darrell": 1}, {"curiositydriven": 1, "exploration": 1, "selfsupervised": 1, "prediction": 1}, {"ieee": 1, "conference": 1, "computer": 1, "vision": 1, "pattern": 1, "recognition": 1, "cvpr": 1, "workshops": 1, "july": 1, "2017": 1}, {"21": 1, "evan": 1, "shelhamer": 1, "parsa": 1, "mahmoudieh": 1, "max": 1, "argus": 1, "trevor": 1, "darrell": 1}, {"loss": 1, "reward": 1, "selfsupervision": 1, "reinforcement": 1, "learn": 1}, {"arxiv": 1, "preprint": 1, "arxiv161207307": 1, "2016": 1}, {"22": 1, "piotr": 1, "mirowski": 1, "razvan": 1, "pascanu": 1, "fabio": 1, "viola": 1, "hubert": 1, "soyer": 1, "raia": 1, "hadsell": 1}, {"learn": 1, "navigate": 1, "complex": 1, "environments": 1}, {"international": 1, "conference": 1, "learn": 1, "representations": 1, "2017": 1}, {"23": 1, "tom": 1, "schaul": 1, "daniel": 1, "horgan": 1, "karol": 1, "gregor": 1, "david": 1, "silver": 1}, {"universal": 1, "value": 1, "function": 1, "approximators": 1}, {"francis": 1, "bach": 1, "david": 1, "blei": 1, "editors": 1, "proceed": 2, "32nd": 1, "international": 1, "conference": 1, "machine": 2, "learn": 2, "volume": 1, "37": 1, "research": 1, "page": 1, "13121320": 1, "lille": 1, "france": 1, "0709": 1, "jul": 1, "2015": 1}, {"pmlr": 1}, {"24": 1, "volodymyr": 1, "mnih": 1, "koray": 1, "kavukcuoglu": 1, "david": 1, "silver": 1, "andrei": 1, "rusu": 1, "joel": 1, "veness": 1, "marc": 1, "g": 1, "bellemare": 1, "alex": 1, "grave": 1, "martin": 1, "riedmiller": 1, "andreas": 1, "k": 1, "fidjeland": 1, "georg": 1, "ostrovski": 1, "et": 1, "al": 1}, {"humanlevel": 1, "control": 1, "deep": 1, "reinforcement": 1, "learn": 1}, {"nature": 1, "5187540529": 1, "2015": 1}, {"25": 1, "john": 1, "schulman": 1, "filip": 1, "wolski": 1, "prafulla": 1, "dhariwal": 1, "alec": 1, "radford": 1, "oleg": 1, "klimov": 1}, {"proximal": 1, "policy": 1, "optimization": 1, "algorithms": 1}, {"arxiv": 1, "preprint": 1, "arxiv170706347": 1, "2017": 1}, {"26": 1, "lon": 1, "bottou": 1}, {"largescale": 1, "machine": 1, "learn": 1, "stochastic": 1, "gradient": 1, "descent": 1}, {"proceed": 1, "compstat2010": 1, "page": 1, "177186": 1}, {"springer": 1, "2010": 1}, {"27": 1, "robert": 1, "hechtnielsen": 1}, {"kolmogorovs": 1, "map": 1, "neural": 1, "network": 1, "existence": 1, "theorem": 1}, {"proceed": 1, "ieee": 1, "international": 1, "conference": 1, "neural": 1, "network": 1, "iii": 1, "page": 1, "1113": 1}, {"ieee": 1, "press": 1, "1987": 1}, {"28": 1, "kurt": 1, "hornik": 1, "maxwell": 1, "stinchcombe": 1, "halbert": 1, "white": 1}, {"multilayer": 1, "feedforward": 1, "network": 1, "universal": 1, "approximators": 1}, {"neural": 1, "network": 1, "25359366": 1, "1989": 1}, {"29": 1, "vera": 1, "kuurkova": 1}, {"kolmogorovs": 1, "theorem": 1, "multilayer": 1, "neural": 1, "network": 1}, {"neural": 1, "network": 1, "53501506": 1, "1992": 1}, {"30": 1, "yuri": 1, "burda": 1, "harrison": 1, "edwards": 1, "amos": 1, "storkey": 1, "oleg": 1, "klimov": 1}, {"exploration": 1, "random": 1, "network": 1, "distillation": 1}, {"arxiv": 1, "preprint": 1, "arxiv181012894": 1, "2018": 1}, {"31": 1, "aviv": 1, "tamar": 1, "yi": 1, "wu": 1, "garrett": 1, "thomas": 1, "sergey": 1, "levine": 1, "pieter": 1, "abbeel": 1}, {"value": 1, "iteration": 1, "network": 1}, {"advance": 1, "neural": 1, "information": 1, "process": 1, "systems": 1, "page": 1, "21542162": 1, "2016": 1}, {"32": 1, "kaiming": 1, "xiangyu": 1, "zhang": 1, "shaoqing": 1, "ren": 1, "jian": 1, "sun": 1}, {"deep": 1, "residual": 1, "learn": 1, "image": 1, "recognition": 1}, {"proceed": 1, "ieee": 1, "conference": 1, "computer": 1, "vision": 1, "pattern": 1, "recognition": 1, "page": 1, "770778": 1, "2016": 1}, {"33": 1, "larbi": 1, "alili": 1, "pierre": 1, "patie": 1, "jesper": 1, "lund": 1, "pedersen": 1}, {"representations": 1, "first": 1, "hit": 1, "time": 1, "density": 1, "ornsteinuhlenbeck": 1, "process": 1}, {"stochastic": 1, "model": 1, "214967980": 1, "2005": 1}, {"34": 1, "marlin": 1, "u": 1, "thomas": 1}, {"mean": 1, "firstpassage": 1, "time": 1, "approximations": 1, "ornsteinuhlenbeck": 1, "process": 1}, {"journal": 1, "apply": 1, "probability": 1, "123600604": 1, "1975": 1}, {"35": 1, "luigi": 1, "ricciardi": 1, "shunsuke": 1, "sato": 1}, {"firstpassagetime": 1, "density": 1, "moments": 1, "ornsteinuhlenbeck": 1, "process": 1}, {"journal": 1, "apply": 1, "probability": 1, "2514357": 1, "1988": 1}, {"36": 1, "ian": 1, "blake": 1, "william": 1, "lindsey": 1}, {"levelcrossing": 1, "problems": 1, "random": 1, "process": 1}, {"ieee": 1, "transactions": 1, "information": 1, "theory": 1, "193295315": 1, "1973": 1}, {"11": 1}]